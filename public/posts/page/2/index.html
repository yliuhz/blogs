<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | LIU Yue's blogs</title><meta name=keywords content><meta name=description content="Posts - LIU Yue's blogs"><meta name=author content="Yue"><link rel=canonical href=https://yliuhz.github.io/blogs/posts/><link crossorigin=anonymous href=/blogs/assets/css/stylesheet.min.ad30084630fe889ef998d8f1baed12d2a35f52185d429fcd2d35fa7304f2dca2.css integrity="sha256-rTAIRjD+iJ75mNjxuu0S0qNfUhhdQp/NLTX6cwTy3KI=" rel="preload stylesheet" as=style><link rel=icon href=https://yliuhz.github.io/blogs/favicon.ico><link rel=apple-touch-icon href=https://yliuhz.github.io/blogs/apple-touch-icon.png><link rel=alternate type=application/rss+xml href=https://yliuhz.github.io/blogs/posts/feed.xml><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts | LIU Yue's blogs"><meta name=twitter:description content><meta property="og:title" content="Posts | LIU Yue's blogs"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://yliuhz.github.io/blogs/posts/"><meta property="og:image" content="https://yliuhz.github.io/blogs/papermod-cover.png"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yliuhz.github.io/blogs/posts/"}]}</script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript></head><body class="list-page type-posts kind-section layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://yliuhz.github.io/blogs accesskey=h title="LIU Yue's blogs (Alt + H)">LIU Yue's blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://yliuhz.github.io/blogs/archives/ title=Archive>Archive</a></li><li><a href=https://yliuhz.github.io/blogs/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=https://yliuhz.github.io/blogs/tags/ title=Tags>Tags</a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://yliuhz.github.io/blogs>Home</a></div><h1>Posts</h1><div class=inline-links><span><a href=https://yliuhz.github.io/blogs/posts/feed.xml target=_blank>RSS</a></span></div></header><article class=post-entry><header class=entry-header><h2>GNN and MLP</h2></header><section class=entry-content><p>最近发现一篇ICLR2023 spotlight的蒸馏GNN到MLP的论文 ( Citation: Tian, Zhang & al., 2023 Tian, Y., Zhang, C., Guo, Z., Zhang, X. & Chawla, N. (2023). NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs. https://doi.org/10.48550/arXiv.2208.10010 ) ，觉得很新鲜。向前追溯发现其是基于ICLR2022的GLNN ( Citation: Zhang, Liu & al., 2022 Zhang, S., Liu, Y., Sun, Y. & Shah, N. (2022). Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation. https://doi.org/10.48550/arXiv.2110.08727 ) 做的，遂在这里整理一下相关内容和自己的理解。
Graph-less Neural Networks (GLNN) 作者 ( Citation: Zhang, Liu & al., 2022 Zhang, S., Liu, Y., Sun, Y....</p></section><footer class=entry-footer><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>June 8, 2023</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>354 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>2 min</span></span></footer><a class=entry-link aria-label="post link to GNN and MLP" href=https://yliuhz.github.io/blogs/posts/gnn2mlp/></a></article><article class=post-entry><header class=entry-header><h2>Diffusion Models</h2></header><section class=entry-content><p>So why diffusion models perform well?
When \(a \ne 0\), there are two solutions to \(ax^2 + bx + c = 0\) and they are \[x = {-b \pm \sqrt{b^2-4ac} \over 2a}.\] $a^2$
$$b^2$$</p></section><footer class=entry-footer><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>May 24, 2023</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>34 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>1 min</span></span></footer><a class=entry-link aria-label="post link to Diffusion Models" href=https://yliuhz.github.io/blogs/posts/diffusion/></a></article><article class=post-entry><header class=entry-header><h2>VAE</h2></header><section class=entry-content><p>Variational Autoencoders 原博主为Lilian Weng
与简单的自编码器不同，变分自编码器的表征$\mathbf{z}$是一个分布。 给定一个数据集$\mathbf{X}=\{\mathbf{x}_i\}_{i=1}^N$，变分自编码器的观点是$\mathbf{x}$由一个隐变量$\mathbf{z}$产生，而$\mathbf{z}$则遵循一个先验分布，通常取正态分布。 因此，变分自编码器可以由3个概率分布刻画：
$p(\mathbf{z})$: 先验分布 $p(\mathbf{x}|\mathbf{z})$: 解码器 $p(\mathbf{z}|\mathbf{x})$: 后验分布，编码器 其中后验分布很难直接计算，因此自编码器从一个未训练过的编码器，即对后验分布的估计$q(\mathbf{z}|\mathbf{x})$开始，通过优化目标函数不断逼近$q(\mathbf{z}|\mathbf{x})$和$p(\mathbf{z}|\mathbf{x})$的距离。
这里使用KL散度衡量两个分布的距离，即$D_{KL}(q(\mathbf{z}|\mathbf{x})||p(\mathbf{z}|\mathbf{x}))$。注意KL散度不具有对称性，原博主Lilian Weng甚至指出了为什么不使用$D_{KL}(p(\mathbf{z}|\mathbf{x})||q(\mathbf{z}|\mathbf{x}))$。
具体来说，前向KL散度$D_{KL}(p||q)=\mathbb{E}_{\mathbf{z}\sim p(\mathbf{z})}\log \frac{p(\mathbf{z})}{q(\mathbf{z})}=\int p(\mathbf{z})\log \frac{p(\mathbf{z})}{q(\mathbf{z})}d\mathbf{z}$中，p>0的位置要求q必须同时>0(因为$\lim_{q\to 0}p\log \frac{p}{q}\to \infty$)。因此优化前向KL散度会导致q覆盖了每个p分布概率不为0的点。反过来，我们这里使用的反向KL散度$D_{KL}(q||p)=\mathbb{E}_{\mathbf{z}\sim q(\mathbf{z})}\log \frac{q(\mathbf{z})}{p(\mathbf{z})}=\int q(\mathbf{z})\log \frac{q(\mathbf{z})}{p(\mathbf{z})}d\mathbf{z}$，在p=0时保证了q必须=0。
前向KL散度：p>0时q>0，可能导致q平铺在p>0的区域 反向KL散度（使用的）：p=0时q=0，可能导致q被挤压在p的一个峰上 在推导KL散度的表达式时就可以得到变分自编码器的损失函数ELBO。
(图源Lilian Weng的博客：https://lilianweng.github.io/posts/2018-08-12-vae/)
我们想同时极大化观测数据点$\mathbf{x}$的似然，以及真假编码器的分布差距，即最大化 $$\mathbb{E}_{\mathbf{z}\sim q(\mathbf{z}|\mathbf{x})}\log p(\mathbf{x}|\mathbf{z})-D_{KL}(q(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))$$ 左边的是重构误差取反，右边的在先验分布为正态分布时可以显式展开。
在计算重构误差时用到了重参数技巧（reparameterization trick），即把从一个带参数的编码器采样$\mathbf{z}$，转化为从一个确定的分布（如标准正态）采样一个值，再通过将采样的值与编码器的输出（均值和方差）加减乘除得到$\mathbf{z}$。这样梯度就和采样独立开来，可以反向传播了。</p></section><footer class=entry-footer><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>May 23, 2023</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>40 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>1 min</span></span></footer><a class=entry-link aria-label="post link to VAE" href=https://yliuhz.github.io/blogs/posts/vae/></a></article><article class=post-entry><header class=entry-header><h2>FlashAttention</h2></header><section class=entry-content><p>FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。
作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。
输入：$\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$
输出：$\mathbf{O}\in\mathbb{R}^{N\times d}$
标准self-attention：
$\mathbf{S}=\mathbf{Q}\mathbf{K}^T\in\mathbb{R}^{N\times N}$
$\mathbf{P}=\exp(\mathbf{S})$
$\mathbf{O}=\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。
Flash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\mathbf{S}$。
首先推导增量式的softmax函数：
对一个向量$\mathbf{x}$计算softmax：$\sigma(\mathbf{x})=\exp(\mathbf{x})/{\sum_i {\exp(\mathbf{x}_i)}}$
对两个向量的拼接$[\mathbf{x},\mathbf{y}]$计算softmax：$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(\sum_i\exp(\mathbf{x}_i)+\sum_j\exp(\mathbf{y}_j))$
设$l(\mathbf{x})=\sum_i\exp(\mathbf{x}_i)$，则$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(l(\mathbf{x})+l(\mathbf{y}))$
将$\mathbf{Q,O},l$分成$T_r$块，将$\mathbf{K,V}$分成$T_c$块，进行二重循环。
1 2 3 4 for j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：
$\mathbf{S}_{ij}=\mathbf{Q}_i\mathbf{K}_j^T$
$\mathbf{P}_{ij}=\exp(\mathbf{S}_{ij})$
$l_{ij}=\text{rowsum}(\mathbf{P}_{ij})$
$\mathbf{O}_i’=\mathbf{P}_{ij}\mathbf{V}_j$
然后需要对上一轮的$\mathbf{O_i}$和$l_i$进行更新，以d=1为例。
$l_i^{new}=l_i+l_{ij}$比较直接
两个红色的矩阵相乘得到当前的$\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\mathbf{O}_i$由i行j列的内积得来，还需要加上$\mathbf{O}_{ij}$，这样得到$\mathbf{O}_i$的增量式更新：
$\mathbf{O}_i=\mathbf{O}_i*l_i/l_i^{new} + \mathbf{O}_{ij}$
论文中的Algorithm1由于考虑了算术稳定性防止\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。
发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github
A1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\mathbf{O}$。但显然增加了$\mathbf{K}_j$和$\mathbf{V}_j$的IO次数。
1 2 3 4 5 6 7 8 for i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。...</p></section><footer class=entry-footer><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>May 18, 2023</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>109 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>1 min</span></span></footer><a class=entry-link aria-label="post link to FlashAttention" href=https://yliuhz.github.io/blogs/posts/flashattention/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://yliuhz.github.io/blogs/posts/>« Prev Page</a></nav></footer></main><footer class=footer><span>&copy; 2023 <a href=https://yliuhz.github.io/blogs>LIU Yue's blogs</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a></span>
<span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/blogs/js/medium-zoom.min.js data-no-instant></script></body></html>