<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on LIU Yue&#39;s blogs</title>
    <link>https://yliuhz.github.io/blogs/posts/</link>
    <description>Recent content in Posts on LIU Yue&#39;s blogs</description>
    <image>
      <title>LIU Yue&#39;s blogs</title>
      <url>https://yliuhz.github.io/blogs/papermod-cover.png</url>
      <link>https://yliuhz.github.io/blogs/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 09 Jun 2023 11:16:54 +0800</lastBuildDate><atom:link href="https://yliuhz.github.io/blogs/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Eng</title>
      <link>https://yliuhz.github.io/blogs/posts/eng/</link>
      <pubDate>Fri, 09 Jun 2023 11:16:54 +0800</pubDate>
      
      <guid>https://yliuhz.github.io/blogs/posts/eng/</guid>
      <description>Demo Page Ferre hinnitibus erat accipitrem dixi Troiae tollens Lorem ipsum dolor ( Citation: Vitali Rosati,&amp;#32;2016,&amp;#32;p.&amp;nbsp;20 Vitali Rosati,&amp;#32; M. &amp;#32; (2016). &amp;#32;Qu&amp;rsquo;est-ce que l&amp;rsquo;éditorialisation ?. Sens Public.&amp;#32;Retrieved from&amp;#32; http://sens-public.org/article1184.html ;&amp;#32; Citation: Sinatra&amp;#32;&amp;amp;&amp;#32;Vitali-Rosati,&amp;#32;2014,&amp;#32;pp.&amp;nbsp;22-23 Sinatra,&amp;#32; M.&amp;#32;&amp;amp;&amp;#32;Vitali-Rosati,&amp;#32; M.&amp;#32; (2014). &amp;#32; Pratiques de l&amp;rsquo;édition numérique. &amp;#32; Les Presses de l&amp;#39;Université de Montréal. ) amet.
Lorem markdownum ( Citation: Vial&amp;#32;&amp;amp;&amp;#32;Catoir-Brisson,&amp;#32;2017 Vial,&amp;#32; S.&amp;#32;&amp;amp;&amp;#32;Catoir-Brisson,&amp;#32; M.&amp;#32; (2017). &amp;#32; Design et innovation dans la chaîne du livre: écrire, éditer, lire à l&amp;rsquo;ère numérique.</description>
    </item>
    
    <item>
      <title>Gnn2mlp</title>
      <link>https://yliuhz.github.io/blogs/posts/gnn2mlp/</link>
      <pubDate>Thu, 08 Jun 2023 16:37:51 +0800</pubDate>
      
      <guid>https://yliuhz.github.io/blogs/posts/gnn2mlp/</guid>
      <description>最近发现一篇ICLR2023 spotlight的蒸馏GNN到MLP的论文，觉得很新鲜。向前追溯发现其是基于ICLR2022的GLNN做的，遂在这里整理一下相关内容和自己的理解。
Graph-less Neural Networks (GLNN) 作者( ( Citation: Vitali Rosati,&amp;#32;2016,&amp;#32;p.&amp;nbsp;20 Vitali Rosati,&amp;#32; M. &amp;#32; (2016). &amp;#32;Qu&amp;rsquo;est-ce que l&amp;rsquo;éditorialisation ?. Sens Public.&amp;#32;Retrieved from&amp;#32; http://sens-public.org/article1184.html ;&amp;#32; Citation: Sinatra&amp;#32;&amp;amp;&amp;#32;Vitali-Rosati,&amp;#32;2014,&amp;#32;pp.&amp;nbsp;22-23 Sinatra,&amp;#32; M.&amp;#32;&amp;amp;&amp;#32;Vitali-Rosati,&amp;#32; M.&amp;#32; (2014). &amp;#32; Pratiques de l&amp;rsquo;édition numérique. &amp;#32; Les Presses de l&amp;#39;Université de Montréal. ) )指出现实场景难以落地GNN的一大原因是GNN的推理速度很慢。假设图中平均的顶点度为$R$，那么对于一个$L$层GNN的网络，总共需要提取(fetch)$O(R^L)$次邻居和自己的节点特征。如下图所示。该指数量级的提取次数导致GNN的推理时间随层数增加而指数上升。 另一方面，多层感知机MLP由于不需要图结构作为输入，因此无需提取其他节点的特征，推理速度是线性的。
为了节省推理时间，直接使用MLP在图上训练也是不可行的，因为丢掉了图结构信息。为了达到MLP的推理时间同时尽量保留图的结构信息，作者提出了从GNN蒸馏知识到MLP的方法，并验证了其有效性。
解决框架 GLNN的结构容易理解，先训练一个笨重的GNN模型作为教师模型，再使用该GNN的输出$\mathbf{z}_v$以及带标签节点本身的标签$\mathbf{y}_v$训练简单的MLP学生网络。在归纳式学习(inductive learning)场景中， 当新的节点到来时，不再考虑其与训练图结构的连边，而是直接输入到MLP中做推理。 如下图所示。
作者对于直推式和归纳式的详细描述： 可以看到测试时MLP和GLNN的学生网络是没有图结构输入的，只有测试顶点的特征向量。 同时，在测试教师网络GNN的归纳式推理时，只使用训练集图结构训练，而在测试时使用了包括测试顶点在内的整张图作为输入。这样对比是公平的。因为在使用GNN模型推理时我们会尽可能发挥模型的性能，为模型提供尽可能多的信息（见代码 official code）。
训练学生网络时使用的损失函数为
$$\mathcal{L}=\lambda\sum_{v\in\mathcal{V}^L}\mathcal{L}_{label}(\hat{\mathbf{y}}_v,\mathbf{y}_v)+(1-\lambda)\sum_{v\in\mathcal{V}}\mathcal{L}_{teacher}(\hat{\mathbf{y}}_v,\mathbf{z}_v)$$
其中$\mathcal{L}_{label}$为交叉熵损失，$\mathcal{L}_{teacher}$为KL散度损失，$\lambda\in[0,1]$是超参数，$\mathcal{V}^L$表示带标签的训练节点，$\mathcal{V}$表示所有训练节点。$\mathcal{L}_{teacher}$的含义是使学生网络输出的分布与教师网络的输出分布相近。
实验 作者做了大量的实验，包括直推式(transductive)的推理、归纳式(inductive)的推理，与其他GNN加速方法做了比较，通过一个参量(min-cut loss)验证蒸馏的有效性，验证GLNN的表达能力(理论推导)，分析了GLNN失败的场景。参数实验（消融实验）中验证了特征中加噪声的影响，归纳式推理时不同训练测试划分的影响，以及使用其他教师网络模型的情况。在附录部分还加了在异配图（NON-HOMOPHILY）上的实验，并更加详细地分析了节点特征噪声对GLNN的影响。
作者指出虽然现实场景中大多是归纳式推理，但直推式推理的实验仍然是有意义的(附录A.5)。第一，大多数现有GNN文献使用的是直推式的推理，为了公平对比。第二，直推式推理相对简单，因为在训练时看到了测试节点的特征。只有直推式能够work才能接着考虑更有挑战性的归纳式推理。第三，因为半监督训练时使用的标签很少，如pubmed数据集只有每个类别20个共60个标签，作者希望尽可能使用更多无标签节点提升性能。在现实场景中，当有很多无标签节点需要推理时，同样可以把它们拿来训练，然后用另一组不同的带标签测试节点做评测。
在附录J中，作者详细讨论了使用噪声扰动节点特征的实验结果(如下图Left)。发现2点：
当节点特征为纯高斯噪声($\alpha=1$)时，原始GNN仍然相对较好； 当节点特征为纯高斯噪声($\alpha=1$)时，蒸馏的GLNN比纯训练MLP好。 觉得这两个结果特殊是因为作者在正文5.8简单分析了什么情况下GLNN会失效，通过互信息：
$$I(G;y_i)=I(X^{[i]},\mathcal{E}^{[i]};y_i)=I(\mathcal{E}^{[i]};y_i)+I(X^{[i]};y_i|\mathcal{E}^{[i]})$$
最小化$\mathcal{L}_{label}$相当于在最大化$I(G;y_i)$。在上式中，$I(\mathcal{E}^{[i]};y_i)$表示边与节点标签的互信息，这是MLP无法访问到的。因此MLP只能最大化第二项$I(X^{[i]};y_i|\mathcal{E}^{[i]})$。然而，当节点的标签与特征无关时，比如节点的标签表示节点的度或者节点是否构成一个三角形，此时MLP和GLNN的学生网络都无法学到单单从节点特征到标签的映射$f$。这样分析的话上面的第2条实验结果就有点奇怪。</description>
    </item>
    
    <item>
      <title>Diffusion Models</title>
      <link>https://yliuhz.github.io/blogs/posts/diffusion/</link>
      <pubDate>Wed, 24 May 2023 10:45:03 +0800</pubDate>
      
      <guid>https://yliuhz.github.io/blogs/posts/diffusion/</guid>
      <description>So why diffusion models perform well?
When \(a \ne 0\), there are two solutions to \(ax^2 + bx + c = 0\) and they are \[x = {-b \pm \sqrt{b^2-4ac} \over 2a}.\] $a^2$
$$b^2$$</description>
    </item>
    
    <item>
      <title>VAE</title>
      <link>https://yliuhz.github.io/blogs/posts/vae/</link>
      <pubDate>Tue, 23 May 2023 20:42:27 +0800</pubDate>
      
      <guid>https://yliuhz.github.io/blogs/posts/vae/</guid>
      <description>Variational Autoencoders 原博主为Lilian Weng
与简单的自编码器不同，变分自编码器的表征$\mathbf{z}$是一个分布。 给定一个数据集$\mathbf{X}=\{\mathbf{x}_i\}_{i=1}^N$，变分自编码器的观点是$\mathbf{x}$由一个隐变量$\mathbf{z}$产生，而$\mathbf{z}$则遵循一个先验分布，通常取正态分布。 因此，变分自编码器可以由3个概率分布刻画：
$p(\mathbf{z})$: 先验分布 $p(\mathbf{x}|\mathbf{z})$: 解码器 $p(\mathbf{z}|\mathbf{x})$: 后验分布，编码器 其中后验分布很难直接计算，因此自编码器从一个未训练过的编码器，即对后验分布的估计$q(\mathbf{z}|\mathbf{x})$开始，通过优化目标函数不断逼近$q(\mathbf{z}|\mathbf{x})$和$p(\mathbf{z}|\mathbf{x})$的距离。
这里使用KL散度衡量两个分布的距离，即$D_{KL}(q(\mathbf{z}|\mathbf{x})||p(\mathbf{z}|\mathbf{x}))$。注意KL散度不具有对称性，原博主Lilian Weng甚至指出了为什么不使用$D_{KL}(p(\mathbf{z}|\mathbf{x})||q(\mathbf{z}|\mathbf{x}))$。
具体来说，前向KL散度$D_{KL}(p||q)=\mathbb{E}_{\mathbf{z}\sim p(\mathbf{z})}\log \frac{p(\mathbf{z})}{q(\mathbf{z})}=\int p(\mathbf{z})\log \frac{p(\mathbf{z})}{q(\mathbf{z})}d\mathbf{z}$中，p&amp;gt;0的位置要求q必须同时&amp;gt;0(因为$\lim_{q\to 0}p\log \frac{p}{q}\to \infty$)。因此优化前向KL散度会导致q覆盖了每个p分布概率不为0的点。反过来，我们这里使用的反向KL散度$D_{KL}(q||p)=\mathbb{E}_{\mathbf{z}\sim q(\mathbf{z})}\log \frac{q(\mathbf{z})}{p(\mathbf{z})}=\int q(\mathbf{z})\log \frac{q(\mathbf{z})}{p(\mathbf{z})}d\mathbf{z}$，在p=0时保证了q必须=0。
前向KL散度：p&amp;gt;0时q&amp;gt;0，可能导致q平铺在p&amp;gt;0的区域 反向KL散度（使用的）：p=0时q=0，可能导致q被挤压在p的一个峰上 在推导KL散度的表达式时就可以得到变分自编码器的损失函数ELBO。
(图源Lilian Weng的博客：https://lilianweng.github.io/posts/2018-08-12-vae/)
我们想同时极大化观测数据点$\mathbf{x}$的似然，以及真假编码器的分布差距，即最大化 $$\mathbb{E}_{\mathbf{z}\sim q(\mathbf{z}|\mathbf{x})}\log p(\mathbf{x}|\mathbf{z})-D_{KL}(q(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))$$ 左边的是重构误差取反，右边的在先验分布为正态分布时可以显式展开。
在计算重构误差时用到了重参数技巧（reparameterization trick），即把从一个带参数的编码器采样$\mathbf{z}$，转化为从一个确定的分布（如标准正态）采样一个值，再通过将采样的值与编码器的输出（均值和方差）加减乘除得到$\mathbf{z}$。这样梯度就和采样独立开来，可以反向传播了。</description>
    </item>
    
    <item>
      <title>FlashAttention</title>
      <link>https://yliuhz.github.io/blogs/posts/flashattention/</link>
      <pubDate>Thu, 18 May 2023 18:59:54 +0800</pubDate>
      
      <guid>https://yliuhz.github.io/blogs/posts/flashattention/</guid>
      <description>FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。
作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。
输入：$\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$
输出：$\mathbf{O}\in\mathbb{R}^{N\times d}$
标准self-attention：
$\mathbf{S}=\mathbf{Q}\mathbf{K}^T\in\mathbb{R}^{N\times N}$
$\mathbf{P}=\exp(\mathbf{S})$
$\mathbf{O}=\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。
Flash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\mathbf{S}$。
首先推导增量式的softmax函数：
对一个向量$\mathbf{x}$计算softmax：$\sigma(\mathbf{x})=\exp(\mathbf{x})/{\sum_i {\exp(\mathbf{x}_i)}}$
对两个向量的拼接$[\mathbf{x},\mathbf{y}]$计算softmax：$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(\sum_i\exp(\mathbf{x}_i)+\sum_j\exp(\mathbf{y}_j))$
设$l(\mathbf{x})=\sum_i\exp(\mathbf{x}_i)$，则$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(l(\mathbf{x})+l(\mathbf{y}))$
将$\mathbf{Q,O},l$分成$T_r$块，将$\mathbf{K,V}$分成$T_c$块，进行二重循环。
for j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：
$\mathbf{S}_{ij}=\mathbf{Q}_i\mathbf{K}_j^T$
$\mathbf{P}_{ij}=\exp(\mathbf{S}_{ij})$
$l_{ij}=\text{rowsum}(\mathbf{P}_{ij})$
$\mathbf{O}_i&amp;rsquo;=\mathbf{P}_{ij}\mathbf{V}_j$
然后需要对上一轮的$\mathbf{O_i}$和$l_i$进行更新，以d=1为例。
$l_i^{new}=l_i+l_{ij}$比较直接
两个红色的矩阵相乘得到当前的$\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\mathbf{O}_i$由i行j列的内积得来，还需要加上$\mathbf{O}_{ij}$，这样得到$\mathbf{O}_i$的增量式更新：
$\mathbf{O}_i=\mathbf{O}_i*l_i/l_i^{new} + \mathbf{O}_{ij}$
论文中的Algorithm1由于考虑了算术稳定性防止\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。
发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github
A1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\mathbf{O}$。但显然增加了$\mathbf{K}_j$和$\mathbf{V}_j$的IO次数。
for i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。
for i in 1...T_r: 取出Q_i,O_i和l_i for j in 1...T_c: 取出K_j和V_j ... 更新O_i和l_i Q2.</description>
    </item>
    
  </channel>
</rss>
