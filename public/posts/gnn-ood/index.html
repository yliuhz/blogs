<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generalization of GNNs and MLPs | LIU Yue's blogs</title><meta name=keywords content><meta name=description content="本文是 ( Citation: Xu, Zhang & al., 2021 Xu,  K.,  Zhang,  M.,  Li,  J.,  Du,  S.,  Kawarabayashi,  K. & Jegelka,  S.   (2021).  How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. https://doi.org/10.48550/arXiv.2009.11848 ) 的论文解读。OpenReview显示这篇论文是ICLR2021的Oral论文（前5%）。
引言 人类具有泛化性，例如学会算术后可以应用到任意大的数字。对于神经网络而言，前馈网络（也叫多层感知机，MLPs）在学习简单的多项式函数时就不能很好地泛化了。然而，基于MLP的图神经网络（GNNs）却在近期的一些任务上表现出较好的泛化性，包括预测物理系统的演进规律，学习图论算法，解决数学公式等。 粗略地分析可能会觉得神经网络可以在训练分布以外的数据上有任意不确定的表现，但是现实中的神经网络大多是用梯度下降训练的，这就导致其泛化性能有规律可以分析。作者使用&#34;神经切线核&#34;（neural tangent kernel，NTK）工具进行分析。
本文的第一个结论是使用梯度下降训练的MLPs会收敛到任意方向线性的函数，因此MLPs在大多数非线性任务上无法泛化。
接着本文将分析延伸到基于MLP的GNNs，得到第二个结论：使用线性对齐简化目标函数使得基于MLP的GNNs在非线性任务上能够泛化，即将非线性部分提前集成在模型的结构（如GNN的聚合和读出函数）或在输入的表征向量中（使用无监督方法将输入特征转化为表征向量）。
前置知识 设$\mathcal{X}$表示数据（向量或图）的域。任务是学习一个函数$g:\mathcal{X}\to \mathbb{R}$，其中训练数据$\{(\pmb{x}_i,y_i)\}\in\mathcal{D}$，$y_i=g(\pmb{x_i})$，$\mathcal{D}$表示训练数据的分布。在训练数据和测试数据同分布的情况下，$\mathcal{D}=\mathcal{X}$；而在评估泛化能力时，$\mathcal{D} $是$\mathcal{X}$的子集。一个模型的泛化能力可以用泛化误差评估：设$f$为模型在训练数据上得到的函数，$l$为任意损失函数，则泛化误差定义为$\mathbb{E}_{\pmb{x}\sim \mathcal{X} \setminus \mathcal{D}}[l(f(\pmb{x}), g(\pmb{x}))]$
图神经网络GNNs是在MLPs基础上定义的网络。具体来说，初始顶点表征为$\pmb{h}_u^{(0)}=\pmb{x}_u$。在第$k={1..K}$层，顶点表征更新公式为
$$\begin{aligned}\pmb{h}_u^{(k)}&=\sum_{v\in\mathcal{N}(u)}\text{MLP}^{(k)}(\pmb{h}_u^{(k-1)},\pmb{h}_v^{(k-1)},\pmb{w}_{(v,u)}) \\ \pmb{h}_G&=\text{MLP}^{(K+1)}(\sum_{u\in G}\pmb{h}_u^{(K)})\end{aligned}$$
其中$\pmb{h}_u^{(k)}$表示第$k$层GNN输出的顶点$u$的表征，$\pmb{h}_G$表示整张图的表征。$\pmb{h}_u^{(k)}$的计算过程称为聚合，$\pmb{h}_G$的计算过程称为读出。以往研究大多使用$\text{sum}$聚合与$\text{sum}$读出，而本文指出替换为另外的函数能够提升泛化性能。
前馈网络MLPs如何泛化 ReLU MLPs的线性泛化 作者用下图呈现MLPs的泛化方式。灰色表示MLPs要学习的真实函数，蓝色和黑色分别表示模型在训练集和测试集上的预测。可以看到模型可以拟合训练集上的非线性函数，但脱离训练集后迅速变为线性函数。用数字来说，脱离训练集后MLPs预测的决定系数大于$0.99$。
定理1（线性泛化）：假设在NTK机制下使用均方误差训练了一个两层MLP：$f:\mathbb{R}^d\to\mathbb{R}$。对于任意方向$\pmb{v}\in\mathbb{R}^d$，令$\pmb{x}_0=t\pmb{v}$，那么当$t\to\infty$时，$f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)\to\beta_vh$对任意的$h>0$成立，$\beta_v$是常数。进一步地，给定$\epsilon>0$，对于$t=O(\frac{1}{\epsilon})$，$|\frac{f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)}{h}-\beta_v|<\epsilon$。
定理1说明了在训练数据集以外，ReLU MLPs可以拟合几乎线性的函数。对于二次函数（$\pmb{x}^TA\pmb{x}$）、余弦函数($\sum_{i=1}^d\cos(2\pi\cdot\pmb{x}^{(i)})$)、根次函数（$\sum_{i=1}^d\sqrt{\pmb{x}^{(i)}}$）等，ReLU MLPs不能泛化。 在合适的超参数下，MLPs可以正确地泛化L1范数，与定理1一致。如下图所示。
ReLU MLPs什么时候一定（provably）可以泛化 尽管上图显示MLPs对于线性函数可以较好地泛化，但这需要一定的条件，即训练数据集的分布必须足够“多样”。下面的引理1指出只需要$2d$条认真挑选的数据就可以实现ReLU MLPs的线性泛化。
引理1：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设数据集$\{\pmb{x}_i\}_{i=1}^n$包含正交基$\{\hat{\pmb{x}}_i\}_{i=1}^d$和$\{-\hat{\pmb{x}}_i\}_{i=1}^d$。若使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，那么$f({\pmb{x}})={\pmb{\beta}}^T{\pmb{x}}$对任意的${\pmb{x}}\in\mathbb{R}^d$成立。
然而，仔细挑选出$2d$条符合条件的样本并不容易。下面的定理2基于更现实的场景，指出只要训练数据的分布包含所有的方向（例如一个包含原点的超球），那么在足够的训练数据量下MLP能够收敛到线性函数。
定理2（泛化的条件）：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设$\{\pmb{x}_i\}_{i=1}^n$从域$\mathcal{D}$中采样，其中$\mathcal{D}$包含一个连通的子集$S$，满足对任意非零向量$\pmb{w}\in\mathbb{R}^d$，存在$k>0$使得$k\pmb{w}\in S$。若在NTK机制下，使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，$f(\pmb{x})\xrightarrow{p}\pmb{\beta}^T\pmb{x}$在$n\to\infty$时成立，即$f$依概率收敛到$g$。"><meta name=author content="Yue"><link rel=canonical href=https://yliuhz.github.io/blogs/posts/gnn-ood/><link crossorigin=anonymous href=/blogs/assets/css/stylesheet.dc9bb5ce5660fc78ad32757018b41c1e0e7005f422801daf510da9bc9fdb0543.css integrity="sha256-3Ju1zlZg/HitMnVwGLQcHg5wBfQigB2vUQ2pvJ/bBUM=" rel="preload stylesheet" as=style><link rel=icon href=https://yliuhz.github.io/blogs/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yliuhz.github.io/blogs/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yliuhz.github.io/blogs/favicon-32x32.png><link rel=apple-touch-icon href=https://yliuhz.github.io/blogs/apple-touch-icon.png><link rel=mask-icon href=https://yliuhz.github.io/blogs/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<link rel=stylesheet href=https://yliuhz.github.io/blogs/css/extended/syntax.min.a2148732277aa7e9d7ccd409b4ecdc9f209783063d8fd3514ac8efd1d2647cf3.css integrity="sha256-ohSHMid6p+nXzNQJtOzcnyCXgwY9j9NRSsjv0dJkfPM="><meta property="og:title" content="Generalization of GNNs and MLPs"><meta property="og:description" content="本文是 ( Citation: Xu, Zhang & al., 2021 Xu,  K.,  Zhang,  M.,  Li,  J.,  Du,  S.,  Kawarabayashi,  K. & Jegelka,  S.   (2021).  How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. https://doi.org/10.48550/arXiv.2009.11848 ) 的论文解读。OpenReview显示这篇论文是ICLR2021的Oral论文（前5%）。
引言 人类具有泛化性，例如学会算术后可以应用到任意大的数字。对于神经网络而言，前馈网络（也叫多层感知机，MLPs）在学习简单的多项式函数时就不能很好地泛化了。然而，基于MLP的图神经网络（GNNs）却在近期的一些任务上表现出较好的泛化性，包括预测物理系统的演进规律，学习图论算法，解决数学公式等。 粗略地分析可能会觉得神经网络可以在训练分布以外的数据上有任意不确定的表现，但是现实中的神经网络大多是用梯度下降训练的，这就导致其泛化性能有规律可以分析。作者使用&#34;神经切线核&#34;（neural tangent kernel，NTK）工具进行分析。
本文的第一个结论是使用梯度下降训练的MLPs会收敛到任意方向线性的函数，因此MLPs在大多数非线性任务上无法泛化。
接着本文将分析延伸到基于MLP的GNNs，得到第二个结论：使用线性对齐简化目标函数使得基于MLP的GNNs在非线性任务上能够泛化，即将非线性部分提前集成在模型的结构（如GNN的聚合和读出函数）或在输入的表征向量中（使用无监督方法将输入特征转化为表征向量）。
前置知识 设$\mathcal{X}$表示数据（向量或图）的域。任务是学习一个函数$g:\mathcal{X}\to \mathbb{R}$，其中训练数据$\{(\pmb{x}_i,y_i)\}\in\mathcal{D}$，$y_i=g(\pmb{x_i})$，$\mathcal{D}$表示训练数据的分布。在训练数据和测试数据同分布的情况下，$\mathcal{D}=\mathcal{X}$；而在评估泛化能力时，$\mathcal{D} $是$\mathcal{X}$的子集。一个模型的泛化能力可以用泛化误差评估：设$f$为模型在训练数据上得到的函数，$l$为任意损失函数，则泛化误差定义为$\mathbb{E}_{\pmb{x}\sim \mathcal{X} \setminus \mathcal{D}}[l(f(\pmb{x}), g(\pmb{x}))]$
图神经网络GNNs是在MLPs基础上定义的网络。具体来说，初始顶点表征为$\pmb{h}_u^{(0)}=\pmb{x}_u$。在第$k={1..K}$层，顶点表征更新公式为
$$\begin{aligned}\pmb{h}_u^{(k)}&=\sum_{v\in\mathcal{N}(u)}\text{MLP}^{(k)}(\pmb{h}_u^{(k-1)},\pmb{h}_v^{(k-1)},\pmb{w}_{(v,u)}) \\ \pmb{h}_G&=\text{MLP}^{(K+1)}(\sum_{u\in G}\pmb{h}_u^{(K)})\end{aligned}$$
其中$\pmb{h}_u^{(k)}$表示第$k$层GNN输出的顶点$u$的表征，$\pmb{h}_G$表示整张图的表征。$\pmb{h}_u^{(k)}$的计算过程称为聚合，$\pmb{h}_G$的计算过程称为读出。以往研究大多使用$\text{sum}$聚合与$\text{sum}$读出，而本文指出替换为另外的函数能够提升泛化性能。
前馈网络MLPs如何泛化 ReLU MLPs的线性泛化 作者用下图呈现MLPs的泛化方式。灰色表示MLPs要学习的真实函数，蓝色和黑色分别表示模型在训练集和测试集上的预测。可以看到模型可以拟合训练集上的非线性函数，但脱离训练集后迅速变为线性函数。用数字来说，脱离训练集后MLPs预测的决定系数大于$0.99$。
定理1（线性泛化）：假设在NTK机制下使用均方误差训练了一个两层MLP：$f:\mathbb{R}^d\to\mathbb{R}$。对于任意方向$\pmb{v}\in\mathbb{R}^d$，令$\pmb{x}_0=t\pmb{v}$，那么当$t\to\infty$时，$f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)\to\beta_vh$对任意的$h>0$成立，$\beta_v$是常数。进一步地，给定$\epsilon>0$，对于$t=O(\frac{1}{\epsilon})$，$|\frac{f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)}{h}-\beta_v|<\epsilon$。
定理1说明了在训练数据集以外，ReLU MLPs可以拟合几乎线性的函数。对于二次函数（$\pmb{x}^TA\pmb{x}$）、余弦函数($\sum_{i=1}^d\cos(2\pi\cdot\pmb{x}^{(i)})$)、根次函数（$\sum_{i=1}^d\sqrt{\pmb{x}^{(i)}}$）等，ReLU MLPs不能泛化。 在合适的超参数下，MLPs可以正确地泛化L1范数，与定理1一致。如下图所示。
ReLU MLPs什么时候一定（provably）可以泛化 尽管上图显示MLPs对于线性函数可以较好地泛化，但这需要一定的条件，即训练数据集的分布必须足够“多样”。下面的引理1指出只需要$2d$条认真挑选的数据就可以实现ReLU MLPs的线性泛化。
引理1：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设数据集$\{\pmb{x}_i\}_{i=1}^n$包含正交基$\{\hat{\pmb{x}}_i\}_{i=1}^d$和$\{-\hat{\pmb{x}}_i\}_{i=1}^d$。若使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，那么$f({\pmb{x}})={\pmb{\beta}}^T{\pmb{x}}$对任意的${\pmb{x}}\in\mathbb{R}^d$成立。
然而，仔细挑选出$2d$条符合条件的样本并不容易。下面的定理2基于更现实的场景，指出只要训练数据的分布包含所有的方向（例如一个包含原点的超球），那么在足够的训练数据量下MLP能够收敛到线性函数。
定理2（泛化的条件）：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设$\{\pmb{x}_i\}_{i=1}^n$从域$\mathcal{D}$中采样，其中$\mathcal{D}$包含一个连通的子集$S$，满足对任意非零向量$\pmb{w}\in\mathbb{R}^d$，存在$k>0$使得$k\pmb{w}\in S$。若在NTK机制下，使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，$f(\pmb{x})\xrightarrow{p}\pmb{\beta}^T\pmb{x}$在$n\to\infty$时成立，即$f$依概率收敛到$g$。"><meta property="og:type" content="article"><meta property="og:url" content="https://yliuhz.github.io/blogs/posts/gnn-ood/"><meta property="og:image" content="https://yliuhz.github.io/blogs/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-26T20:08:51+08:00"><meta property="article:modified_time" content="2023-06-26T20:08:51+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yliuhz.github.io/blogs/papermod-cover.png"><meta name=twitter:title content="Generalization of GNNs and MLPs"><meta name=twitter:description content="本文是 ( Citation: Xu, Zhang & al., 2021 Xu,  K.,  Zhang,  M.,  Li,  J.,  Du,  S.,  Kawarabayashi,  K. & Jegelka,  S.   (2021).  How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. https://doi.org/10.48550/arXiv.2009.11848 ) 的论文解读。OpenReview显示这篇论文是ICLR2021的Oral论文（前5%）。
引言 人类具有泛化性，例如学会算术后可以应用到任意大的数字。对于神经网络而言，前馈网络（也叫多层感知机，MLPs）在学习简单的多项式函数时就不能很好地泛化了。然而，基于MLP的图神经网络（GNNs）却在近期的一些任务上表现出较好的泛化性，包括预测物理系统的演进规律，学习图论算法，解决数学公式等。 粗略地分析可能会觉得神经网络可以在训练分布以外的数据上有任意不确定的表现，但是现实中的神经网络大多是用梯度下降训练的，这就导致其泛化性能有规律可以分析。作者使用&#34;神经切线核&#34;（neural tangent kernel，NTK）工具进行分析。
本文的第一个结论是使用梯度下降训练的MLPs会收敛到任意方向线性的函数，因此MLPs在大多数非线性任务上无法泛化。
接着本文将分析延伸到基于MLP的GNNs，得到第二个结论：使用线性对齐简化目标函数使得基于MLP的GNNs在非线性任务上能够泛化，即将非线性部分提前集成在模型的结构（如GNN的聚合和读出函数）或在输入的表征向量中（使用无监督方法将输入特征转化为表征向量）。
前置知识 设$\mathcal{X}$表示数据（向量或图）的域。任务是学习一个函数$g:\mathcal{X}\to \mathbb{R}$，其中训练数据$\{(\pmb{x}_i,y_i)\}\in\mathcal{D}$，$y_i=g(\pmb{x_i})$，$\mathcal{D}$表示训练数据的分布。在训练数据和测试数据同分布的情况下，$\mathcal{D}=\mathcal{X}$；而在评估泛化能力时，$\mathcal{D} $是$\mathcal{X}$的子集。一个模型的泛化能力可以用泛化误差评估：设$f$为模型在训练数据上得到的函数，$l$为任意损失函数，则泛化误差定义为$\mathbb{E}_{\pmb{x}\sim \mathcal{X} \setminus \mathcal{D}}[l(f(\pmb{x}), g(\pmb{x}))]$
图神经网络GNNs是在MLPs基础上定义的网络。具体来说，初始顶点表征为$\pmb{h}_u^{(0)}=\pmb{x}_u$。在第$k={1..K}$层，顶点表征更新公式为
$$\begin{aligned}\pmb{h}_u^{(k)}&=\sum_{v\in\mathcal{N}(u)}\text{MLP}^{(k)}(\pmb{h}_u^{(k-1)},\pmb{h}_v^{(k-1)},\pmb{w}_{(v,u)}) \\ \pmb{h}_G&=\text{MLP}^{(K+1)}(\sum_{u\in G}\pmb{h}_u^{(K)})\end{aligned}$$
其中$\pmb{h}_u^{(k)}$表示第$k$层GNN输出的顶点$u$的表征，$\pmb{h}_G$表示整张图的表征。$\pmb{h}_u^{(k)}$的计算过程称为聚合，$\pmb{h}_G$的计算过程称为读出。以往研究大多使用$\text{sum}$聚合与$\text{sum}$读出，而本文指出替换为另外的函数能够提升泛化性能。
前馈网络MLPs如何泛化 ReLU MLPs的线性泛化 作者用下图呈现MLPs的泛化方式。灰色表示MLPs要学习的真实函数，蓝色和黑色分别表示模型在训练集和测试集上的预测。可以看到模型可以拟合训练集上的非线性函数，但脱离训练集后迅速变为线性函数。用数字来说，脱离训练集后MLPs预测的决定系数大于$0.99$。
定理1（线性泛化）：假设在NTK机制下使用均方误差训练了一个两层MLP：$f:\mathbb{R}^d\to\mathbb{R}$。对于任意方向$\pmb{v}\in\mathbb{R}^d$，令$\pmb{x}_0=t\pmb{v}$，那么当$t\to\infty$时，$f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)\to\beta_vh$对任意的$h>0$成立，$\beta_v$是常数。进一步地，给定$\epsilon>0$，对于$t=O(\frac{1}{\epsilon})$，$|\frac{f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)}{h}-\beta_v|<\epsilon$。
定理1说明了在训练数据集以外，ReLU MLPs可以拟合几乎线性的函数。对于二次函数（$\pmb{x}^TA\pmb{x}$）、余弦函数($\sum_{i=1}^d\cos(2\pi\cdot\pmb{x}^{(i)})$)、根次函数（$\sum_{i=1}^d\sqrt{\pmb{x}^{(i)}}$）等，ReLU MLPs不能泛化。 在合适的超参数下，MLPs可以正确地泛化L1范数，与定理1一致。如下图所示。
ReLU MLPs什么时候一定（provably）可以泛化 尽管上图显示MLPs对于线性函数可以较好地泛化，但这需要一定的条件，即训练数据集的分布必须足够“多样”。下面的引理1指出只需要$2d$条认真挑选的数据就可以实现ReLU MLPs的线性泛化。
引理1：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设数据集$\{\pmb{x}_i\}_{i=1}^n$包含正交基$\{\hat{\pmb{x}}_i\}_{i=1}^d$和$\{-\hat{\pmb{x}}_i\}_{i=1}^d$。若使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，那么$f({\pmb{x}})={\pmb{\beta}}^T{\pmb{x}}$对任意的${\pmb{x}}\in\mathbb{R}^d$成立。
然而，仔细挑选出$2d$条符合条件的样本并不容易。下面的定理2基于更现实的场景，指出只要训练数据的分布包含所有的方向（例如一个包含原点的超球），那么在足够的训练数据量下MLP能够收敛到线性函数。
定理2（泛化的条件）：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设$\{\pmb{x}_i\}_{i=1}^n$从域$\mathcal{D}$中采样，其中$\mathcal{D}$包含一个连通的子集$S$，满足对任意非零向量$\pmb{w}\in\mathbb{R}^d$，存在$k>0$使得$k\pmb{w}\in S$。若在NTK机制下，使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，$f(\pmb{x})\xrightarrow{p}\pmb{\beta}^T\pmb{x}$在$n\to\infty$时成立，即$f$依概率收敛到$g$。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yliuhz.github.io/blogs/posts/"},{"@type":"ListItem","position":3,"name":"Generalization of GNNs and MLPs","item":"https://yliuhz.github.io/blogs/posts/gnn-ood/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generalization of GNNs and MLPs","name":"Generalization of GNNs and MLPs","description":"本文是 ( Citation: Xu,\u0026#32;Zhang \u0026amp; al.,\u0026#32;2021 Xu,\u0026#32; K.,\u0026#32; Zhang,\u0026#32; M.,\u0026#32; Li,\u0026#32; J.,\u0026#32; Du,\u0026#32; S.,\u0026#32; Kawarabayashi,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Jegelka,\u0026#32; S. \u0026#32; (2021). \u0026#32;How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. https://doi.org/10.48550/arXiv.2009.11848 ) 的论文解读。OpenReview显示这篇论文是ICLR2021的Oral论文（前5%）。\n引言 人类具有泛化性，例如学会算术后可以应用到任意大的数字。对于神经网络而言，前馈网络（也叫多层感知机，MLPs）在学习简单的多项式函数时就不能很好地泛化了。然而，基于MLP的图神经网络（GNNs）却在近期的一些任务上表现出较好的泛化性，包括预测物理系统的演进规律，学习图论算法，解决数学公式等。 粗略地分析可能会觉得神经网络可以在训练分布以外的数据上有任意不确定的表现，但是现实中的神经网络大多是用梯度下降训练的，这就导致其泛化性能有规律可以分析。作者使用\u0026quot;神经切线核\u0026quot;（neural tangent kernel，NTK）工具进行分析。\n本文的第一个结论是使用梯度下降训练的MLPs会收敛到任意方向线性的函数，因此MLPs在大多数非线性任务上无法泛化。\n接着本文将分析延伸到基于MLP的GNNs，得到第二个结论：使用线性对齐简化目标函数使得基于MLP的GNNs在非线性任务上能够泛化，即将非线性部分提前集成在模型的结构（如GNN的聚合和读出函数）或在输入的表征向量中（使用无监督方法将输入特征转化为表征向量）。\n前置知识 设$\\mathcal{X}$表示数据（向量或图）的域。任务是学习一个函数$g:\\mathcal{X}\\to \\mathbb{R}$，其中训练数据$\\{(\\pmb{x}_i,y_i)\\}\\in\\mathcal{D}$，$y_i=g(\\pmb{x_i})$，$\\mathcal{D}$表示训练数据的分布。在训练数据和测试数据同分布的情况下，$\\mathcal{D}=\\mathcal{X}$；而在评估泛化能力时，$\\mathcal{D} $是$\\mathcal{X}$的子集。一个模型的泛化能力可以用泛化误差评估：设$f$为模型在训练数据上得到的函数，$l$为任意损失函数，则泛化误差定义为$\\mathbb{E}_{\\pmb{x}\\sim \\mathcal{X} \\setminus \\mathcal{D}}[l(f(\\pmb{x}), g(\\pmb{x}))]$\n图神经网络GNNs是在MLPs基础上定义的网络。具体来说，初始顶点表征为$\\pmb{h}_u^{(0)}=\\pmb{x}_u$。在第$k={1..K}$层，顶点表征更新公式为\n$$\\begin{aligned}\\pmb{h}_u^{(k)}\u0026amp;=\\sum_{v\\in\\mathcal{N}(u)}\\text{MLP}^{(k)}(\\pmb{h}_u^{(k-1)},\\pmb{h}_v^{(k-1)},\\pmb{w}_{(v,u)}) \\\\ \\pmb{h}_G\u0026amp;=\\text{MLP}^{(K+1)}(\\sum_{u\\in G}\\pmb{h}_u^{(K)})\\end{aligned}$$\n其中$\\pmb{h}_u^{(k)}$表示第$k$层GNN输出的顶点$u$的表征，$\\pmb{h}_G$表示整张图的表征。$\\pmb{h}_u^{(k)}$的计算过程称为聚合，$\\pmb{h}_G$的计算过程称为读出。以往研究大多使用$\\text{sum}$聚合与$\\text{sum}$读出，而本文指出替换为另外的函数能够提升泛化性能。\n前馈网络MLPs如何泛化 ReLU MLPs的线性泛化 作者用下图呈现MLPs的泛化方式。灰色表示MLPs要学习的真实函数，蓝色和黑色分别表示模型在训练集和测试集上的预测。可以看到模型可以拟合训练集上的非线性函数，但脱离训练集后迅速变为线性函数。用数字来说，脱离训练集后MLPs预测的决定系数大于$0.99$。\n定理1（线性泛化）：假设在NTK机制下使用均方误差训练了一个两层MLP：$f:\\mathbb{R}^d\\to\\mathbb{R}$。对于任意方向$\\pmb{v}\\in\\mathbb{R}^d$，令$\\pmb{x}_0=t\\pmb{v}$，那么当$t\\to\\infty$时，$f(\\pmb{x}_0+h\\pmb{v})-f(\\pmb{x}_0)\\to\\beta_vh$对任意的$h\u0026gt;0$成立，$\\beta_v$是常数。进一步地，给定$\\epsilon\u0026gt;0$，对于$t=O(\\frac{1}{\\epsilon})$，$|\\frac{f(\\pmb{x}_0+h\\pmb{v})-f(\\pmb{x}_0)}{h}-\\beta_v|\u0026lt;\\epsilon$。\n定理1说明了在训练数据集以外，ReLU MLPs可以拟合几乎线性的函数。对于二次函数（$\\pmb{x}^TA\\pmb{x}$）、余弦函数($\\sum_{i=1}^d\\cos(2\\pi\\cdot\\pmb{x}^{(i)})$)、根次函数（$\\sum_{i=1}^d\\sqrt{\\pmb{x}^{(i)}}$）等，ReLU MLPs不能泛化。 在合适的超参数下，MLPs可以正确地泛化L1范数，与定理1一致。如下图所示。\nReLU MLPs什么时候一定（provably）可以泛化 尽管上图显示MLPs对于线性函数可以较好地泛化，但这需要一定的条件，即训练数据集的分布必须足够“多样”。下面的引理1指出只需要$2d$条认真挑选的数据就可以实现ReLU MLPs的线性泛化。\n引理1：令$g(\\pmb{x})=\\pmb{\\beta}^T\\pmb{x}$表示待拟合的目标函数，$\\pmb{\\beta}\\in\\mathbb{R}^d$。假设数据集$\\{\\pmb{x}_i\\}_{i=1}^n$包含正交基$\\{\\hat{\\pmb{x}}_i\\}_{i=1}^d$和$\\{-\\hat{\\pmb{x}}_i\\}_{i=1}^d$。若使用均方误差在$\\{({\\pmb{x}}_i,y_i)\\}_{i=1}^n$上训练一个两层ReLU MLP，那么$f({\\pmb{x}})={\\pmb{\\beta}}^T{\\pmb{x}}$对任意的${\\pmb{x}}\\in\\mathbb{R}^d$成立。\n然而，仔细挑选出$2d$条符合条件的样本并不容易。下面的定理2基于更现实的场景，指出只要训练数据的分布包含所有的方向（例如一个包含原点的超球），那么在足够的训练数据量下MLP能够收敛到线性函数。\n定理2（泛化的条件）：令$g(\\pmb{x})=\\pmb{\\beta}^T\\pmb{x}$表示待拟合的目标函数，$\\pmb{\\beta}\\in\\mathbb{R}^d$。假设$\\{\\pmb{x}_i\\}_{i=1}^n$从域$\\mathcal{D}$中采样，其中$\\mathcal{D}$包含一个连通的子集$S$，满足对任意非零向量$\\pmb{w}\\in\\mathbb{R}^d$，存在$k\u0026gt;0$使得$k\\pmb{w}\\in S$。若在NTK机制下，使用均方误差在$\\{({\\pmb{x}}_i,y_i)\\}_{i=1}^n$上训练一个两层ReLU MLP，$f(\\pmb{x})\\xrightarrow{p}\\pmb{\\beta}^T\\pmb{x}$在$n\\to\\infty$时成立，即$f$依概率收敛到$g$。","keywords":[],"articleBody":"本文是 ( Citation: Xu, Zhang \u0026 al., 2021 Xu, K., Zhang, M., Li, J., Du, S., Kawarabayashi, K. \u0026 Jegelka, S. (2021). How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. https://doi.org/10.48550/arXiv.2009.11848 ) 的论文解读。OpenReview显示这篇论文是ICLR2021的Oral论文（前5%）。\n引言 人类具有泛化性，例如学会算术后可以应用到任意大的数字。对于神经网络而言，前馈网络（也叫多层感知机，MLPs）在学习简单的多项式函数时就不能很好地泛化了。然而，基于MLP的图神经网络（GNNs）却在近期的一些任务上表现出较好的泛化性，包括预测物理系统的演进规律，学习图论算法，解决数学公式等。 粗略地分析可能会觉得神经网络可以在训练分布以外的数据上有任意不确定的表现，但是现实中的神经网络大多是用梯度下降训练的，这就导致其泛化性能有规律可以分析。作者使用\"神经切线核\"（neural tangent kernel，NTK）工具进行分析。\n本文的第一个结论是使用梯度下降训练的MLPs会收敛到任意方向线性的函数，因此MLPs在大多数非线性任务上无法泛化。\n接着本文将分析延伸到基于MLP的GNNs，得到第二个结论：使用线性对齐简化目标函数使得基于MLP的GNNs在非线性任务上能够泛化，即将非线性部分提前集成在模型的结构（如GNN的聚合和读出函数）或在输入的表征向量中（使用无监督方法将输入特征转化为表征向量）。\n前置知识 设$\\mathcal{X}$表示数据（向量或图）的域。任务是学习一个函数$g:\\mathcal{X}\\to \\mathbb{R}$，其中训练数据$\\{(\\pmb{x}_i,y_i)\\}\\in\\mathcal{D}$，$y_i=g(\\pmb{x_i})$，$\\mathcal{D}$表示训练数据的分布。在训练数据和测试数据同分布的情况下，$\\mathcal{D}=\\mathcal{X}$；而在评估泛化能力时，$\\mathcal{D} $是$\\mathcal{X}$的子集。一个模型的泛化能力可以用泛化误差评估：设$f$为模型在训练数据上得到的函数，$l$为任意损失函数，则泛化误差定义为$\\mathbb{E}_{\\pmb{x}\\sim \\mathcal{X} \\setminus \\mathcal{D}}[l(f(\\pmb{x}), g(\\pmb{x}))]$\n图神经网络GNNs是在MLPs基础上定义的网络。具体来说，初始顶点表征为$\\pmb{h}_u^{(0)}=\\pmb{x}_u$。在第$k={1..K}$层，顶点表征更新公式为\n$$\\begin{aligned}\\pmb{h}_u^{(k)}\u0026=\\sum_{v\\in\\mathcal{N}(u)}\\text{MLP}^{(k)}(\\pmb{h}_u^{(k-1)},\\pmb{h}_v^{(k-1)},\\pmb{w}_{(v,u)}) \\\\ \\pmb{h}_G\u0026=\\text{MLP}^{(K+1)}(\\sum_{u\\in G}\\pmb{h}_u^{(K)})\\end{aligned}$$\n其中$\\pmb{h}_u^{(k)}$表示第$k$层GNN输出的顶点$u$的表征，$\\pmb{h}_G$表示整张图的表征。$\\pmb{h}_u^{(k)}$的计算过程称为聚合，$\\pmb{h}_G$的计算过程称为读出。以往研究大多使用$\\text{sum}$聚合与$\\text{sum}$读出，而本文指出替换为另外的函数能够提升泛化性能。\n前馈网络MLPs如何泛化 ReLU MLPs的线性泛化 作者用下图呈现MLPs的泛化方式。灰色表示MLPs要学习的真实函数，蓝色和黑色分别表示模型在训练集和测试集上的预测。可以看到模型可以拟合训练集上的非线性函数，但脱离训练集后迅速变为线性函数。用数字来说，脱离训练集后MLPs预测的决定系数大于$0.99$。\n定理1（线性泛化）：假设在NTK机制下使用均方误差训练了一个两层MLP：$f:\\mathbb{R}^d\\to\\mathbb{R}$。对于任意方向$\\pmb{v}\\in\\mathbb{R}^d$，令$\\pmb{x}_0=t\\pmb{v}$，那么当$t\\to\\infty$时，$f(\\pmb{x}_0+h\\pmb{v})-f(\\pmb{x}_0)\\to\\beta_vh$对任意的$h\u003e0$成立，$\\beta_v$是常数。进一步地，给定$\\epsilon\u003e0$，对于$t=O(\\frac{1}{\\epsilon})$，$|\\frac{f(\\pmb{x}_0+h\\pmb{v})-f(\\pmb{x}_0)}{h}-\\beta_v|\u003c\\epsilon$。\n定理1说明了在训练数据集以外，ReLU MLPs可以拟合几乎线性的函数。对于二次函数（$\\pmb{x}^TA\\pmb{x}$）、余弦函数($\\sum_{i=1}^d\\cos(2\\pi\\cdot\\pmb{x}^{(i)})$)、根次函数（$\\sum_{i=1}^d\\sqrt{\\pmb{x}^{(i)}}$）等，ReLU MLPs不能泛化。 在合适的超参数下，MLPs可以正确地泛化L1范数，与定理1一致。如下图所示。\nReLU MLPs什么时候一定（provably）可以泛化 尽管上图显示MLPs对于线性函数可以较好地泛化，但这需要一定的条件，即训练数据集的分布必须足够“多样”。下面的引理1指出只需要$2d$条认真挑选的数据就可以实现ReLU MLPs的线性泛化。\n引理1：令$g(\\pmb{x})=\\pmb{\\beta}^T\\pmb{x}$表示待拟合的目标函数，$\\pmb{\\beta}\\in\\mathbb{R}^d$。假设数据集$\\{\\pmb{x}_i\\}_{i=1}^n$包含正交基$\\{\\hat{\\pmb{x}}_i\\}_{i=1}^d$和$\\{-\\hat{\\pmb{x}}_i\\}_{i=1}^d$。若使用均方误差在$\\{({\\pmb{x}}_i,y_i)\\}_{i=1}^n$上训练一个两层ReLU MLP，那么$f({\\pmb{x}})={\\pmb{\\beta}}^T{\\pmb{x}}$对任意的${\\pmb{x}}\\in\\mathbb{R}^d$成立。\n然而，仔细挑选出$2d$条符合条件的样本并不容易。下面的定理2基于更现实的场景，指出只要训练数据的分布包含所有的方向（例如一个包含原点的超球），那么在足够的训练数据量下MLP能够收敛到线性函数。\n定理2（泛化的条件）：令$g(\\pmb{x})=\\pmb{\\beta}^T\\pmb{x}$表示待拟合的目标函数，$\\pmb{\\beta}\\in\\mathbb{R}^d$。假设$\\{\\pmb{x}_i\\}_{i=1}^n$从域$\\mathcal{D}$中采样，其中$\\mathcal{D}$包含一个连通的子集$S$，满足对任意非零向量$\\pmb{w}\\in\\mathbb{R}^d$，存在$k\u003e0$使得$k\\pmb{w}\\in S$。若在NTK机制下，使用均方误差在$\\{({\\pmb{x}}_i,y_i)\\}_{i=1}^n$上训练一个两层ReLU MLP，$f(\\pmb{x})\\xrightarrow{p}\\pmb{\\beta}^T\\pmb{x}$在$n\\to\\infty$时成立，即$f$依概率收敛到$g$。\n定理2说明了为什么数据集中“虚假”的相关性（真实不应存在的相关性）会损害模型的泛化性能，补充了因果推理的论点。例如，如果人工收集的训练图片中只有在沙漠中的骆驼，这里骆驼和沙漠就是数据集不够“多样”导致的虚假相关性，实际上骆驼还生活在草原等多种环境上。那么此时定理2的条件不再满足，模型也可能因此不能很好地泛化。\n总的来说，定理1指出MLPs对于大多数非线性函数不能泛化，定理2指出MLPs当训练数据足够多样时能够在线性目标函数下泛化。\n使用其他激活函数的MLPs 以上讨论都基于使用ReLU激活函数的MLPs。除了ReLU，还有$\\tanh(x),\\cos(x),x^2$等激活函数。作者发现，在激活函数和待拟合的目标函数相近时，MLPs的泛化性能较好。\n图神经网络GNNs如何泛化 以上讨论说明了MLPs在非线性任务上难以泛化。然而基于MLPs的GNNs却在一些非线性任务上表现出良好的泛化性，例如直观物理（intuitive Physics）、图论算法（graph algorithms）、符号数学（symbolic mathematics）等。\n一个假设：线性对齐辅助了GNNs的泛化 GNNs可以被用来解决最短路径问题。人们发现在GNN的聚合函数中使用最小$\\min$函数后，训练后的GNN可以较好地泛化到比训练集更大的图上：\n$$\\pmb{h}_u^{(k)}=\\min_{v\\in\\mathcal{N}(u)}\\text{MLP}^{(k)}(\\pmb{h}_u^{(k-1)},\\pmb{h}_v^{(k-1)},\\pmb{w}_{(v,u)})$$\n另一方面，传统的最短路问题可以通过Bellman-Fold（BF）算法解决：\n$$d[k][u]=\\min_{v\\in\\mathcal{N}(u)}d[k-1][v]+\\pmb{w}(v,u)$$\n其中$\\pmb{w}(v,u)$表示边$(v,u)$的权重，$d[k][u]$表示$k$步以内到达节点$u$的最短距离。可以发现BF算法的更新式子可以很容易地与GNN的更新式子对齐：只需令MLP学习一个线性函数$d[k-1][v]+\\pmb{w}(v,u)$即可。由于MLPs可以较好地对线性目标函数泛化，所以GNNs可以在计算最短路径问题上良好地泛化。 反之，如果在GNN的聚合函数中使用$\\text{sum}$或其他函数，那么MLP就需要学习一个非线性目标函数，导致其无法泛化（定理1），进而导致GNN无法泛化。\n由上述最短路问题推广到其他问题，作者发现许多GNNs泛化性能好的问题都可以用动态规划（dynamic programming, DP）解决，而DP中的迭代更新式子与GNNs的特征聚合函数很相似。\n定义3：动态规划方法可以被形式化为：\n$$\\text{Answer}[k][s]=\\text{DP-Update}(\\{\\text{Answer}[k-1][s’],s’=1,\\cdots,n\\})$$\n其中$\\text{Answer}[k][s]$表示第$k$次迭代、状态为$s$的子问题的解。\n假设1（线性算法对齐）：令$f:\\mathcal{X}\\to\\mathbb{R}$表示目标函数。$\\mathcal{N}$是一个神经网络，包含$m$个MLP模块。假设存在$m$个线性函数$\\{g_i\\}_{i=1}^m$，使得替换$\\mathcal{N}$中的MLP后，$\\mathcal{N}$能够模拟$f$。那么对于给定的$\\epsilon\u003e0$，存在$\\{(x_i,f(x_i))\\}_{i=1}^n\\subset\\mathcal{D}\\subsetneq\\mathcal{X}$，使得在其上通过梯度下降和均方误差损失训练的$\\mathcal{N}$得到的$\\hat{f}$满足$\\parallel\\hat{f}-f\\parallel\u003c\\epsilon$。\n作者认为，模型的线性对齐不局限于GNNs。人们可以将非线性的操作集成在模型的结构或者输入的表征向量中，这样在梯度下降训练时，模型只需要学习一个线性函数，从而提高了泛化能力。 GNNs学习DP的迭代表达式是一个例子，另外的例子是在算术任务中使用log-and-exp编码来提高乘法的泛化性。 另外，在一些任务中可能变换输入表征更容易。具体来说，目标函数$f$可以拆解为\n$$f=g\\circ h$$\n其中$h$是表征向量，$g$是更简单的目标函数，例如线性函数，这样模型就更容易学习和泛化。对于表征向量$h$，可以使用领域知识；或者表征学习方法，在测试域$\\mathcal{X}\\setminus\\mathcal{D}$进行无监督地表征学习。例如，在自然语言学习中，预训练表征和使用领域知识的特征转化可以帮助模型在不同语种之间泛化。在计量经济学中，人类对于本质因素或特征的判断（领域知识）尤其重要，因为金融市场经常需要模型进行泛化外推。\n理论推导和实验结果 作者在3个DP任务上验证假设：最大度、最短路和$n$-body问题。首先，考虑计算图的最大度，可以通过1步DP解决。作为定理1的一个推论，使用$\\text{sum}$聚合函数的GNNs无法在该问题上良好泛化。\n推论1：使用$\\text{sum}$聚合和$\\text{sum}$读出的GNNs在最大度问题上不能泛化。\n利用假设1，为了实现线性对齐，将GNN的读出函数修改为$\\max$函数。下面的定理3说明修改后的GNN能够良好地泛化。\n定理3（GNNs的泛化）：假设图中所有的顶点有相同的特征向量。令$g$和$g’$分别表示最大度和最小度函数。令$\\{(G_i,g(G_i))\\}_{i=1}^n$表示训练集。如果$\\{(g(G_i),g’(G_i),g(G_i)\\cdot N_i^{\\max},g’(G_i)\\cdot N_i^{\\min})\\}_{i=1}^n$通过线性变换能表示$\\mathbb{R}^4$（是$\\mathbb{R}^4$的一组基），其中$N_i^{\\max}$和$N_i^{\\min}$分别表示$G_i$中具有最大度和最小度的顶点的个数，那么一个一层的使用$\\max$读出函数的GNN，通过在$\\{(G_i,g(G_i))\\}_{i=1}^n$上使用均方误差损失和NTK机制训练，能够学习到目标函数$g$。\n定理3中的条件与定理2类似，都是在保证训练集的多样性，只不过这里是用图的结构（即最大度）的多样性，而定理2中是用数据集的“方向”。如果训练集中所有的图有相同的最大度或者最小度，例如训练集仅仅属于path、$C$-regular graphs（度为$C$的正则图）、cycle、ladder四种类型之一，那么定理3的条件就遭到破坏，相应的GNN也不能保证学习到目标函数。\n作者通过实验验证定理3和推论1。作者发现在最大度任务上，使用$\\max$读出函数的GNN确实比使用$\\text{sum}$读出函数的GNN的泛化效果要好；在最短路任务上，使用$\\min$读出函数的GNN也确实比使用$\\text{sum}$读出函数的GNN要好（图(a)）。 此外，在图(b)中，作者考虑第3个任务：$n$-body问题，即预测重力系统中$n$个物体随时间演变的规律。GNN的输入是完全图，每个顶点代表一个物体。顶点的特征由物体的质量$m_u$、在$t$时刻的位置$\\pmb{x}_u^{(t)}$和速度$\\pmb{v}_u^{(t)}$拼接而来。边特征设定为$0$。GNN的输出是在$t+1$时刻每个物体$u$的速度。真实的速度$f(G;u)$可以近似表示为\n$$ \\begin{aligned} f(G;u)\u0026\\approx\\pmb{v}_u^t+\\pmb{a}_u^t\\cdot\\text{d}t \\\\ \\pmb{a}_u^t \u0026= C\\cdot\\sum_{v\\neq u}\\frac{m_v}{\\parallel\\pmb{x}_u^t-\\pmb{x}_v^t\\parallel_2^3}\\cdot(\\pmb{x}_v^t-\\pmb{x}_u^t) \\end{aligned} $$\n其中$C$是常数。为了学习$f$，GNN中的MLP需要学习一个非线性函数，因此难以泛化。为了简化MLP的学习任务，使用表征$h(G)$替换输入的顶点特征。具体来说，将边特征由$0$替换为\n$$\\pmb{w}_{(u,v)}^{(t)}=m_v\\cdot\\frac{\\pmb{x}_v^{(t)}-\\pmb{x}_u^{(t)}}{\\parallel\\pmb{x}_u^t-\\pmb{x}_v^t\\parallel_2^3}$$\n这样MLP只需要学习一个线性函数，从而提高了泛化能力。如下图(b)所示。\n作者还发现训练图的结构也会对GNN的泛化能力造成影响，不同的任务中GNN更“喜欢”不同结构的训练图。如下图所示。\n本文与其他分布外问题设置的联系 领域自适应（domain adaptation）研究如何泛化到一个特定的目标域。典型的方法是在训练中加入目标域的无标签样本。\n自监督学习（self-supervised learning）研究如何在无标签样本上进行训练。\n不变模型（invariant models）研究如何在多种训练分布之间学习内在不变的本质特征。\n分布鲁棒性（distributional robustness）研究在数据分布上进行微小的对抗扰动，并保证模型在扰动后依然表现良好。\n总结 本文的核心思想是通过改进模型结构或者输入信息简化目标函数，从而提升泛化性能。 对于实际场景，真实的目标函数很可能是复杂且未知的，所以通过修改模型的结构达到提高泛化能力的方法可能不太容易。使用无监督表征方法改进输入的特征向量是不错的方法。\n","wordCount":"138","inLanguage":"en","datePublished":"2023-06-26T20:08:51+08:00","dateModified":"2023-06-26T20:08:51+08:00","author":{"@type":"Person","name":"Yue"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yliuhz.github.io/blogs/posts/gnn-ood/"},"publisher":{"@type":"Organization","name":"LIU Yue's blogs","logo":{"@type":"ImageObject","url":"https://yliuhz.github.io/blogs/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yliuhz.github.io/blogs accesskey=h title="LIU Yue's blogs (Alt + H)">LIU Yue's blogs</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yliuhz.github.io/blogs/archives title=Archive><span>Archive</span></a></li><li><a href=https://yliuhz.github.io/blogs/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://yliuhz.github.io/blogs/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yliuhz.github.io/blogs>Home</a>&nbsp;»&nbsp;<a href=https://yliuhz.github.io/blogs/posts/>Posts</a></div><h1 class=post-title>Generalization of GNNs and MLPs</h1><div class=post-meta><span title='2023-06-26 20:08:51 +0800 HKT'>June 26, 2023</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Yue&nbsp;|&nbsp;<a href=https://github.com/yliuhz/blogs/tree/main/content/posts/gnn-ood.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%95%e8%a8%80 aria-label=引言>引言</a></li><li><a href=#%e5%89%8d%e7%bd%ae%e7%9f%a5%e8%af%86 aria-label=前置知识>前置知识</a></li><li><a href=#%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9cmlps%e5%a6%82%e4%bd%95%e6%b3%9b%e5%8c%96 aria-label=前馈网络MLPs如何泛化>前馈网络MLPs如何泛化</a><ul><li><a href=#relu-mlps%e7%9a%84%e7%ba%bf%e6%80%a7%e6%b3%9b%e5%8c%96 aria-label="ReLU MLPs的线性泛化">ReLU MLPs的线性泛化</a></li><li><a href=#relu-mlps%e4%bb%80%e4%b9%88%e6%97%b6%e5%80%99%e4%b8%80%e5%ae%9aprovably%e5%8f%af%e4%bb%a5%e6%b3%9b%e5%8c%96 aria-label="ReLU MLPs什么时候一定（provably）可以泛化">ReLU MLPs什么时候一定（provably）可以泛化</a></li><li><a href=#%e4%bd%bf%e7%94%a8%e5%85%b6%e4%bb%96%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%e7%9a%84mlps aria-label=使用其他激活函数的MLPs>使用其他激活函数的MLPs</a></li></ul></li><li><a href=#%e5%9b%be%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cgnns%e5%a6%82%e4%bd%95%e6%b3%9b%e5%8c%96 aria-label=图神经网络GNNs如何泛化>图神经网络GNNs如何泛化</a><ul><li><a href=#%e4%b8%80%e4%b8%aa%e5%81%87%e8%ae%be%e7%ba%bf%e6%80%a7%e5%af%b9%e9%bd%90%e8%be%85%e5%8a%a9%e4%ba%86gnns%e7%9a%84%e6%b3%9b%e5%8c%96 aria-label=一个假设：线性对齐辅助了GNNs的泛化>一个假设：线性对齐辅助了GNNs的泛化</a></li><li><a href=#%e7%90%86%e8%ae%ba%e6%8e%a8%e5%af%bc%e5%92%8c%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c aria-label=理论推导和实验结果>理论推导和实验结果</a></li></ul></li><li><a href=#%e6%9c%ac%e6%96%87%e4%b8%8e%e5%85%b6%e4%bb%96%e5%88%86%e5%b8%83%e5%a4%96%e9%97%ae%e9%a2%98%e8%ae%be%e7%bd%ae%e7%9a%84%e8%81%94%e7%b3%bb aria-label=本文与其他分布外问题设置的联系>本文与其他分布外问题设置的联系</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></div></details></div><div class=post-content><p>本文是
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#18wo6er1h><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Keyulu"><span itemprop=familyName>Xu</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Mozhi"><span itemprop=familyName>Zhang</span></span>
<em>& al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xu</span>, <meta itemprop=givenName content="Keyulu">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Mozhi">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Jingling">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Du</span>, <meta itemprop=givenName content="Simon S.">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kawarabayashi</span>, <meta itemprop=givenName content="Ken-ichi">K.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jegelka</span>, <meta itemprop=givenName content="Stefanie">S.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks</span>.
<a href=https://doi.org/10.48550/arXiv.2009.11848 itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2009.11848</a></span>
</span></span>)</span>
的论文解读。OpenReview显示这篇论文是ICLR2021的Oral论文（前5%）。</p><h2 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h2><p>人类具有泛化性，例如学会算术后可以应用到任意大的数字。对于神经网络而言，前馈网络（也叫多层感知机，MLPs）在学习简单的多项式函数时就不能很好地泛化了。然而，基于MLP的图神经网络（GNNs）却在近期的一些任务上表现出较好的泛化性，包括预测物理系统的演进规律，学习图论算法，解决数学公式等。
粗略地分析可能会觉得神经网络可以在训练分布以外的数据上有任意不确定的表现，但是现实中的神经网络大多是用梯度下降训练的，这就导致其泛化性能有规律可以分析。作者使用"神经切线核"（neural tangent kernel，NTK）工具进行分析。</p><p>本文的第一个结论是使用梯度下降训练的MLPs会收敛到任意方向线性的函数，因此<strong>MLPs在大多数非线性任务上无法泛化</strong>。</p><p>接着本文将分析延伸到基于MLP的GNNs，得到第二个结论：<strong>使用线性对齐简化目标函数使得基于MLP的GNNs在非线性任务上能够泛化，即将非线性部分提前集成在模型的结构（如GNN的聚合和读出函数）或在输入的表征向量中（使用无监督方法将输入特征转化为表征向量）。</strong></p><h2 id=前置知识>前置知识<a hidden class=anchor aria-hidden=true href=#前置知识>#</a></h2><p>设$\mathcal{X}$表示数据（向量或图）的域。任务是学习一个函数$g:\mathcal{X}\to \mathbb{R}$，其中训练数据$\{(\pmb{x}_i,y_i)\}\in\mathcal{D}$，$y_i=g(\pmb{x_i})$，$\mathcal{D}$表示训练数据的分布。在训练数据和测试数据同分布的情况下，$\mathcal{D}=\mathcal{X}$；而在评估泛化能力时，$\mathcal{D}
$是$\mathcal{X}$的子集。一个模型的泛化能力可以用<strong>泛化误差</strong>评估：设$f$为模型在训练数据上得到的函数，$l$为任意损失函数，则泛化误差定义为$\mathbb{E}_{\pmb{x}\sim \mathcal{X} \setminus \mathcal{D}}[l(f(\pmb{x}), g(\pmb{x}))]$</p><p>图神经网络GNNs是在MLPs基础上定义的网络。具体来说，初始顶点表征为$\pmb{h}_u^{(0)}=\pmb{x}_u$。在第$k={1..K}$层，顶点表征更新公式为</p><p>$$\begin{aligned}\pmb{h}_u^{(k)}&=\sum_{v\in\mathcal{N}(u)}\text{MLP}^{(k)}(\pmb{h}_u^{(k-1)},\pmb{h}_v^{(k-1)},\pmb{w}_{(v,u)}) \\
\pmb{h}_G&=\text{MLP}^{(K+1)}(\sum_{u\in G}\pmb{h}_u^{(K)})\end{aligned}$$</p><p>其中$\pmb{h}_u^{(k)}$表示第$k$层GNN输出的顶点$u$的表征，$\pmb{h}_G$表示整张图的表征。$\pmb{h}_u^{(k)}$的计算过程称为聚合，$\pmb{h}_G$的计算过程称为读出。以往研究大多使用$\text{sum}$聚合与$\text{sum}$读出，而本文指出替换为另外的函数能够提升泛化性能。</p><h2 id=前馈网络mlps如何泛化>前馈网络MLPs如何泛化<a hidden class=anchor aria-hidden=true href=#前馈网络mlps如何泛化>#</a></h2><h3 id=relu-mlps的线性泛化>ReLU MLPs的线性泛化<a hidden class=anchor aria-hidden=true href=#relu-mlps的线性泛化>#</a></h3><p>作者用下图呈现MLPs的泛化方式。灰色表示MLPs要学习的真实函数，蓝色和黑色分别表示模型在训练集和测试集上的预测。可以看到模型可以拟合训练集上的非线性函数，但脱离训练集后迅速变为线性函数。用数字来说，脱离训练集后MLPs预测的决定系数大于$0.99$。</p><p><strong>定理1</strong>（线性泛化）：假设在NTK机制下使用均方误差训练了一个两层MLP：$f:\mathbb{R}^d\to\mathbb{R}$。对于任意方向$\pmb{v}\in\mathbb{R}^d$，令$\pmb{x}_0=t\pmb{v}$，那么当$t\to\infty$时，$f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)\to\beta_vh$对任意的$h>0$成立，$\beta_v$是常数。进一步地，给定$\epsilon>0$，对于$t=O(\frac{1}{\epsilon})$，$|\frac{f(\pmb{x}_0+h\pmb{v})-f(\pmb{x}_0)}{h}-\beta_v|&lt;\epsilon$。</p><p>定理1说明了在训练数据集以外，ReLU MLPs可以拟合几乎线性的函数。对于二次函数（$\pmb{x}^TA\pmb{x}$）、余弦函数($\sum_{i=1}^d\cos(2\pi\cdot\pmb{x}^{(i)})$)、根次函数（$\sum_{i=1}^d\sqrt{\pmb{x}^{(i)}}$）等，ReLU MLPs不能泛化。
在合适的超参数下，MLPs可以正确地泛化L1范数，与定理1一致。如下图所示。</p><h3 id=relu-mlps什么时候一定provably可以泛化>ReLU MLPs什么时候一定（provably）可以泛化<a hidden class=anchor aria-hidden=true href=#relu-mlps什么时候一定provably可以泛化>#</a></h3><p>尽管上图显示MLPs对于线性函数可以较好地泛化，但这需要一定的条件，即训练数据集的分布必须足够“多样”。下面的引理1指出只需要$2d$条认真挑选的数据就可以实现ReLU MLPs的线性泛化。</p><p><strong>引理1</strong>：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设数据集$\{\pmb{x}_i\}_{i=1}^n$包含正交基$\{\hat{\pmb{x}}_i\}_{i=1}^d$和$\{-\hat{\pmb{x}}_i\}_{i=1}^d$。若使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，那么$f({\pmb{x}})={\pmb{\beta}}^T{\pmb{x}}$对任意的${\pmb{x}}\in\mathbb{R}^d$成立。</p><p>然而，仔细挑选出$2d$条符合条件的样本并不容易。下面的定理2基于更现实的场景，指出只要训练数据的分布包含所有的方向（例如一个包含原点的超球），那么在足够的训练数据量下MLP能够收敛到线性函数。</p><p><strong>定理2</strong>（泛化的条件）：令$g(\pmb{x})=\pmb{\beta}^T\pmb{x}$表示待拟合的目标函数，$\pmb{\beta}\in\mathbb{R}^d$。假设$\{\pmb{x}_i\}_{i=1}^n$从域$\mathcal{D}$中采样，其中$\mathcal{D}$包含一个连通的子集$S$，满足对任意非零向量$\pmb{w}\in\mathbb{R}^d$，存在$k>0$使得$k\pmb{w}\in S$。若在NTK机制下，使用均方误差在$\{({\pmb{x}}_i,y_i)\}_{i=1}^n$上训练一个两层ReLU MLP，$f(\pmb{x})\xrightarrow{p}\pmb{\beta}^T\pmb{x}$在$n\to\infty$时成立，即$f$依概率收敛到$g$。</p><p>定理2说明了为什么数据集中“虚假”的相关性（真实不应存在的相关性）会损害模型的泛化性能，补充了因果推理的论点。例如，如果人工收集的训练图片中只有在沙漠中的骆驼，这里骆驼和沙漠就是数据集不够“多样”导致的虚假相关性，实际上骆驼还生活在草原等多种环境上。那么此时定理2的条件不再满足，模型也可能因此不能很好地泛化。</p><p>总的来说，定理1指出MLPs对于大多数非线性函数不能泛化，定理2指出MLPs当训练数据足够多样时能够在线性目标函数下泛化。</p><h3 id=使用其他激活函数的mlps>使用其他激活函数的MLPs<a hidden class=anchor aria-hidden=true href=#使用其他激活函数的mlps>#</a></h3><p>以上讨论都基于使用ReLU激活函数的MLPs。除了ReLU，还有$\tanh(x),\cos(x),x^2$等激活函数。作者发现，在激活函数和待拟合的目标函数相近时，MLPs的泛化性能较好。</p><h2 id=图神经网络gnns如何泛化>图神经网络GNNs如何泛化<a hidden class=anchor aria-hidden=true href=#图神经网络gnns如何泛化>#</a></h2><p>以上讨论说明了MLPs在非线性任务上难以泛化。然而基于MLPs的GNNs却在一些非线性任务上表现出良好的泛化性，例如直观物理（intuitive Physics）、图论算法（graph algorithms）、符号数学（symbolic mathematics）等。</p><h3 id=一个假设线性对齐辅助了gnns的泛化>一个假设：线性对齐辅助了GNNs的泛化<a hidden class=anchor aria-hidden=true href=#一个假设线性对齐辅助了gnns的泛化>#</a></h3><p>GNNs可以被用来解决<strong>最短路径</strong>问题。人们发现在GNN的聚合函数中使用最小$\min$函数后，训练后的GNN可以较好地泛化到比训练集更大的图上：</p><p>$$\pmb{h}_u^{(k)}=\min_{v\in\mathcal{N}(u)}\text{MLP}^{(k)}(\pmb{h}_u^{(k-1)},\pmb{h}_v^{(k-1)},\pmb{w}_{(v,u)})$$</p><p>另一方面，传统的最短路问题可以通过Bellman-Fold（BF）算法解决：</p><p>$$d[k][u]=\min_{v\in\mathcal{N}(u)}d[k-1][v]+\pmb{w}(v,u)$$</p><p>其中$\pmb{w}(v,u)$表示边$(v,u)$的权重，$d[k][u]$表示$k$步以内到达节点$u$的最短距离。可以发现BF算法的更新式子可以很容易地与GNN的更新式子对齐：只需令MLP学习一个线性函数$d[k-1][v]+\pmb{w}(v,u)$即可。由于MLPs可以较好地对线性目标函数泛化，所以GNNs可以在计算最短路径问题上良好地泛化。
反之，如果在GNN的聚合函数中使用$\text{sum}$或其他函数，那么MLP就需要学习一个非线性目标函数，导致其无法泛化（定理1），进而导致GNN无法泛化。</p><p>由上述最短路问题推广到其他问题，作者发现许多GNNs泛化性能好的问题都可以用动态规划（dynamic programming, DP）解决，而DP中的迭代更新式子与GNNs的特征聚合函数很相似。</p><p><strong>定义3</strong>：动态规划方法可以被形式化为：</p><p>$$\text{Answer}[k][s]=\text{DP-Update}(\{\text{Answer}[k-1][s&rsquo;],s&rsquo;=1,\cdots,n\})$$</p><p>其中$\text{Answer}[k][s]$表示第$k$次迭代、状态为$s$的子问题的解。</p><p><strong>假设1</strong>（线性算法对齐）：令$f:\mathcal{X}\to\mathbb{R}$表示目标函数。$\mathcal{N}$是一个神经网络，包含$m$个MLP模块。假设存在$m$个线性函数$\{g_i\}_{i=1}^m$，使得替换$\mathcal{N}$中的MLP后，$\mathcal{N}$能够模拟$f$。那么对于给定的$\epsilon>0$，存在$\{(x_i,f(x_i))\}_{i=1}^n\subset\mathcal{D}\subsetneq\mathcal{X}$，使得在其上通过梯度下降和均方误差损失训练的$\mathcal{N}$得到的$\hat{f}$满足$\parallel\hat{f}-f\parallel&lt;\epsilon$。</p><p>作者认为，模型的线性对齐不局限于GNNs。人们可以将非线性的操作集成在模型的结构或者输入的表征向量中，这样在梯度下降训练时，模型只需要学习一个线性函数，从而提高了泛化能力。
GNNs学习DP的迭代表达式是一个例子，另外的例子是在算术任务中使用log-and-exp编码来提高乘法的泛化性。
另外，在一些任务中可能变换输入表征更容易。具体来说，目标函数$f$可以拆解为</p><p>$$f=g\circ h$$</p><p>其中$h$是表征向量，$g$是更简单的目标函数，例如线性函数，这样模型就更容易学习和泛化。对于表征向量$h$，可以使用领域知识；或者表征学习方法，在测试域$\mathcal{X}\setminus\mathcal{D}$进行无监督地表征学习。例如，在自然语言学习中，预训练表征和使用领域知识的特征转化可以帮助模型在不同语种之间泛化。在计量经济学中，人类对于本质因素或特征的判断（领域知识）尤其重要，因为金融市场经常需要模型进行泛化外推。</p><h3 id=理论推导和实验结果>理论推导和实验结果<a hidden class=anchor aria-hidden=true href=#理论推导和实验结果>#</a></h3><p>作者在3个DP任务上验证假设：最大度、最短路和$n$-body问题。首先，考虑计算图的最大度，可以通过1步DP解决。作为定理1的一个推论，使用$\text{sum}$聚合函数的GNNs无法在该问题上良好泛化。</p><p><strong>推论1</strong>：使用$\text{sum}$聚合和$\text{sum}$读出的GNNs在最大度问题上不能泛化。</p><p>利用假设1，为了实现线性对齐，将GNN的读出函数修改为$\max$函数。下面的定理3说明修改后的GNN能够良好地泛化。</p><p><strong>定理3</strong>（GNNs的泛化）：假设图中所有的顶点有相同的特征向量。令$g$和$g&rsquo;$分别表示最大度和最小度函数。令$\{(G_i,g(G_i))\}_{i=1}^n$表示训练集。如果$\{(g(G_i),g&rsquo;(G_i),g(G_i)\cdot N_i^{\max},g&rsquo;(G_i)\cdot N_i^{\min})\}_{i=1}^n$通过线性变换能表示$\mathbb{R}^4$（是$\mathbb{R}^4$的一组基），其中$N_i^{\max}$和$N_i^{\min}$分别表示$G_i$中具有最大度和最小度的顶点的个数，那么一个一层的使用$\max$读出函数的GNN，通过在$\{(G_i,g(G_i))\}_{i=1}^n$上使用均方误差损失和NTK机制训练，能够学习到目标函数$g$。</p><p>定理3中的条件与定理2类似，都是在保证训练集的多样性，只不过这里是用图的结构（即最大度）的多样性，而定理2中是用数据集的“方向”。如果训练集中所有的图有相同的最大度或者最小度，例如训练集仅仅属于path、$C$-regular graphs（度为$C$的正则图）、cycle、ladder四种类型之一，那么定理3的条件就遭到破坏，相应的GNN也不能保证学习到目标函数。</p><p>作者通过实验验证定理3和推论1。作者发现在最大度任务上，使用$\max$读出函数的GNN确实比使用$\text{sum}$读出函数的GNN的泛化效果要好；在最短路任务上，使用$\min$读出函数的GNN也确实比使用$\text{sum}$读出函数的GNN要好（图(a)）。
此外，在图(b)中，作者考虑第3个任务：$n$-body问题，即预测重力系统中$n$个物体随时间演变的规律。GNN的输入是完全图，每个顶点代表一个物体。顶点的特征由物体的质量$m_u$、在$t$时刻的位置$\pmb{x}_u^{(t)}$和速度$\pmb{v}_u^{(t)}$拼接而来。边特征设定为$0$。GNN的输出是在$t+1$时刻每个物体$u$的速度。真实的速度$f(G;u)$可以近似表示为</p><p>$$
\begin{aligned}
f(G;u)&\approx\pmb{v}_u^t+\pmb{a}_u^t\cdot\text{d}t \\
\pmb{a}_u^t &= C\cdot\sum_{v\neq u}\frac{m_v}{\parallel\pmb{x}_u^t-\pmb{x}_v^t\parallel_2^3}\cdot(\pmb{x}_v^t-\pmb{x}_u^t)
\end{aligned}
$$</p><p>其中$C$是常数。为了学习$f$，GNN中的MLP需要学习一个非线性函数，因此难以泛化。为了简化MLP的学习任务，使用表征$h(G)$替换输入的顶点特征。具体来说，将边特征由$0$替换为</p><p>$$\pmb{w}_{(u,v)}^{(t)}=m_v\cdot\frac{\pmb{x}_v^{(t)}-\pmb{x}_u^{(t)}}{\parallel\pmb{x}_u^t-\pmb{x}_v^t\parallel_2^3}$$</p><p>这样MLP只需要学习一个线性函数，从而提高了泛化能力。如下图(b)所示。</p><p>作者还发现训练图的结构也会对GNN的泛化能力造成影响，不同的任务中GNN更“喜欢”不同结构的训练图。如下图所示。</p><h2 id=本文与其他分布外问题设置的联系>本文与其他分布外问题设置的联系<a hidden class=anchor aria-hidden=true href=#本文与其他分布外问题设置的联系>#</a></h2><p><strong>领域自适应</strong>（domain adaptation）研究如何泛化到一个特定的目标域。典型的方法是在训练中加入目标域的无标签样本。</p><p><strong>自监督学习</strong>（self-supervised learning）研究如何在无标签样本上进行训练。</p><p><strong>不变模型</strong>（invariant models）研究如何在多种训练分布之间学习内在不变的本质特征。</p><p><strong>分布鲁棒性</strong>（distributional robustness）研究在数据分布上进行微小的对抗扰动，并保证模型在扰动后依然表现良好。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>本文的核心思想是通过改进模型结构或者输入信息简化目标函数，从而提升泛化性能。
对于实际场景，真实的目标函数很可能是<strong>复杂且未知</strong>的，所以通过修改模型的结构达到提高泛化能力的方法可能不太容易。使用无监督表征方法改进输入的特征向量是不错的方法。</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yliuhz.github.io/blogs/posts/gslearning/><span class=title>« Prev</span><br><span>Unsupervised Deep Graph Structure Learning</span></a>
<a class=next href=https://yliuhz.github.io/blogs/posts/sbm/><span class=title>Next »</span><br><span>Stochastic Blockmodels</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Generalization of GNNs and MLPs on twitter" href="https://twitter.com/intent/tweet/?text=Generalization%20of%20GNNs%20and%20MLPs&amp;url=https%3a%2f%2fyliuhz.github.io%2fblogs%2fposts%2fgnn-ood%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Generalization of GNNs and MLPs on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyliuhz.github.io%2fblogs%2fposts%2fgnn-ood%2f&amp;title=Generalization%20of%20GNNs%20and%20MLPs&amp;summary=Generalization%20of%20GNNs%20and%20MLPs&amp;source=https%3a%2f%2fyliuhz.github.io%2fblogs%2fposts%2fgnn-ood%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Generalization of GNNs and MLPs on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyliuhz.github.io%2fblogs%2fposts%2fgnn-ood%2f&title=Generalization%20of%20GNNs%20and%20MLPs"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Generalization of GNNs and MLPs on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyliuhz.github.io%2fblogs%2fposts%2fgnn-ood%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Generalization of GNNs and MLPs on whatsapp" href="https://api.whatsapp.com/send?text=Generalization%20of%20GNNs%20and%20MLPs%20-%20https%3a%2f%2fyliuhz.github.io%2fblogs%2fposts%2fgnn-ood%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Generalization of GNNs and MLPs on telegram" href="https://telegram.me/share/url?text=Generalization%20of%20GNNs%20and%20MLPs&amp;url=https%3a%2f%2fyliuhz.github.io%2fblogs%2fposts%2fgnn-ood%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://yliuhz.github.io/blogs>LIU Yue's blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script type=text/javascript id=clstr_globe src="//clustrmaps.com/globe.js?d=mOVPhw_QyxeNIldDVLiHhU4icnVeTRaMhH8bNjuQ1kk"></script>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>