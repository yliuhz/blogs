<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>FlashAttention | LIU Yue's blogs</title><meta name=keywords content><meta name=description content="FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。
作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。
输入：$\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$
输出：$\mathbf{O}\in\mathbb{R}^{N\times d}$
标准self-attention：
$\mathbf{S}=\mathbf{Q}\mathbf{K}^T\in\mathbb{R}^{N\times N}$
$\mathbf{P}=\exp(\mathbf{S})$
$\mathbf{O}=\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。
Flash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\mathbf{S}$。
首先推导增量式的softmax函数：
对一个向量$\mathbf{x}$计算softmax：$\sigma(\mathbf{x})=\exp(\mathbf{x})/{\sum_i {\exp(\mathbf{x}_i)}}$
对两个向量的拼接$[\mathbf{x},\mathbf{y}]$计算softmax：$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(\sum_i\exp(\mathbf{x}_i)+\sum_j\exp(\mathbf{y}_j))$
设$l(\mathbf{x})=\sum_i\exp(\mathbf{x}_i)$，则$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(l(\mathbf{x})+l(\mathbf{y}))$
将$\mathbf{Q,O},l$分成$T_r$块，将$\mathbf{K,V}$分成$T_c$块，进行二重循环。
1 2 3 4 for j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：
$\mathbf{S}_{ij}=\mathbf{Q}_i\mathbf{K}_j^T$
$\mathbf{P}_{ij}=\exp(\mathbf{S}_{ij})$
$l_{ij}=\text{rowsum}(\mathbf{P}_{ij})$
$\mathbf{O}_i&rsquo;=\mathbf{P}_{ij}\mathbf{V}_j$
然后需要对上一轮的$\mathbf{O_i}$和$l_i$进行更新，以d=1为例。
$l_i^{new}=l_i+l_{ij}$比较直接
两个红色的矩阵相乘得到当前的$\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\mathbf{O}_i$由i行j列的内积得来，还需要加上$\mathbf{O}_{ij}$，这样得到$\mathbf{O}_i$的增量式更新：
$\mathbf{O}_i=\mathbf{O}_i*l_i/l_i^{new} + \mathbf{O}_{ij}$
论文中的Algorithm1由于考虑了算术稳定性防止\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。
发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github
A1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\mathbf{O}$。但显然增加了$\mathbf{K}_j$和$\mathbf{V}_j$的IO次数。
1 2 3 4 5 6 7 8 for i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。"><meta name=author content="Yue"><link rel=canonical href=https://yliuhz.github.io/blogs/posts/flashattention/><link crossorigin=anonymous href=/blogs/assets/css/stylesheet.min.ad30084630fe889ef998d8f1baed12d2a35f52185d429fcd2d35fa7304f2dca2.css integrity="sha256-rTAIRjD+iJ75mNjxuu0S0qNfUhhdQp/NLTX6cwTy3KI=" rel="preload stylesheet" as=style><link rel=icon href=https://yliuhz.github.io/blogs/favicon.ico><link rel=apple-touch-icon href=https://yliuhz.github.io/blogs/apple-touch-icon.png><meta name=twitter:card content="summary"><meta name=twitter:title content="FlashAttention | LIU Yue's blogs"><meta name=twitter:description content="FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。
作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。
输入：$\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$
输出：$\mathbf{O}\in\mathbb{R}^{N\times d}$
标准self-attention：
$\mathbf{S}=\mathbf{Q}\mathbf{K}^T\in\mathbb{R}^{N\times N}$
$\mathbf{P}=\exp(\mathbf{S})$
$\mathbf{O}=\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。
Flash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\mathbf{S}$。
首先推导增量式的softmax函数：
对一个向量$\mathbf{x}$计算softmax：$\sigma(\mathbf{x})=\exp(\mathbf{x})/{\sum_i {\exp(\mathbf{x}_i)}}$
对两个向量的拼接$[\mathbf{x},\mathbf{y}]$计算softmax：$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(\sum_i\exp(\mathbf{x}_i)+\sum_j\exp(\mathbf{y}_j))$
设$l(\mathbf{x})=\sum_i\exp(\mathbf{x}_i)$，则$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(l(\mathbf{x})+l(\mathbf{y}))$
将$\mathbf{Q,O},l$分成$T_r$块，将$\mathbf{K,V}$分成$T_c$块，进行二重循环。
1 2 3 4 for j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：
$\mathbf{S}_{ij}=\mathbf{Q}_i\mathbf{K}_j^T$
$\mathbf{P}_{ij}=\exp(\mathbf{S}_{ij})$
$l_{ij}=\text{rowsum}(\mathbf{P}_{ij})$
$\mathbf{O}_i&rsquo;=\mathbf{P}_{ij}\mathbf{V}_j$
然后需要对上一轮的$\mathbf{O_i}$和$l_i$进行更新，以d=1为例。
$l_i^{new}=l_i+l_{ij}$比较直接
两个红色的矩阵相乘得到当前的$\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\mathbf{O}_i$由i行j列的内积得来，还需要加上$\mathbf{O}_{ij}$，这样得到$\mathbf{O}_i$的增量式更新：
$\mathbf{O}_i=\mathbf{O}_i*l_i/l_i^{new} + \mathbf{O}_{ij}$
论文中的Algorithm1由于考虑了算术稳定性防止\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。
发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github
A1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\mathbf{O}$。但显然增加了$\mathbf{K}_j$和$\mathbf{V}_j$的IO次数。
1 2 3 4 5 6 7 8 for i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。"><meta property="og:title" content="FlashAttention | LIU Yue's blogs"><meta property="og:description" content="FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。
作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。
输入：$\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$
输出：$\mathbf{O}\in\mathbb{R}^{N\times d}$
标准self-attention：
$\mathbf{S}=\mathbf{Q}\mathbf{K}^T\in\mathbb{R}^{N\times N}$
$\mathbf{P}=\exp(\mathbf{S})$
$\mathbf{O}=\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。
Flash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\mathbf{S}$。
首先推导增量式的softmax函数：
对一个向量$\mathbf{x}$计算softmax：$\sigma(\mathbf{x})=\exp(\mathbf{x})/{\sum_i {\exp(\mathbf{x}_i)}}$
对两个向量的拼接$[\mathbf{x},\mathbf{y}]$计算softmax：$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(\sum_i\exp(\mathbf{x}_i)+\sum_j\exp(\mathbf{y}_j))$
设$l(\mathbf{x})=\sum_i\exp(\mathbf{x}_i)$，则$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(l(\mathbf{x})+l(\mathbf{y}))$
将$\mathbf{Q,O},l$分成$T_r$块，将$\mathbf{K,V}$分成$T_c$块，进行二重循环。
1 2 3 4 for j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：
$\mathbf{S}_{ij}=\mathbf{Q}_i\mathbf{K}_j^T$
$\mathbf{P}_{ij}=\exp(\mathbf{S}_{ij})$
$l_{ij}=\text{rowsum}(\mathbf{P}_{ij})$
$\mathbf{O}_i&rsquo;=\mathbf{P}_{ij}\mathbf{V}_j$
然后需要对上一轮的$\mathbf{O_i}$和$l_i$进行更新，以d=1为例。
$l_i^{new}=l_i+l_{ij}$比较直接
两个红色的矩阵相乘得到当前的$\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\mathbf{O}_i$由i行j列的内积得来，还需要加上$\mathbf{O}_{ij}$，这样得到$\mathbf{O}_i$的增量式更新：
$\mathbf{O}_i=\mathbf{O}_i*l_i/l_i^{new} + \mathbf{O}_{ij}$
论文中的Algorithm1由于考虑了算术稳定性防止\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。
发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github
A1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\mathbf{O}$。但显然增加了$\mathbf{K}_j$和$\mathbf{V}_j$的IO次数。
1 2 3 4 5 6 7 8 for i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。"><meta property="og:type" content="article"><meta property="og:url" content="https://yliuhz.github.io/blogs/posts/flashattention/"><meta property="og:image" content="https://yliuhz.github.io/blogs/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-18T18:59:54+08:00"><meta property="article:modified_time" content="2023-05-18T18:59:54+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://yliuhz.github.io/blogs/posts/"},{"@type":"ListItem","position":3,"name":"FlashAttention","item":"https://yliuhz.github.io/blogs/posts/flashattention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"FlashAttention | LIU Yue's blogs","name":"FlashAttention","description":"FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。\n作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。\n输入：$\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}$\n输出：$\\mathbf{O}\\in\\mathbb{R}^{N\\times d}$\n标准self-attention：\n$\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^T\\in\\mathbb{R}^{N\\times N}$\n$\\mathbf{P}=\\exp(\\mathbf{S})$\n$\\mathbf{O}=\\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。\nFlash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\\mathbf{S}$。\n首先推导增量式的softmax函数：\n对一个向量$\\mathbf{x}$计算softmax：$\\sigma(\\mathbf{x})=\\exp(\\mathbf{x})/{\\sum_i {\\exp(\\mathbf{x}_i)}}$\n对两个向量的拼接$[\\mathbf{x},\\mathbf{y}]$计算softmax：$\\sigma([\\mathbf{x},\\mathbf{y}])=[\\exp(\\mathbf{x}),\\exp(\\mathbf{y})]/(\\sum_i\\exp(\\mathbf{x}_i)+\\sum_j\\exp(\\mathbf{y}_j))$\n设$l(\\mathbf{x})=\\sum_i\\exp(\\mathbf{x}_i)$，则$\\sigma([\\mathbf{x},\\mathbf{y}])=[\\exp(\\mathbf{x}),\\exp(\\mathbf{y})]/(l(\\mathbf{x})+l(\\mathbf{y}))$\n将$\\mathbf{Q,O},l$分成$T_r$块，将$\\mathbf{K,V}$分成$T_c$块，进行二重循环。\n1 2 3 4 for j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：\n$\\mathbf{S}_{ij}=\\mathbf{Q}_i\\mathbf{K}_j^T$\n$\\mathbf{P}_{ij}=\\exp(\\mathbf{S}_{ij})$\n$l_{ij}=\\text{rowsum}(\\mathbf{P}_{ij})$\n$\\mathbf{O}_i\u0026rsquo;=\\mathbf{P}_{ij}\\mathbf{V}_j$\n然后需要对上一轮的$\\mathbf{O_i}$和$l_i$进行更新，以d=1为例。\n$l_i^{new}=l_i+l_{ij}$比较直接\n两个红色的矩阵相乘得到当前的$\\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\\mathbf{O}_i$由i行j列的内积得来，还需要加上$\\mathbf{O}_{ij}$，这样得到$\\mathbf{O}_i$的增量式更新：\n$\\mathbf{O}_i=\\mathbf{O}_i*l_i/l_i^{new} + \\mathbf{O}_{ij}$\n论文中的Algorithm1由于考虑了算术稳定性防止\\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。\n发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github\nA1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\\mathbf{O}$。但显然增加了$\\mathbf{K}_j$和$\\mathbf{V}_j$的IO次数。\n1 2 3 4 5 6 7 8 for i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。","keywords":[],"wordCount":"109","inLanguage":"en","datePublished":"2023-05-18T18:59:54+08:00","dateModified":"2023-05-18T18:59:54+08:00","author":{"@type":"Person","name":"Yue"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yliuhz.github.io/blogs/posts/flashattention/"},"publisher":{"@type":"Organization","name":"LIU Yue's blogs","logo":{"@type":"ImageObject","url":"https://yliuhz.github.io/blogs/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script>
<script>function __initialize_katex(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js integrity=sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF crossorigin=anonymous onload=__initialize_katex()></script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary-bg:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list-page{background:var(--theme)}.list-page:not(.dark)::-webkit-scrollbar-track{background:0 0}.list-page:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="auto",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://yliuhz.github.io/blogs accesskey=h title="LIU Yue's blogs (Alt + H)">LIU Yue's blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://yliuhz.github.io/blogs/archives/ title=Archive>Archive</a></li><li><a href=https://yliuhz.github.io/blogs/search/ title="Search (Alt + /)" data-no-instant accesskey=/>Search</a></li><li><a href=https://yliuhz.github.io/blogs/tags/ title=Tags>Tags</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yliuhz.github.io/blogs>Home</a>&nbsp;»&nbsp;<a href=https://yliuhz.github.io/blogs/posts/>Posts</a></div><h1 class=post-title>FlashAttention</h1><div class=post-meta><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>May 18, 2023</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>109 words</span></span><span class=meta-item><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>1 min</span></span></div></header><div class="toc side right"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%8f%91%e6%95%a3qa aria-label=发散QA>发散QA</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p><strong>FlashAttention</strong>论文发表于Neurips2022，第一单位是斯坦福大学。</p><p>作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。</p><p><strong>输入</strong>：$\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{N\times d}$</p><p><strong>输出</strong>：$\mathbf{O}\in\mathbb{R}^{N\times d}$</p><p>标准self-attention：</p><p>$\mathbf{S}=\mathbf{Q}\mathbf{K}^T\in\mathbb{R}^{N\times N}$</p><p>$\mathbf{P}=\exp(\mathbf{S})$</p><p>$\mathbf{O}=\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。</p><p>Flash-attention的思路：分块在高速on-chip显存上<strong>增量式</strong>计算，避免平方空间的$\mathbf{S}$。</p><p>首先推导增量式的softmax函数：</p><p>对一个向量$\mathbf{x}$计算softmax：$\sigma(\mathbf{x})=\exp(\mathbf{x})/{\sum_i {\exp(\mathbf{x}_i)}}$</p><p>对两个向量的拼接$[\mathbf{x},\mathbf{y}]$计算softmax：$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(\sum_i\exp(\mathbf{x}_i)+\sum_j\exp(\mathbf{y}_j))$</p><p>设$l(\mathbf{x})=\sum_i\exp(\mathbf{x}_i)$，则$\sigma([\mathbf{x},\mathbf{y}])=[\exp(\mathbf{x}),\exp(\mathbf{y})]/(l(\mathbf{x})+l(\mathbf{y}))$</p><p>将$\mathbf{Q,O},l$分成$T_r$块，将$\mathbf{K,V}$分成$T_c$块，进行二重循环。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3>3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=mf>1.</span><span class=o>..</span><span class=n>T_c</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>取出K_j和V_j</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=mf>1.</span><span class=o>..</span><span class=n>T_r</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>取出Q_i</span><span class=p>,</span><span class=n>O_i和l_i</span>
</span></span></code></pre></td></tr></table></div></div><p>计算当前块内的self-attention，即：</p><p>$\mathbf{S}_{ij}=\mathbf{Q}_i\mathbf{K}_j^T$</p><p>$\mathbf{P}_{ij}=\exp(\mathbf{S}_{ij})$</p><p>$l_{ij}=\text{rowsum}(\mathbf{P}_{ij})$</p><p>$\mathbf{O}_i&rsquo;=\mathbf{P}_{ij}\mathbf{V}_j$</p><p>然后需要对上一轮的$\mathbf{O_i}$和$l_i$进行更新，以d=1为例。</p><img src=https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/image-20230518194619599.png alt=image width=50%/><p>$l_i^{new}=l_i+l_{ij}$比较直接</p><p>两个红色的矩阵相乘得到当前的$\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\mathbf{O}_i$由i行j列的内积得来，还需要加上$\mathbf{O}_{ij}$，这样得到$\mathbf{O}_i$的增量式更新：</p><p>$\mathbf{O}_i=\mathbf{O}_i*l_i/l_i^{new} + \mathbf{O}_{ij}$</p><p>论文中的Algorithm1由于考虑了算术稳定性防止\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。</p><p><img loading=lazy src=https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/iShot_2023-05-19_09.53.57.png alt="Algorithm 1"></p><h3 id=发散qa>发散QA<a hidden class=anchor aria-hidden=true href=#发散qa>¶</a></h3><p><strong>Q1</strong>. Algorithm 1中的i、j循环可以交换吗？<a href=https://github.com/yliuhz/awesome-papers/tree/main/FlashAttention>github</a></p><p><strong>A1</strong>. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\mathbf{O}$。但显然增加了$\mathbf{K}_j$和$\mathbf{V}_j$的IO次数。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1>1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2>2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3>3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4>4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5>5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6>6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7>7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=mf>1.</span><span class=o>..</span><span class=n>T_r</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=mf>1.</span><span class=o>..</span><span class=n>T_c</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>取出K_j和V_j</span>
</span></span><span class=line><span class=cl>    <span class=n>取出Q_i</span><span class=p>,</span><span class=n>O_i和l_i</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>更新O_i和l_i</span>
</span></span></code></pre></td></tr></table></div></div><p>如下不可以。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4>4</a>
</span><span class=lnt id=hl-2-5><a class=lnlinks href=#hl-2-5>5</a>
</span><span class=lnt id=hl-2-6><a class=lnlinks href=#hl-2-6>6</a>
</span><span class=lnt id=hl-2-7><a class=lnlinks href=#hl-2-7>7</a>
</span><span class=lnt id=hl-2-8><a class=lnlinks href=#hl-2-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=mf>1.</span><span class=o>..</span><span class=n>T_r</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>取出Q_i</span><span class=p>,</span><span class=n>O_i和l_i</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=mf>1.</span><span class=o>..</span><span class=n>T_c</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>取出K_j和V_j</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>  <span class=n>更新O_i和l_i</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Q2</strong>. $\mathbf{O}_i$的更新表达式怎么得来的？</p><p><strong>A2</strong>. $\mathbf{O}_i=\mathbf{O}_i*l_i/l_i^{new} + \mathbf{O}_{ij}$。$\mathbf{O}_i$由$\mathbf{S}$的第i行的softmax和$\mathbf{V}$的第j列的内积得来。然而在这里分块计算时，softmax的分母，即对行的求和值$l$，是在不断更新的。只有到$\mathbf{S}$的i行最后一列时才得到正确的l，因此有这样的增量更新表达式。</p><p>同时，不按当前的算式更新，每次只累加softmax的分母，直到最后一列才除以$l$，肯定也是可以的。</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>¶</a></h3><p>[1] 我的flash-attention实现：<a href=https://github.com/yliuhz/awesome-papers/tree/main/FlashAttention>awesome-papers/FlashAttention at main · yliuhz/awesome-papers (github.com)</a></p><p>[2] Flash Attention论文地址：<a href=https://doi.org/10.48550/arXiv.2205.14135>https://doi.org/10.48550/arXiv.2205.14135</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://yliuhz.github.io/blogs/posts/vae/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>VAE</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2023 <a href=https://yliuhz.github.io/blogs>LIU Yue's blogs</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a></span>
<span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
    <a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/blogs/js/medium-zoom.min.js data-no-instant></script>
<script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script><script>mediumZoom(".entry-cover img"),mediumZoom(".post-content img:not([no-zoom])")</script></body></html>