[{"content":" Demo Page Ferre hinnitibus erat accipitrem dixi Troiae tollens Lorem ipsum dolor ( Citation: Vitali Rosati,\u0026#32;2016,\u0026#32;p.\u0026nbsp;20 Vitali Rosati,\u0026#32; M. \u0026#32; (2016). \u0026#32;Qu\u0026rsquo;est-ce que l\u0026rsquo;éditorialisation ?. Sens Public.\u0026#32;Retrieved from\u0026#32; http://sens-public.org/article1184.html ;\u0026#32; Citation: Sinatra\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32;2014,\u0026#32;pp.\u0026nbsp;22-23 Sinatra,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32; M.\u0026#32; (2014). \u0026#32; Pratiques de l\u0026rsquo;édition numérique. \u0026#32; Les Presses de l\u0026#39;Université de Montréal. ) amet.\nLorem markdownum ( Citation: Vial\u0026#32;\u0026amp;\u0026#32;Catoir-Brisson,\u0026#32;2017 Vial,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Catoir-Brisson,\u0026#32; M.\u0026#32; (2017). \u0026#32; Design et innovation dans la chaîne du livre: écrire, éditer, lire à l\u0026rsquo;ère numérique. \u0026#32; PUF. ) , a quoque nutu est quodcumque mandasset veluti ( Citation: Sinatra\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32;2014,\u0026#32;pp.\u0026nbsp;20-22 Sinatra,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32; M.\u0026#32; (2014). \u0026#32; Pratiques de l\u0026rsquo;édition numérique. \u0026#32; Les Presses de l\u0026#39;Université de Montréal. ) . Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris ( Citation: Goody,\u0026#32;1978,\u0026#32;p.\u0026nbsp;25 Goody,\u0026#32; J.\u0026#32; (1978). \u0026#32; La raison graphique: la domestication de la pensée sauvage. \u0026#32; Éditions de Minuit. ) servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid ( Citation: Masure,\u0026#32;2017 Masure,\u0026#32; A.\u0026#32; (2017). \u0026#32; Design et humanités numériques. \u0026#32; Éditions B42. ) ; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen ( Citation: Tadier,\u0026#32;2018 Tadier,\u0026#32; E. \u0026#32; (2018). \u0026#32; Les corps du livre, du codex au numérique. Enjeux des corporéités d\u0026rsquo;une forme médiatique: vers une anthropologie communicationnelle du livre \u0026nbsp;(Thèse de doctorat).\u0026#32; Sorbonne Université,\u0026#32; France. ) .\nCited Bibliography Goody (1978) Goody,\u0026#32; J.\u0026#32; (1978). \u0026#32; La raison graphique: la domestication de la pensée sauvage. \u0026#32; Éditions de Minuit. Masure (2017) Masure,\u0026#32; A.\u0026#32; (2017). \u0026#32; Design et humanités numériques. \u0026#32; Éditions B42. Sinatra\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati (2014) Sinatra,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32; M.\u0026#32; (2014). \u0026#32; Pratiques de l\u0026rsquo;édition numérique. \u0026#32; Les Presses de l\u0026#39;Université de Montréal. Tadier (2018) Tadier,\u0026#32; E. \u0026#32; (2018). \u0026#32; Les corps du livre, du codex au numérique. Enjeux des corporéités d\u0026rsquo;une forme médiatique: vers une anthropologie communicationnelle du livre \u0026nbsp;(Thèse de doctorat).\u0026#32; Sorbonne Université,\u0026#32; France. Vitali Rosati (2016) Vitali Rosati,\u0026#32; M. \u0026#32; (2016). \u0026#32;Qu\u0026rsquo;est-ce que l\u0026rsquo;éditorialisation ?. Sens Public.\u0026#32;Retrieved from\u0026#32; http://sens-public.org/article1184.html Vial\u0026#32;\u0026amp;\u0026#32;Catoir-Brisson (2017) Vial,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Catoir-Brisson,\u0026#32; M.\u0026#32; (2017). \u0026#32; Design et innovation dans la chaîne du livre: écrire, éditer, lire à l\u0026rsquo;ère numérique. \u0026#32; PUF. ","permalink":"https://yliuhz.github.io/blogs/posts/eng/","summary":"Demo Page Ferre hinnitibus erat accipitrem dixi Troiae tollens Lorem ipsum dolor ( Citation: Vitali Rosati,\u0026#32;2016,\u0026#32;p.\u0026nbsp;20 Vitali Rosati,\u0026#32; M. \u0026#32; (2016). \u0026#32;Qu\u0026rsquo;est-ce que l\u0026rsquo;éditorialisation ?. Sens Public.\u0026#32;Retrieved from\u0026#32; http://sens-public.org/article1184.html ;\u0026#32; Citation: Sinatra\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32;2014,\u0026#32;pp.\u0026nbsp;22-23 Sinatra,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32; M.\u0026#32; (2014). \u0026#32; Pratiques de l\u0026rsquo;édition numérique. \u0026#32; Les Presses de l\u0026#39;Université de Montréal. ) amet.\nLorem markdownum ( Citation: Vial\u0026#32;\u0026amp;\u0026#32;Catoir-Brisson,\u0026#32;2017 Vial,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Catoir-Brisson,\u0026#32; M.\u0026#32; (2017). \u0026#32; Design et innovation dans la chaîne du livre: écrire, éditer, lire à l\u0026rsquo;ère numérique.","title":"Eng"},{"content":"最近发现一篇ICLR2023 spotlight的蒸馏GNN到MLP的论文，觉得很新鲜。向前追溯发现其是基于ICLR2022的GLNN做的，遂在这里整理一下相关内容和自己的理解。\nGraph-less Neural Networks (GLNN) 作者( ( Citation: Vitali Rosati,\u0026#32;2016,\u0026#32;p.\u0026nbsp;20 Vitali Rosati,\u0026#32; M. \u0026#32; (2016). \u0026#32;Qu\u0026rsquo;est-ce que l\u0026rsquo;éditorialisation ?. Sens Public.\u0026#32;Retrieved from\u0026#32; http://sens-public.org/article1184.html ;\u0026#32; Citation: Sinatra\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32;2014,\u0026#32;pp.\u0026nbsp;22-23 Sinatra,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32; M.\u0026#32; (2014). \u0026#32; Pratiques de l\u0026rsquo;édition numérique. \u0026#32; Les Presses de l\u0026#39;Université de Montréal. ) )指出现实场景难以落地GNN的一大原因是GNN的推理速度很慢。假设图中平均的顶点度为$R$，那么对于一个$L$层GNN的网络，总共需要提取(fetch)$O(R^L)$次邻居和自己的节点特征。如下图所示。该指数量级的提取次数导致GNN的推理时间随层数增加而指数上升。 另一方面，多层感知机MLP由于不需要图结构作为输入，因此无需提取其他节点的特征，推理速度是线性的。\n为了节省推理时间，直接使用MLP在图上训练也是不可行的，因为丢掉了图结构信息。为了达到MLP的推理时间同时尽量保留图的结构信息，作者提出了从GNN蒸馏知识到MLP的方法，并验证了其有效性。\n解决框架 GLNN的结构容易理解，先训练一个笨重的GNN模型作为教师模型，再使用该GNN的输出$\\mathbf{z}_v$以及带标签节点本身的标签$\\mathbf{y}_v$训练简单的MLP学生网络。在归纳式学习(inductive learning)场景中， 当新的节点到来时，不再考虑其与训练图结构的连边，而是直接输入到MLP中做推理。 如下图所示。\n作者对于直推式和归纳式的详细描述： 可以看到测试时MLP和GLNN的学生网络是没有图结构输入的，只有测试顶点的特征向量。 同时，在测试教师网络GNN的归纳式推理时，只使用训练集图结构训练，而在测试时使用了包括测试顶点在内的整张图作为输入。这样对比是公平的。因为在使用GNN模型推理时我们会尽可能发挥模型的性能，为模型提供尽可能多的信息（见代码 official code）。\n训练学生网络时使用的损失函数为\n$$\\mathcal{L}=\\lambda\\sum_{v\\in\\mathcal{V}^L}\\mathcal{L}_{label}(\\hat{\\mathbf{y}}_v,\\mathbf{y}_v)+(1-\\lambda)\\sum_{v\\in\\mathcal{V}}\\mathcal{L}_{teacher}(\\hat{\\mathbf{y}}_v,\\mathbf{z}_v)$$\n其中$\\mathcal{L}_{label}$为交叉熵损失，$\\mathcal{L}_{teacher}$为KL散度损失，$\\lambda\\in[0,1]$是超参数，$\\mathcal{V}^L$表示带标签的训练节点，$\\mathcal{V}$表示所有训练节点。$\\mathcal{L}_{teacher}$的含义是使学生网络输出的分布与教师网络的输出分布相近。\n实验 作者做了大量的实验，包括直推式(transductive)的推理、归纳式(inductive)的推理，与其他GNN加速方法做了比较，通过一个参量(min-cut loss)验证蒸馏的有效性，验证GLNN的表达能力(理论推导)，分析了GLNN失败的场景。参数实验（消融实验）中验证了特征中加噪声的影响，归纳式推理时不同训练测试划分的影响，以及使用其他教师网络模型的情况。在附录部分还加了在异配图（NON-HOMOPHILY）上的实验，并更加详细地分析了节点特征噪声对GLNN的影响。\n作者指出虽然现实场景中大多是归纳式推理，但直推式推理的实验仍然是有意义的(附录A.5)。第一，大多数现有GNN文献使用的是直推式的推理，为了公平对比。第二，直推式推理相对简单，因为在训练时看到了测试节点的特征。只有直推式能够work才能接着考虑更有挑战性的归纳式推理。第三，因为半监督训练时使用的标签很少，如pubmed数据集只有每个类别20个共60个标签，作者希望尽可能使用更多无标签节点提升性能。在现实场景中，当有很多无标签节点需要推理时，同样可以把它们拿来训练，然后用另一组不同的带标签测试节点做评测。\n在附录J中，作者详细讨论了使用噪声扰动节点特征的实验结果(如下图Left)。发现2点：\n当节点特征为纯高斯噪声($\\alpha=1$)时，原始GNN仍然相对较好； 当节点特征为纯高斯噪声($\\alpha=1$)时，蒸馏的GLNN比纯训练MLP好。 觉得这两个结果特殊是因为作者在正文5.8简单分析了什么情况下GLNN会失效，通过互信息：\n$$I(G;y_i)=I(X^{[i]},\\mathcal{E}^{[i]};y_i)=I(\\mathcal{E}^{[i]};y_i)+I(X^{[i]};y_i|\\mathcal{E}^{[i]})$$\n最小化$\\mathcal{L}_{label}$相当于在最大化$I(G;y_i)$。在上式中，$I(\\mathcal{E}^{[i]};y_i)$表示边与节点标签的互信息，这是MLP无法访问到的。因此MLP只能最大化第二项$I(X^{[i]};y_i|\\mathcal{E}^{[i]})$。然而，当节点的标签与特征无关时，比如节点的标签表示节点的度或者节点是否构成一个三角形，此时MLP和GLNN的学生网络都无法学到单单从节点特征到标签的映射$f$。这样分析的话上面的第2条实验结果就有点奇怪。\n作者在附录J首先分析了为什么GNN在随机节点特征数据上仍然表现良好：过拟合。假设有一个A,B,C,D共4个顶点构成的全连接图(clique)，以及只与D相连的顶点E。令A,B,C,D的节点特征为纯高斯噪声，且具有相同的标签c。现在使用B,C,D,E训练GNN，A用作归纳测试。假设使用1层GNN。由于B,C,D内部的连边过于稠密，导致聚合邻居后B,C,D的表征十分地接近，而E对D的影响则十分小。因此模型会过拟合地将B,C,D的特征映射到标签c。当使用A做测试时，同样的聚合操作导致A与B,C,D十分接近，因此会输出相同的标签c，从而导致分类正确（GNN归纳式推理时使用全部邻接矩阵作为输入）。总的来说，如果A与许多具有相同标签的训练邻居稠密连接，那么就可能训练出一个过拟合的分类器，直接将其映射到相同的标签。\n第二分析了为什么GLNN的学生网络好于MLP：测试集标签的不平衡。按照现有的训练/测试集划分，训练集的节点标签是均衡的，而测试集可能是不均衡的。结果是，MLP的预测也相对均衡，而GLNN可以从教师网络的soft labels中学习，因此GLNN的预测标签分布与真实的不平衡标签分布更加相似。如下图所示。\nReferences Sinatra\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati (2014) Sinatra,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32; M.\u0026#32; (2014). \u0026#32; Pratiques de l\u0026rsquo;édition numérique. \u0026#32; Les Presses de l\u0026#39;Université de Montréal. Vitali Rosati (2016) Vitali Rosati,\u0026#32; M. \u0026#32; (2016). \u0026#32;Qu\u0026rsquo;est-ce que l\u0026rsquo;éditorialisation ?. Sens Public.\u0026#32;Retrieved from\u0026#32; http://sens-public.org/article1184.html ","permalink":"https://yliuhz.github.io/blogs/posts/gnn2mlp/","summary":"最近发现一篇ICLR2023 spotlight的蒸馏GNN到MLP的论文，觉得很新鲜。向前追溯发现其是基于ICLR2022的GLNN做的，遂在这里整理一下相关内容和自己的理解。\nGraph-less Neural Networks (GLNN) 作者( ( Citation: Vitali Rosati,\u0026#32;2016,\u0026#32;p.\u0026nbsp;20 Vitali Rosati,\u0026#32; M. \u0026#32; (2016). \u0026#32;Qu\u0026rsquo;est-ce que l\u0026rsquo;éditorialisation ?. Sens Public.\u0026#32;Retrieved from\u0026#32; http://sens-public.org/article1184.html ;\u0026#32; Citation: Sinatra\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32;2014,\u0026#32;pp.\u0026nbsp;22-23 Sinatra,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Vitali-Rosati,\u0026#32; M.\u0026#32; (2014). \u0026#32; Pratiques de l\u0026rsquo;édition numérique. \u0026#32; Les Presses de l\u0026#39;Université de Montréal. ) )指出现实场景难以落地GNN的一大原因是GNN的推理速度很慢。假设图中平均的顶点度为$R$，那么对于一个$L$层GNN的网络，总共需要提取(fetch)$O(R^L)$次邻居和自己的节点特征。如下图所示。该指数量级的提取次数导致GNN的推理时间随层数增加而指数上升。 另一方面，多层感知机MLP由于不需要图结构作为输入，因此无需提取其他节点的特征，推理速度是线性的。\n为了节省推理时间，直接使用MLP在图上训练也是不可行的，因为丢掉了图结构信息。为了达到MLP的推理时间同时尽量保留图的结构信息，作者提出了从GNN蒸馏知识到MLP的方法，并验证了其有效性。\n解决框架 GLNN的结构容易理解，先训练一个笨重的GNN模型作为教师模型，再使用该GNN的输出$\\mathbf{z}_v$以及带标签节点本身的标签$\\mathbf{y}_v$训练简单的MLP学生网络。在归纳式学习(inductive learning)场景中， 当新的节点到来时，不再考虑其与训练图结构的连边，而是直接输入到MLP中做推理。 如下图所示。\n作者对于直推式和归纳式的详细描述： 可以看到测试时MLP和GLNN的学生网络是没有图结构输入的，只有测试顶点的特征向量。 同时，在测试教师网络GNN的归纳式推理时，只使用训练集图结构训练，而在测试时使用了包括测试顶点在内的整张图作为输入。这样对比是公平的。因为在使用GNN模型推理时我们会尽可能发挥模型的性能，为模型提供尽可能多的信息（见代码 official code）。\n训练学生网络时使用的损失函数为\n$$\\mathcal{L}=\\lambda\\sum_{v\\in\\mathcal{V}^L}\\mathcal{L}_{label}(\\hat{\\mathbf{y}}_v,\\mathbf{y}_v)+(1-\\lambda)\\sum_{v\\in\\mathcal{V}}\\mathcal{L}_{teacher}(\\hat{\\mathbf{y}}_v,\\mathbf{z}_v)$$\n其中$\\mathcal{L}_{label}$为交叉熵损失，$\\mathcal{L}_{teacher}$为KL散度损失，$\\lambda\\in[0,1]$是超参数，$\\mathcal{V}^L$表示带标签的训练节点，$\\mathcal{V}$表示所有训练节点。$\\mathcal{L}_{teacher}$的含义是使学生网络输出的分布与教师网络的输出分布相近。\n实验 作者做了大量的实验，包括直推式(transductive)的推理、归纳式(inductive)的推理，与其他GNN加速方法做了比较，通过一个参量(min-cut loss)验证蒸馏的有效性，验证GLNN的表达能力(理论推导)，分析了GLNN失败的场景。参数实验（消融实验）中验证了特征中加噪声的影响，归纳式推理时不同训练测试划分的影响，以及使用其他教师网络模型的情况。在附录部分还加了在异配图（NON-HOMOPHILY）上的实验，并更加详细地分析了节点特征噪声对GLNN的影响。\n作者指出虽然现实场景中大多是归纳式推理，但直推式推理的实验仍然是有意义的(附录A.5)。第一，大多数现有GNN文献使用的是直推式的推理，为了公平对比。第二，直推式推理相对简单，因为在训练时看到了测试节点的特征。只有直推式能够work才能接着考虑更有挑战性的归纳式推理。第三，因为半监督训练时使用的标签很少，如pubmed数据集只有每个类别20个共60个标签，作者希望尽可能使用更多无标签节点提升性能。在现实场景中，当有很多无标签节点需要推理时，同样可以把它们拿来训练，然后用另一组不同的带标签测试节点做评测。\n在附录J中，作者详细讨论了使用噪声扰动节点特征的实验结果(如下图Left)。发现2点：\n当节点特征为纯高斯噪声($\\alpha=1$)时，原始GNN仍然相对较好； 当节点特征为纯高斯噪声($\\alpha=1$)时，蒸馏的GLNN比纯训练MLP好。 觉得这两个结果特殊是因为作者在正文5.8简单分析了什么情况下GLNN会失效，通过互信息：\n$$I(G;y_i)=I(X^{[i]},\\mathcal{E}^{[i]};y_i)=I(\\mathcal{E}^{[i]};y_i)+I(X^{[i]};y_i|\\mathcal{E}^{[i]})$$\n最小化$\\mathcal{L}_{label}$相当于在最大化$I(G;y_i)$。在上式中，$I(\\mathcal{E}^{[i]};y_i)$表示边与节点标签的互信息，这是MLP无法访问到的。因此MLP只能最大化第二项$I(X^{[i]};y_i|\\mathcal{E}^{[i]})$。然而，当节点的标签与特征无关时，比如节点的标签表示节点的度或者节点是否构成一个三角形，此时MLP和GLNN的学生网络都无法学到单单从节点特征到标签的映射$f$。这样分析的话上面的第2条实验结果就有点奇怪。","title":"Gnn2mlp"},{"content":"So why diffusion models perform well?\nWhen \\(a \\ne 0\\), there are two solutions to \\(ax^2 + bx + c = 0\\) and they are \\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.\\] $a^2$\n$$b^2$$\n","permalink":"https://yliuhz.github.io/blogs/posts/diffusion/","summary":"So why diffusion models perform well?\nWhen \\(a \\ne 0\\), there are two solutions to \\(ax^2 + bx + c = 0\\) and they are \\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.\\] $a^2$\n$$b^2$$","title":"Diffusion Models"},{"content":"Variational Autoencoders 原博主为Lilian Weng\n与简单的自编码器不同，变分自编码器的表征$\\mathbf{z}$是一个分布。 给定一个数据集$\\mathbf{X}=\\{\\mathbf{x}_i\\}_{i=1}^N$，变分自编码器的观点是$\\mathbf{x}$由一个隐变量$\\mathbf{z}$产生，而$\\mathbf{z}$则遵循一个先验分布，通常取正态分布。 因此，变分自编码器可以由3个概率分布刻画：\n$p(\\mathbf{z})$: 先验分布 $p(\\mathbf{x}|\\mathbf{z})$: 解码器 $p(\\mathbf{z}|\\mathbf{x})$: 后验分布，编码器 其中后验分布很难直接计算，因此自编码器从一个未训练过的编码器，即对后验分布的估计$q(\\mathbf{z}|\\mathbf{x})$开始，通过优化目标函数不断逼近$q(\\mathbf{z}|\\mathbf{x})$和$p(\\mathbf{z}|\\mathbf{x})$的距离。\n这里使用KL散度衡量两个分布的距离，即$D_{KL}(q(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}|\\mathbf{x}))$。注意KL散度不具有对称性，原博主Lilian Weng甚至指出了为什么不使用$D_{KL}(p(\\mathbf{z}|\\mathbf{x})||q(\\mathbf{z}|\\mathbf{x}))$。\n具体来说，前向KL散度$D_{KL}(p||q)=\\mathbb{E}_{\\mathbf{z}\\sim p(\\mathbf{z})}\\log \\frac{p(\\mathbf{z})}{q(\\mathbf{z})}=\\int p(\\mathbf{z})\\log \\frac{p(\\mathbf{z})}{q(\\mathbf{z})}d\\mathbf{z}$中，p\u0026gt;0的位置要求q必须同时\u0026gt;0(因为$\\lim_{q\\to 0}p\\log \\frac{p}{q}\\to \\infty$)。因此优化前向KL散度会导致q覆盖了每个p分布概率不为0的点。反过来，我们这里使用的反向KL散度$D_{KL}(q||p)=\\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z})}\\log \\frac{q(\\mathbf{z})}{p(\\mathbf{z})}=\\int q(\\mathbf{z})\\log \\frac{q(\\mathbf{z})}{p(\\mathbf{z})}d\\mathbf{z}$，在p=0时保证了q必须=0。\n前向KL散度：p\u0026gt;0时q\u0026gt;0，可能导致q平铺在p\u0026gt;0的区域 反向KL散度（使用的）：p=0时q=0，可能导致q被挤压在p的一个峰上 在推导KL散度的表达式时就可以得到变分自编码器的损失函数ELBO。\n(图源Lilian Weng的博客：https://lilianweng.github.io/posts/2018-08-12-vae/)\n我们想同时极大化观测数据点$\\mathbf{x}$的似然，以及真假编码器的分布差距，即最大化 $$\\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z}|\\mathbf{x})}\\log p(\\mathbf{x}|\\mathbf{z})-D_{KL}(q(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))$$ 左边的是重构误差取反，右边的在先验分布为正态分布时可以显式展开。\n在计算重构误差时用到了重参数技巧（reparameterization trick），即把从一个带参数的编码器采样$\\mathbf{z}$，转化为从一个确定的分布（如标准正态）采样一个值，再通过将采样的值与编码器的输出（均值和方差）加减乘除得到$\\mathbf{z}$。这样梯度就和采样独立开来，可以反向传播了。\n","permalink":"https://yliuhz.github.io/blogs/posts/vae/","summary":"Variational Autoencoders 原博主为Lilian Weng\n与简单的自编码器不同，变分自编码器的表征$\\mathbf{z}$是一个分布。 给定一个数据集$\\mathbf{X}=\\{\\mathbf{x}_i\\}_{i=1}^N$，变分自编码器的观点是$\\mathbf{x}$由一个隐变量$\\mathbf{z}$产生，而$\\mathbf{z}$则遵循一个先验分布，通常取正态分布。 因此，变分自编码器可以由3个概率分布刻画：\n$p(\\mathbf{z})$: 先验分布 $p(\\mathbf{x}|\\mathbf{z})$: 解码器 $p(\\mathbf{z}|\\mathbf{x})$: 后验分布，编码器 其中后验分布很难直接计算，因此自编码器从一个未训练过的编码器，即对后验分布的估计$q(\\mathbf{z}|\\mathbf{x})$开始，通过优化目标函数不断逼近$q(\\mathbf{z}|\\mathbf{x})$和$p(\\mathbf{z}|\\mathbf{x})$的距离。\n这里使用KL散度衡量两个分布的距离，即$D_{KL}(q(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}|\\mathbf{x}))$。注意KL散度不具有对称性，原博主Lilian Weng甚至指出了为什么不使用$D_{KL}(p(\\mathbf{z}|\\mathbf{x})||q(\\mathbf{z}|\\mathbf{x}))$。\n具体来说，前向KL散度$D_{KL}(p||q)=\\mathbb{E}_{\\mathbf{z}\\sim p(\\mathbf{z})}\\log \\frac{p(\\mathbf{z})}{q(\\mathbf{z})}=\\int p(\\mathbf{z})\\log \\frac{p(\\mathbf{z})}{q(\\mathbf{z})}d\\mathbf{z}$中，p\u0026gt;0的位置要求q必须同时\u0026gt;0(因为$\\lim_{q\\to 0}p\\log \\frac{p}{q}\\to \\infty$)。因此优化前向KL散度会导致q覆盖了每个p分布概率不为0的点。反过来，我们这里使用的反向KL散度$D_{KL}(q||p)=\\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z})}\\log \\frac{q(\\mathbf{z})}{p(\\mathbf{z})}=\\int q(\\mathbf{z})\\log \\frac{q(\\mathbf{z})}{p(\\mathbf{z})}d\\mathbf{z}$，在p=0时保证了q必须=0。\n前向KL散度：p\u0026gt;0时q\u0026gt;0，可能导致q平铺在p\u0026gt;0的区域 反向KL散度（使用的）：p=0时q=0，可能导致q被挤压在p的一个峰上 在推导KL散度的表达式时就可以得到变分自编码器的损失函数ELBO。\n(图源Lilian Weng的博客：https://lilianweng.github.io/posts/2018-08-12-vae/)\n我们想同时极大化观测数据点$\\mathbf{x}$的似然，以及真假编码器的分布差距，即最大化 $$\\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z}|\\mathbf{x})}\\log p(\\mathbf{x}|\\mathbf{z})-D_{KL}(q(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))$$ 左边的是重构误差取反，右边的在先验分布为正态分布时可以显式展开。\n在计算重构误差时用到了重参数技巧（reparameterization trick），即把从一个带参数的编码器采样$\\mathbf{z}$，转化为从一个确定的分布（如标准正态）采样一个值，再通过将采样的值与编码器的输出（均值和方差）加减乘除得到$\\mathbf{z}$。这样梯度就和采样独立开来，可以反向传播了。","title":"VAE"},{"content":"FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。\n作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。\n输入：$\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}$\n输出：$\\mathbf{O}\\in\\mathbb{R}^{N\\times d}$\n标准self-attention：\n$\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^T\\in\\mathbb{R}^{N\\times N}$\n$\\mathbf{P}=\\exp(\\mathbf{S})$\n$\\mathbf{O}=\\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。\nFlash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\\mathbf{S}$。\n首先推导增量式的softmax函数：\n对一个向量$\\mathbf{x}$计算softmax：$\\sigma(\\mathbf{x})=\\exp(\\mathbf{x})/{\\sum_i {\\exp(\\mathbf{x}_i)}}$\n对两个向量的拼接$[\\mathbf{x},\\mathbf{y}]$计算softmax：$\\sigma([\\mathbf{x},\\mathbf{y}])=[\\exp(\\mathbf{x}),\\exp(\\mathbf{y})]/(\\sum_i\\exp(\\mathbf{x}_i)+\\sum_j\\exp(\\mathbf{y}_j))$\n设$l(\\mathbf{x})=\\sum_i\\exp(\\mathbf{x}_i)$，则$\\sigma([\\mathbf{x},\\mathbf{y}])=[\\exp(\\mathbf{x}),\\exp(\\mathbf{y})]/(l(\\mathbf{x})+l(\\mathbf{y}))$\n将$\\mathbf{Q,O},l$分成$T_r$块，将$\\mathbf{K,V}$分成$T_c$块，进行二重循环。\nfor j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：\n$\\mathbf{S}_{ij}=\\mathbf{Q}_i\\mathbf{K}_j^T$\n$\\mathbf{P}_{ij}=\\exp(\\mathbf{S}_{ij})$\n$l_{ij}=\\text{rowsum}(\\mathbf{P}_{ij})$\n$\\mathbf{O}_i\u0026rsquo;=\\mathbf{P}_{ij}\\mathbf{V}_j$\n然后需要对上一轮的$\\mathbf{O_i}$和$l_i$进行更新，以d=1为例。\n$l_i^{new}=l_i+l_{ij}$比较直接\n两个红色的矩阵相乘得到当前的$\\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\\mathbf{O}_i$由i行j列的内积得来，还需要加上$\\mathbf{O}_{ij}$，这样得到$\\mathbf{O}_i$的增量式更新：\n$\\mathbf{O}_i=\\mathbf{O}_i*l_i/l_i^{new} + \\mathbf{O}_{ij}$\n论文中的Algorithm1由于考虑了算术稳定性防止\\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。\n发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github\nA1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\\mathbf{O}$。但显然增加了$\\mathbf{K}_j$和$\\mathbf{V}_j$的IO次数。\nfor i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。\nfor i in 1...T_r: 取出Q_i,O_i和l_i for j in 1...T_c: 取出K_j和V_j ... 更新O_i和l_i Q2. $\\mathbf{O}_i$的更新表达式怎么得来的？\nA2. $\\mathbf{O}_i=\\mathbf{O}_i*l_i/l_i^{new} + \\mathbf{O}_{ij}$。$\\mathbf{O}_i$由$\\mathbf{S}$的第i行的softmax和$\\mathbf{V}$的第j列的内积得来。然而在这里分块计算时，softmax的分母，即对行的求和值$l$，是在不断更新的。只有到$\\mathbf{S}$的i行最后一列时才得到正确的l，因此有这样的增量更新表达式。\n同时，不按当前的算式更新，每次只累加softmax的分母，直到最后一列才除以$l$，肯定也是可以的。\nReferences [1] 我的flash-attention实现：awesome-papers/FlashAttention at main · yliuhz/awesome-papers (github.com)\n[2] Flash Attention论文地址：https://doi.org/10.48550/arXiv.2205.14135\n","permalink":"https://yliuhz.github.io/blogs/posts/flashattention/","summary":"FlashAttention论文发表于Neurips2022，第一单位是斯坦福大学。\n作者提出了一种使用更小代价计算self-attention的方法，并从理论上保证flash-attention给出的是精确的attention值，与现有的近似attention不同。作者指出现有方法专注于减少FLOPs，而本文专注于减少IO。\n输入：$\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}$\n输出：$\\mathbf{O}\\in\\mathbb{R}^{N\\times d}$\n标准self-attention：\n$\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^T\\in\\mathbb{R}^{N\\times N}$\n$\\mathbf{P}=\\exp(\\mathbf{S})$\n$\\mathbf{O}=\\mathbf{PV}/l(S)$，$l$始终表示向量元素求和或矩阵按行求和。\nFlash-attention的思路：分块在高速on-chip显存上增量式计算，避免平方空间的$\\mathbf{S}$。\n首先推导增量式的softmax函数：\n对一个向量$\\mathbf{x}$计算softmax：$\\sigma(\\mathbf{x})=\\exp(\\mathbf{x})/{\\sum_i {\\exp(\\mathbf{x}_i)}}$\n对两个向量的拼接$[\\mathbf{x},\\mathbf{y}]$计算softmax：$\\sigma([\\mathbf{x},\\mathbf{y}])=[\\exp(\\mathbf{x}),\\exp(\\mathbf{y})]/(\\sum_i\\exp(\\mathbf{x}_i)+\\sum_j\\exp(\\mathbf{y}_j))$\n设$l(\\mathbf{x})=\\sum_i\\exp(\\mathbf{x}_i)$，则$\\sigma([\\mathbf{x},\\mathbf{y}])=[\\exp(\\mathbf{x}),\\exp(\\mathbf{y})]/(l(\\mathbf{x})+l(\\mathbf{y}))$\n将$\\mathbf{Q,O},l$分成$T_r$块，将$\\mathbf{K,V}$分成$T_c$块，进行二重循环。\nfor j in 1...T_c: 取出K_j和V_j for i in 1...T_r: 取出Q_i,O_i和l_i 计算当前块内的self-attention，即：\n$\\mathbf{S}_{ij}=\\mathbf{Q}_i\\mathbf{K}_j^T$\n$\\mathbf{P}_{ij}=\\exp(\\mathbf{S}_{ij})$\n$l_{ij}=\\text{rowsum}(\\mathbf{P}_{ij})$\n$\\mathbf{O}_i\u0026rsquo;=\\mathbf{P}_{ij}\\mathbf{V}_j$\n然后需要对上一轮的$\\mathbf{O_i}$和$l_i$进行更新，以d=1为例。\n$l_i^{new}=l_i+l_{ij}$比较直接\n两个红色的矩阵相乘得到当前的$\\mathbf{O}_{ij}$。我们知道上一轮softmax使用的$l_i$只是当前i行的前部分之和，因此这里要乘以旧分母除以新分母，同时由于绿色$\\mathbf{O}_i$由i行j列的内积得来，还需要加上$\\mathbf{O}_{ij}$，这样得到$\\mathbf{O}_i$的增量式更新：\n$\\mathbf{O}_i=\\mathbf{O}_i*l_i/l_i^{new} + \\mathbf{O}_{ij}$\n论文中的Algorithm1由于考虑了算术稳定性防止\\exp得到过大的值，在softmax前减去了最大值m，因此看起来更复杂。\n发散QA Q1. Algorithm 1中的i、j循环可以交换吗？github\nA1. 如下可以，结果仍然保证Flash-Attention得到的是精确的$\\mathbf{O}$。但显然增加了$\\mathbf{K}_j$和$\\mathbf{V}_j$的IO次数。\nfor i in 1...T_r: for j in 1...T_c: 取出K_j和V_j 取出Q_i,O_i和l_i ... 更新O_i和l_i 如下不可以。\nfor i in 1...T_r: 取出Q_i,O_i和l_i for j in 1...T_c: 取出K_j和V_j ... 更新O_i和l_i Q2.","title":"FlashAttention"}]