[
  {
    "id": "dEMqP61a",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.",
    "DOI": "10.48550/arXiv.2110.08727",
    "note": "arXiv:2110.08727 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:doi.org/10.48550/arXiv.2110.08727",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation",
    "title-short": "Graph-less Neural Networks",
    "URL": "http://arxiv.org/abs/2110.08727",
    "author": [
      {
        "family": "Zhang",
        "given": "Shichang"
      },
      {
        "family": "Liu",
        "given": "Yozen"
      },
      {
        "family": "Sun",
        "given": "Yizhou"
      },
      {
        "family": "Shah",
        "given": "Neil"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          8
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          23
        ]
      ]
    }
  },
  {
    "id": "49r47O4e",
    "type": "article",
    "abstract": "While Graph Neural Networks (GNNs) have demonstrated their efficacy in dealing with non-Euclidean structural data, they are difficult to be deployed in real applications due to the scalability constraint imposed by multi-hop data dependency. Existing methods attempt to address this scalability issue by training multi-layer perceptrons (MLPs) exclusively on node content features using labels derived from trained GNNs. Even though the performance of MLPs can be significantly improved, two issues prevent MLPs from outperforming GNNs and being used in practice: the ignorance of graph structural information and the sensitivity to node feature noises. In this paper, we propose to learn NOise-robust Structure-aware MLPs On Graphs (NOSMOG) to overcome the challenges. Specifically, we first complement node content with position features to help MLPs capture graph structural information. We then design a novel representational similarity distillation strategy to inject structural node similarities into MLPs. Finally, we introduce the adversarial feature augmentation to ensure stable learning against feature noises and further improve performance. Extensive experiments demonstrate that NOSMOG outperforms GNNs and the state-of-the-art method in both transductive and inductive settings across seven datasets, while maintaining a competitive inference efficiency. Codes are available at https://github.com/meettyj/NOSMOG.",
    "DOI": "10.48550/arXiv.2208.10010",
    "note": "arXiv:2208.10010 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2208.10010",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs",
    "title-short": "NOSMOG",
    "URL": "http://arxiv.org/abs/2208.10010",
    "author": [
      {
        "family": "Tian",
        "given": "Yijun"
      },
      {
        "family": "Zhang",
        "given": "Chuxu"
      },
      {
        "family": "Guo",
        "given": "Zhichun"
      },
      {
        "family": "Zhang",
        "given": "Xiangliang"
      },
      {
        "family": "Chawla",
        "given": "Nitesh V."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          24
        ]
      ]
    }
  },
  {
    "id": "ByADi6ga",
    "type": "article",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems. As an initial step to analyze the inherent generalizability of GNNs, we show the essential difference between MLP and PMLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, by examining their extrapolation behavior, we find that though many GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extremely out-of-distribution samples, they have greater potential to generalize to testing samples near the training data range as natural advantages of GNN architectures.",
    "DOI": "10.48550/arXiv.2212.09034",
    "note": "arXiv:2212.09034 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2212.09034",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs",
    "title-short": "Graph Neural Networks are Inherently Good Generalizers",
    "URL": "http://arxiv.org/abs/2212.09034",
    "author": [
      {
        "family": "Yang",
        "given": "Chenxiao"
      },
      {
        "family": "Wu",
        "given": "Qitian"
      },
      {
        "family": "Wang",
        "given": "Jiahua"
      },
      {
        "family": "Yan",
        "given": "Junchi"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          5
        ]
      ]
    }
  },
  {
    "id": "6iMLCeK9",
    "type": "article",
    "abstract": "Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs.",
    "DOI": "10.48550/arXiv.2205.10803",
    "note": "arXiv:2205.10803 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2205.10803",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
    "title-short": "GraphMAE",
    "URL": "http://arxiv.org/abs/2205.10803",
    "author": [
      {
        "family": "Hou",
        "given": "Zhenyu"
      },
      {
        "family": "Liu",
        "given": "Xiao"
      },
      {
        "family": "Cen",
        "given": "Yukuo"
      },
      {
        "family": "Dong",
        "given": "Yuxiao"
      },
      {
        "family": "Yang",
        "given": "Hongxia"
      },
      {
        "family": "Wang",
        "given": "Chunjie"
      },
      {
        "family": "Tang",
        "given": "Jie"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          12
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          7,
          13
        ]
      ]
    }
  },
  {
    "id": "2hXkUYg3",
    "type": "article-journal",
    "container-title": "Journal of Statistical Mechanics: Theory and Experiment",
    "DOI": "10.1088/1742-5468/2008/10/P10008",
    "ISSN": "1742-5468",
    "issue": "10",
    "journalAbbreviation": "J. Stat. Mech.",
    "page": "P10008",
    "source": "DOI.org (Crossref)",
    "title": "Fast unfolding of communities in large networks",
    "URL": "https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008",
    "volume": "2008",
    "author": [
      {
        "family": "Blondel",
        "given": "Vincent D"
      },
      {
        "family": "Guillaume",
        "given": "Jean-Loup"
      },
      {
        "family": "Lambiotte",
        "given": "Renaud"
      },
      {
        "family": "Lefebvre",
        "given": "Etienne"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2008",
          10,
          9
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1088/1742-5468/2008/10/P10008"
  },
  {
    "id": "umB3JCfk",
    "type": "article-journal",
    "abstract": "Many real-world networks are so large that we must simplify their structure before we can extract useful information about the systems they represent. As the tools for doing these simplifications proliferate within the network literature, researchers would benefit from some guidelines about which of the so-called community detection algorithms are most appropriate for the structures they are studying and the questions they are asking. Here we show that different methods highlight different aspects of a network's structure and that the the sort of information that we seek to extract about the system must guide us in our decision. For example, many community detection algorithms, including the popular modularity maximization approach, infer module assignments from an underlying model of the network formation process. However, we are not always as interested in how a system's network structure was formed, as we are in how a network's extant structure influences the system's behavior. To see how structure influences current behavior, we will recognize that links in a network induce movement across the network and result in system-wide interdependence. In doing so, we explicitly acknowledge that most networks carry flow. To highlight and simplify the network structure with respect to this flow, we use the map equation. We present an intuitive derivation of this flow-based and information-theoretic method and provide an interactive on-line application that anyone can use to explore the mechanics of the map equation. The differences between the map equation and the modularity maximization approach are not merely conceptual. Because the map equation attends to patterns of flow on the network and the modularity maximization approach does not, the two methods can yield dramatically different results for some network structures. To illustrate this and build our understanding of each method, we partition several sample networks. We also describe an algorithm and provide source code to efficiently decompose large weighted and directed networks based on the map equation.",
    "container-title": "The European Physical Journal Special Topics",
    "DOI": "10.1140/epjst/e2010-01179-1",
    "ISSN": "1951-6401",
    "issue": "1",
    "journalAbbreviation": "Eur. Phys. J. Spec. Top.",
    "language": "en",
    "page": "13-23",
    "source": "Springer Link",
    "title": "The map equation",
    "URL": "https://doi.org/10.1140/epjst/e2010-01179-1",
    "volume": "178",
    "author": [
      {
        "family": "Rosvall",
        "given": "M."
      },
      {
        "family": "Axelsson",
        "given": "D."
      },
      {
        "family": "Bergstrom",
        "given": "C. T."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2009",
          11,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1140/epjst/e2010-01179-1"
  },
  {
    "id": "1AQ92Btmc",
    "type": "article-journal",
    "abstract": "Stochastic blockmodels have been proposed as a tool for detecting community structure in networks as well as for generating synthetic networks for use as benchmarks. Most blockmodels, however, ignore variation in vertex degree, making them unsuitable for applications to real-world networks, which typically display broad degree distributions that can significantly affect the results. Here we demonstrate how the generalization of blockmodels to incorporate this missing element leads to an improved objective function for community detection in complex networks. We also propose a heuristic algorithm for community detection using this objective function or its non-degree-corrected counterpart and show that the degree-corrected version dramatically outperforms the uncorrected one in both real-world and synthetic networks.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.83.016107",
    "issue": "1",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "016107",
    "source": "APS",
    "title": "Stochastic blockmodels and community structure in networks",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.83.016107",
    "volume": "83",
    "author": [
      {
        "family": "Karrer",
        "given": "Brian"
      },
      {
        "family": "Newman",
        "given": "M. E. J."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2011",
          1,
          21
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.83.016107"
  },
  {
    "id": "H0sxwTcP",
    "type": "article-journal",
    "abstract": "The stochastic block model is able to generate random graphs with different types of network partitions, ranging from the traditional assortative structures to the disassortative structures. Since the stochastic block model does not specify which mixing pattern is desired, the inference algorithms discover the locally most likely nodes’ partition, regardless of its type. Here we introduce a new model constraining nodes’ internal degree ratios in the objective function to guide the inference algorithms to converge to the desired type of structure in the observed network data. We show experimentally that given the regularized model, the inference algorithms, such as Markov chain Monte Carlo, reliably and quickly find the assortative or disassortative structure as directed by the value of a single parameter. In contrast, when the sought-after assortative community structure is not strong in the observed network, the traditional inference algorithms using the degree-corrected stochastic block model tend to converge to undesired disassortative partitions.",
    "container-title": "Scientific Reports",
    "DOI": "10.1038/s41598-019-49580-5",
    "ISSN": "2045-2322",
    "issue": "1",
    "journalAbbreviation": "Sci Rep",
    "language": "en",
    "page": "13247",
    "source": "www.nature.com",
    "title": "A Regularized Stochastic Block Model for the robust community detection in complex networks",
    "URL": "https://www.nature.com/articles/s41598-019-49580-5",
    "volume": "9",
    "author": [
      {
        "family": "Lu",
        "given": "Xiaoyan"
      },
      {
        "family": "Szymanski",
        "given": "Boleslaw K."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          9,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1038/s41598-019-49580-5"
  },
  {
    "id": "1P6Rfctx",
    "type": "article-journal",
    "abstract": "We present an efficient algorithm for the inference of stochastic block models in large networks. The algorithm can be used as an optimized Markov chain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced susceptibility to getting trapped in metastable states, or as a greedy agglomerative heuristic, with an almost linear O(Nln2N) complexity, where N is the number of nodes in the network, independent of the number of blocks being inferred. We show that the heuristic is capable of delivering results which are indistinguishable from the more exact and numerically expensive MCMC method in many artificial and empirical networks, despite being much faster. The method is entirely unbiased towards any specific mixing pattern, and in particular it does not favor assortative community structures.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.89.012804",
    "issue": "1",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "012804",
    "source": "APS",
    "title": "Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.89.012804",
    "volume": "89",
    "author": [
      {
        "family": "Peixoto",
        "given": "Tiago P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2014",
          1,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.89.012804"
  },
  {
    "id": "uqJs6E0V",
    "type": "article-journal",
    "abstract": "We propose and study a set of algorithms for discovering community structure in networks—natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible “betweenness” measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.69.026113",
    "issue": "2",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "026113",
    "source": "APS",
    "title": "Finding and evaluating community structure in networks",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.69.026113",
    "volume": "69",
    "author": [
      {
        "family": "Newman",
        "given": "M. E. J."
      },
      {
        "family": "Girvan",
        "given": "M."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          25
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2004",
          2,
          26
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.69.026113"
  },
  {
    "id": "18wo6Er1H",
    "type": "article",
    "abstract": "We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently \"diverse\". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.",
    "DOI": "10.48550/arXiv.2009.11848",
    "note": "arXiv:2009.11848 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2009.11848",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks",
    "title-short": "How Neural Networks Extrapolate",
    "URL": "http://arxiv.org/abs/2009.11848",
    "author": [
      {
        "family": "Xu",
        "given": "Keyulu"
      },
      {
        "family": "Zhang",
        "given": "Mozhi"
      },
      {
        "family": "Li",
        "given": "Jingling"
      },
      {
        "family": "Du",
        "given": "Simon S."
      },
      {
        "family": "Kawarabayashi",
        "given": "Ken-ichi"
      },
      {
        "family": "Jegelka",
        "given": "Stefanie"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          3,
          2
        ]
      ]
    }
  },
  {
    "id": "14nyanSAU",
    "type": "article",
    "abstract": "In recent years, graph neural networks (GNNs) have emerged as a successful tool in a variety of graph-related applications. However, the performance of GNNs can be deteriorated when noisy connections occur in the original graph structures; besides, the dependence on explicit structures prevents GNNs from being applied to general unstructured scenarios. To address these issues, recently emerged deep graph structure learning (GSL) methods propose to jointly optimize the graph structure along with GNN under the supervision of a node classification task. Nonetheless, these methods focus on a supervised learning scenario, which leads to several problems, i.e., the reliance on labels, the bias of edge distribution, and the limitation on application tasks. In this paper, we propose a more practical GSL paradigm, unsupervised graph structure learning, where the learned graph topology is optimized by data itself without any external guidance (i.e., labels). To solve the unsupervised GSL problem, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) with the aid of self-supervised contrastive learning. Specifically, we generate a learning target from the original data as an \"anchor graph\", and use a contrastive loss to maximize the agreement between the anchor graph and the learned graph. To provide persistent guidance, we design a novel bootstrapping mechanism that upgrades the anchor graph with learned structures during model learning. We also design a series of graph learners and post-processing schemes to model the structures to learn. Extensive experiments on eight benchmark datasets demonstrate the significant effectiveness of our proposed SUBLIME and high quality of the optimized graphs.",
    "DOI": "10.48550/arXiv.2201.06367",
    "note": "arXiv:2201.06367 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2201.06367",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Towards Unsupervised Deep Graph Structure Learning",
    "URL": "http://arxiv.org/abs/2201.06367",
    "author": [
      {
        "family": "Liu",
        "given": "Yixin"
      },
      {
        "family": "Zheng",
        "given": "Yu"
      },
      {
        "family": "Zhang",
        "given": "Daokun"
      },
      {
        "family": "Chen",
        "given": "Hongxu"
      },
      {
        "family": "Peng",
        "given": "Hao"
      },
      {
        "family": "Pan",
        "given": "Shirui"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          1,
          17
        ]
      ]
    }
  },
  {
    "id": "19EFWTVYY",
    "type": "article",
    "abstract": "Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.",
    "DOI": "10.48550/arXiv.2102.05034",
    "note": "arXiv:2102.05034 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2102.05034",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks",
    "title-short": "SLAPS",
    "URL": "http://arxiv.org/abs/2102.05034",
    "author": [
      {
        "family": "Fatemi",
        "given": "Bahare"
      },
      {
        "family": "Asri",
        "given": "Layla El"
      },
      {
        "family": "Kazemi",
        "given": "Seyed Mehran"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          10,
          31
        ]
      ]
    }
  },
  {
    "id": "PXPd62Wl",
    "type": "article",
    "abstract": "Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.",
    "DOI": "10.48550/arXiv.1911.05485",
    "note": "arXiv:1911.05485 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1911.05485",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Diffusion Improves Graph Learning",
    "URL": "http://arxiv.org/abs/1911.05485",
    "author": [
      {
        "family": "Gasteiger",
        "given": "Johannes"
      },
      {
        "family": "Weißenberger",
        "given": "Stefan"
      },
      {
        "family": "Günnemann",
        "given": "Stephan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          4,
          5
        ]
      ]
    }
  },
  {
    "id": "5H4Nt6Ww",
    "type": "article",
    "abstract": "We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: https://github.com/kavehhassani/mvgrl",
    "DOI": "10.48550/arXiv.2006.05582",
    "note": "arXiv:2006.05582 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2006.05582",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Contrastive Multi-View Representation Learning on Graphs",
    "URL": "http://arxiv.org/abs/2006.05582",
    "author": [
      {
        "family": "Hassani",
        "given": "Kaveh"
      },
      {
        "family": "Khasahmadi",
        "given": "Amir Hosein"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          6,
          9
        ]
      ]
    }
  },
  {
    "id": "3suxKdnN",
    "type": "article",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "DOI": "10.48550/arXiv.1706.03762",
    "note": "arXiv:1706.03762 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1706.03762",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Attention Is All You Need",
    "URL": "http://arxiv.org/abs/1706.03762",
    "author": [
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Polosukhin",
        "given": "Illia"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          10
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2017",
          12,
          5
        ]
      ]
    }
  },
  {
    "id": "r7iuiKky",
    "type": "article-journal",
    "abstract": "Stochastic blockmodel (SBM) is a widely used statistical network representation model, with good interpretability, expressiveness, generalization, and flexibility, which has become prevalent and important in the field of network science over the last years. However, learning an optimal SBM for a given network is an NP-hard problem. This results in significant limitations when it comes to applications of SBMs in large-scale networks, because of the significant computational overhead of existing SBM models, as well as their learning methods. Reducing the cost of SBM learning and making it scalable for handling large-scale networks, while maintaining the good theoretical properties of SBM, remains an unresolved problem. In this work, we address this challenging task from a novel perspective of model redefinition. We propose a novel redefined SBM with Poisson distribution and its block-wise learning algorithm that can efficiently analyse large-scale networks. Extensive validation conducted on both artificial and real-world data shows that our proposed method significantly outperforms the state-of-the-art methods in terms of a reasonable trade-off between accuracy and scalability.1",
    "container-title": "ACM Transactions on Knowledge Discovery from Data",
    "DOI": "10.1145/3442589",
    "ISSN": "1556-4681",
    "issue": "3",
    "journalAbbreviation": "ACM Trans. Knowl. Discov. Data",
    "page": "46:1–46:28",
    "source": "ACM Digital Library",
    "title": "A Scalable Redefined Stochastic Blockmodel",
    "URL": "https://doi.org/10.1145/3442589",
    "volume": "15",
    "author": [
      {
        "family": "Liu",
        "given": "Xueyan"
      },
      {
        "family": "Yang",
        "given": "Bo"
      },
      {
        "family": "Chen",
        "given": "Hechang"
      },
      {
        "family": "Musial",
        "given": "Katarzyna"
      },
      {
        "family": "Chen",
        "given": "Hongxu"
      },
      {
        "family": "Li",
        "given": "Yang"
      },
      {
        "family": "Zuo",
        "given": "Wanli"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          4,
          21
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1145/3442589"
  },
  {
    "id": "Lw8wOTy5",
    "type": "article",
    "abstract": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel \"readout\" mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
    "DOI": "10.48550/arXiv.2201.08821",
    "note": "arXiv:2201.08821 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2201.08821",
    "number": "arXiv:2201.08821",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention",
    "URL": "http://arxiv.org/abs/2201.08821",
    "author": [
      {
        "family": "Wu",
        "given": "Zhanghao"
      },
      {
        "family": "Jain",
        "given": "Paras"
      },
      {
        "family": "Wright",
        "given": "Matthew A."
      },
      {
        "family": "Mirhoseini",
        "given": "Azalia"
      },
      {
        "family": "Gonzalez",
        "given": "Joseph E."
      },
      {
        "family": "Stoica",
        "given": "Ion"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          18
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          1,
          21
        ]
      ]
    }
  },
  {
    "id": "YFznIUBN",
    "type": "article",
    "abstract": "Anti-money laundering (AML) regulations play a critical role in safeguarding financial systems, but bear high costs for institutions and drive financial exclusion for those on the socioeconomic and international margins. The advent of cryptocurrency has introduced an intriguing paradox: pseudonymity allows criminals to hide in plain sight, but open data gives more power to investigators and enables the crowdsourcing of forensic analysis. Meanwhile advances in learning algorithms show great promise for the AML toolkit. In this workshop tutorial, we motivate the opportunity to reconcile the cause of safety with that of financial inclusion. We contribute the Elliptic Data Set, a time series graph of over 200K Bitcoin transactions (nodes), 234K directed payment flows (edges), and 166 node features, including ones based on non-public data; to our knowledge, this is the largest labelled transaction data set publicly available in any cryptocurrency. We share results from a binary classification task predicting illicit transactions using variations of Logistic Regression (LR), Random Forest (RF), Multilayer Perceptrons (MLP), and Graph Convolutional Networks (GCN), with GCN being of special interest as an emergent new method for capturing relational information. The results show the superiority of Random Forest (RF), but also invite algorithmic work to combine the respective powers of RF and graph methods. Lastly, we consider visualization for analysis and explainability, which is difficult given the size and dynamism of real-world transaction graphs, and we offer a simple prototype capable of navigating the graph and observing model performance on illicit activity over time. With this tutorial and data set, we hope to a) invite feedback in support of our ongoing inquiry, and b) inspire others to work on this societally important challenge.",
    "DOI": "10.48550/arXiv.1908.02591",
    "note": "arXiv:1908.02591 [cs, q-fin]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1908.02591",
    "number": "arXiv:1908.02591",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics",
    "title-short": "Anti-Money Laundering in Bitcoin",
    "URL": "http://arxiv.org/abs/1908.02591",
    "author": [
      {
        "family": "Weber",
        "given": "Mark"
      },
      {
        "family": "Domeniconi",
        "given": "Giacomo"
      },
      {
        "family": "Chen",
        "given": "Jie"
      },
      {
        "family": "Weidele",
        "given": "Daniel Karl I."
      },
      {
        "family": "Bellei",
        "given": "Claudio"
      },
      {
        "family": "Robinson",
        "given": "Tom"
      },
      {
        "family": "Leiserson",
        "given": "Charles E."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          7,
          31
        ]
      ]
    }
  },
  {
    "id": "4QBCTyMI",
    "type": "article-journal",
    "abstract": "Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code is available at https://github.com/IBM/EvolveGCN.",
    "container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
    "DOI": "10.1609/aaai.v34i04.5984",
    "ISSN": "2374-3468",
    "issue": "04",
    "language": "en",
    "page": "5363-5370",
    "source": "ojs.aaai.org",
    "title": "EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs",
    "title-short": "EvolveGCN",
    "URL": "https://ojs.aaai.org/index.php/AAAI/article/view/5984",
    "volume": "34",
    "author": [
      {
        "family": "Pareja",
        "given": "Aldo"
      },
      {
        "family": "Domeniconi",
        "given": "Giacomo"
      },
      {
        "family": "Chen",
        "given": "Jie"
      },
      {
        "family": "Ma",
        "given": "Tengfei"
      },
      {
        "family": "Suzumura",
        "given": "Toyotaro"
      },
      {
        "family": "Kanezashi",
        "given": "Hiroki"
      },
      {
        "family": "Kaler",
        "given": "Tim"
      },
      {
        "family": "Schardl",
        "given": "Tao"
      },
      {
        "family": "Leiserson",
        "given": "Charles"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          4,
          3
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1609/aaai.v34i04.5984"
  },
  {
    "id": "9dxgnN5q",
    "type": "article",
    "abstract": "Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.",
    "DOI": "10.48550/arXiv.2305.19987",
    "note": "arXiv:2305.19987 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2305.19987",
    "number": "arXiv:2305.19987",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs",
    "title-short": "InGram",
    "URL": "http://arxiv.org/abs/2305.19987",
    "author": [
      {
        "family": "Lee",
        "given": "Jaejun"
      },
      {
        "family": "Chung",
        "given": "Chanyoung"
      },
      {
        "family": "Whang",
        "given": "Joyce Jiyoung"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          1
        ]
      ]
    }
  },
  {
    "id": "xUmHCWLY",
    "type": "article",
    "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library.",
    "DOI": "10.48550/arXiv.2105.14491",
    "note": "arXiv:2105.14491 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2105.14491",
    "number": "arXiv:2105.14491",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "How Attentive are Graph Attention Networks?",
    "URL": "http://arxiv.org/abs/2105.14491",
    "author": [
      {
        "family": "Brody",
        "given": "Shaked"
      },
      {
        "family": "Alon",
        "given": "Uri"
      },
      {
        "family": "Yahav",
        "given": "Eran"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          28
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          1,
          31
        ]
      ]
    }
  },
  {
    "id": "hl1cJIOU",
    "type": "paper-conference",
    "abstract": "In node classification tasks, graph convolutional neural networks (GCNs) have demonstrated competitive performance over traditional methods on diverse graph data. However, it is known that the performance of GCNs degrades with increasing number of layers (oversmoothing problem) and recent studies have also shown that GCNs may perform worse in heterophilous graphs, where neighboring nodes tend to belong to different classes (heterophily problem). These two problems are usually viewed as unrelated, and thus are studied independently, often at the graph filter level from a spectral perspective.We are the first to take a unified perspective to jointly explain the oversmoothing and heterophily problems at the node level. Specifically, we profile the nodes via two quantitative metrics: the relative degree of a node (compared to its neighbors) and the node-level heterophily. Our theory shows that the interplay of these two profiling metrics defines three cases of node behaviors, which explain the oversmoothing and heterophily problems jointly and can predict the performance of GCNs. Based on insights from our theory, we show theoretically and empirically the effectiveness of two strategies: structure-based edge correction, which learns corrected edge weights from structural properties (i.e., degrees), and feature-based edge correction, which learns signed edge weights from node features. Compared to other approaches, which tend to handle well either heterophily or oversmoothing, we show that our model, GGCN, which incorporates the two strategies performs well in both problems. We provide a longer version of this paper in [1] and codes on https://github.com/YujunYan/Heterophily_and_oversmoothing.",
    "container-title": "2022 IEEE International Conference on Data Mining (ICDM)",
    "DOI": "10.1109/ICDM54844.2022.00169",
    "event-title": "2022 IEEE International Conference on Data Mining (ICDM)",
    "note": "ISSN: 2374-8486\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1109/ICDM54844.2022.00169",
    "page": "1287-1292",
    "source": "IEEE Xplore",
    "title": "Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks",
    "title-short": "Two Sides of the Same Coin",
    "author": [
      {
        "family": "Yan",
        "given": "Yujun"
      },
      {
        "family": "Hashemi",
        "given": "Milad"
      },
      {
        "family": "Swersky",
        "given": "Kevin"
      },
      {
        "family": "Yang",
        "given": "Yaoqing"
      },
      {
        "family": "Koutra",
        "given": "Danai"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022",
          11
        ]
      ]
    },
    "URL": "https://doi.org/10.1109/ICDM54844.2022.00169"
  },
  {
    "id": "BebO8uzL",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-the-art GNNs on most tasks without incurring significant computational burden.",
    "DOI": "10.48550/arXiv.2210.07606",
    "note": "arXiv:2210.07606 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2210.07606",
    "number": "arXiv:2210.07606",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Revisiting Heterophily For Graph Neural Networks",
    "URL": "http://arxiv.org/abs/2210.07606",
    "author": [
      {
        "family": "Luan",
        "given": "Sitao"
      },
      {
        "family": "Hua",
        "given": "Chenqing"
      },
      {
        "family": "Lu",
        "given": "Qincheng"
      },
      {
        "family": "Zhu",
        "given": "Jiaqi"
      },
      {
        "family": "Zhao",
        "given": "Mingde"
      },
      {
        "family": "Zhang",
        "given": "Shuyuan"
      },
      {
        "family": "Chang",
        "given": "Xiao-Wen"
      },
      {
        "family": "Precup",
        "given": "Doina"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          1
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          10,
          14
        ]
      ]
    }
  },
  {
    "id": "4HJCEYmk",
    "type": "paper-conference",
    "container-title": "NeurIPS",
    "title": "OTKGE: Multi-modal Knowledge Graph Embeddings via Optimal Transport",
    "URL": "http://papers.nips.cc/paper\\_files/paper/2022/hash/ffdb280e7c7b4c4af30e04daf5a84b98-Abstract-Conference.html",
    "author": [
      {
        "family": "Cao",
        "given": "Zongsheng"
      },
      {
        "family": "Xu",
        "given": "Qianqian"
      },
      {
        "family": "Yang",
        "given": "Zhiyong"
      },
      {
        "family": "He",
        "given": "Yuan"
      },
      {
        "family": "Cao",
        "given": "Xiaochun"
      },
      {
        "family": "Huang",
        "given": "Qingming"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://dblp.org/rec/conf/nips/CaoXYHCH22.bib"
  },
  {
    "id": "N08DsO0I",
    "type": "article",
    "abstract": "We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-IGN) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks (GNN). When trained on a large-scale graph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer (TokenGT) achieves significantly better results compared to GNN baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt.",
    "DOI": "10.48550/arXiv.2207.02505",
    "note": "arXiv:2207.02505 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2207.02505",
    "number": "arXiv:2207.02505",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Pure Transformers are Powerful Graph Learners",
    "URL": "http://arxiv.org/abs/2207.02505",
    "author": [
      {
        "family": "Kim",
        "given": "Jinwoo"
      },
      {
        "family": "Nguyen",
        "given": "Tien Dat"
      },
      {
        "family": "Min",
        "given": "Seonwoo"
      },
      {
        "family": "Cho",
        "given": "Sungjun"
      },
      {
        "family": "Lee",
        "given": "Moontae"
      },
      {
        "family": "Lee",
        "given": "Honglak"
      },
      {
        "family": "Hong",
        "given": "Seunghoon"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          10,
          22
        ]
      ]
    }
  },
  {
    "id": "18JvSI3xc",
    "type": "article",
    "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \"X-former\" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored \"X-former\" models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
    "DOI": "10.48550/arXiv.2009.06732",
    "note": "arXiv:2009.06732 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2009.06732",
    "number": "arXiv:2009.06732",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Efficient Transformers: A Survey",
    "title-short": "Efficient Transformers",
    "URL": "http://arxiv.org/abs/2009.06732",
    "author": [
      {
        "family": "Tay",
        "given": "Yi"
      },
      {
        "family": "Dehghani",
        "given": "Mostafa"
      },
      {
        "family": "Bahri",
        "given": "Dara"
      },
      {
        "family": "Metzler",
        "given": "Donald"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          14
        ]
      ]
    }
  },
  {
    "id": "IHZXfRPy",
    "type": "article",
    "abstract": "Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",
    "DOI": "10.48550/arXiv.2103.03206",
    "note": "arXiv:2103.03206 [cs, eess]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2103.03206",
    "number": "arXiv:2103.03206",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Perceiver: General Perception with Iterative Attention",
    "title-short": "Perceiver",
    "URL": "http://arxiv.org/abs/2103.03206",
    "author": [
      {
        "family": "Jaegle",
        "given": "Andrew"
      },
      {
        "family": "Gimeno",
        "given": "Felix"
      },
      {
        "family": "Brock",
        "given": "Andrew"
      },
      {
        "family": "Zisserman",
        "given": "Andrew"
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Carreira",
        "given": "Joao"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          6,
          22
        ]
      ]
    }
  },
  {
    "id": "EijShNli",
    "type": "article",
    "abstract": "The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",
    "note": "arXiv:2106.05234 [cs]\nversion: 5\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2106.05234v5",
    "number": "arXiv:2106.05234",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Do Transformers Really Perform Bad for Graph Representation?",
    "URL": "http://arxiv.org/abs/2106.05234",
    "author": [
      {
        "family": "Ying",
        "given": "Chengxuan"
      },
      {
        "family": "Cai",
        "given": "Tianle"
      },
      {
        "family": "Luo",
        "given": "Shengjie"
      },
      {
        "family": "Zheng",
        "given": "Shuxin"
      },
      {
        "family": "Ke",
        "given": "Guolin"
      },
      {
        "family": "He",
        "given": "Di"
      },
      {
        "family": "Shen",
        "given": "Yanming"
      },
      {
        "family": "Liu",
        "given": "Tie-Yan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          11,
          23
        ]
      ]
    }
  },
  {
    "id": "19GdVA8uU",
    "type": "article",
    "abstract": "The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.",
    "DOI": "10.48550/arXiv.2002.04745",
    "note": "arXiv:2002.04745 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2002.04745",
    "number": "arXiv:2002.04745",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "On Layer Normalization in the Transformer Architecture",
    "URL": "http://arxiv.org/abs/2002.04745",
    "author": [
      {
        "family": "Xiong",
        "given": "Ruibin"
      },
      {
        "family": "Yang",
        "given": "Yunchang"
      },
      {
        "family": "He",
        "given": "Di"
      },
      {
        "family": "Zheng",
        "given": "Kai"
      },
      {
        "family": "Zheng",
        "given": "Shuxin"
      },
      {
        "family": "Xing",
        "given": "Chen"
      },
      {
        "family": "Zhang",
        "given": "Huishuai"
      },
      {
        "family": "Lan",
        "given": "Yanyan"
      },
      {
        "family": "Wang",
        "given": "Liwei"
      },
      {
        "family": "Liu",
        "given": "Tie-Yan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          6,
          29
        ]
      ]
    }
  },
  {
    "id": "mY7YPlVe",
    "type": "article",
    "abstract": "The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.",
    "DOI": "10.48550/arXiv.2102.11972",
    "note": "arXiv:2102.11972 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2102.11972",
    "number": "arXiv:2102.11972",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
    "URL": "http://arxiv.org/abs/2102.11972",
    "author": [
      {
        "family": "Narang",
        "given": "Sharan"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Tay",
        "given": "Yi"
      },
      {
        "family": "Fedus",
        "given": "William"
      },
      {
        "family": "Fevry",
        "given": "Thibault"
      },
      {
        "family": "Matena",
        "given": "Michael"
      },
      {
        "family": "Malkan",
        "given": "Karishma"
      },
      {
        "family": "Fiedel",
        "given": "Noah"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Lan",
        "given": "Zhenzhong"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Li",
        "given": "Wei"
      },
      {
        "family": "Ding",
        "given": "Nan"
      },
      {
        "family": "Marcus",
        "given": "Jake"
      },
      {
        "family": "Roberts",
        "given": "Adam"
      },
      {
        "family": "Raffel",
        "given": "Colin"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          9,
          10
        ]
      ]
    }
  },
  {
    "id": "1EWVWsRQN",
    "type": "article",
    "abstract": "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.",
    "DOI": "10.48550/arXiv.2305.10429",
    "note": "arXiv:2305.10429 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2305.10429",
    "number": "arXiv:2305.10429",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
    "title-short": "DoReMi",
    "URL": "http://arxiv.org/abs/2305.10429",
    "author": [
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Pham",
        "given": "Hieu"
      },
      {
        "family": "Dong",
        "given": "Xuanyi"
      },
      {
        "family": "Du",
        "given": "Nan"
      },
      {
        "family": "Liu",
        "given": "Hanxiao"
      },
      {
        "family": "Lu",
        "given": "Yifeng"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Le",
        "given": "Quoc V."
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      },
      {
        "family": "Yu",
        "given": "Adams Wei"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          15
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          5,
          24
        ]
      ]
    }
  },
  {
    "id": "FvCnruSt",
    "type": "article",
    "abstract": "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
    "DOI": "10.48550/arXiv.1911.00359",
    "note": "arXiv:1911.00359 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1911.00359",
    "number": "arXiv:1911.00359",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    "title-short": "CCNet",
    "URL": "http://arxiv.org/abs/1911.00359",
    "author": [
      {
        "family": "Wenzek",
        "given": "Guillaume"
      },
      {
        "family": "Lachaux",
        "given": "Marie-Anne"
      },
      {
        "family": "Conneau",
        "given": "Alexis"
      },
      {
        "family": "Chaudhary",
        "given": "Vishrav"
      },
      {
        "family": "Guzmán",
        "given": "Francisco"
      },
      {
        "family": "Joulin",
        "given": "Armand"
      },
      {
        "family": "Grave",
        "given": "Edouard"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          11,
          14
        ]
      ]
    }
  },
  {
    "id": "jBsuAX4y",
    "type": "article",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "DOI": "10.48550/arXiv.1412.6980",
    "note": "arXiv:1412.6980 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1412.6980",
    "number": "arXiv:1412.6980",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Adam: A Method for Stochastic Optimization",
    "title-short": "Adam",
    "URL": "http://arxiv.org/abs/1412.6980",
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P."
      },
      {
        "family": "Ba",
        "given": "Jimmy"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          23
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2017",
          1,
          29
        ]
      ]
    }
  },
  {
    "id": "1Bp0NnCGs",
    "type": "article-journal",
    "abstract": "We introduce a new typology and corresponding statistical models for characterizing the core-periphery structure of networks.\r\n          , \r\n            \r\n              Core-periphery structure, the arrangement of a network into a dense core and sparse periphery, is a versatile descriptor of various social, biological, and technological networks. In practice, different core-periphery algorithms are often applied interchangeably despite the fact that they can yield inconsistent descriptions of core-periphery structure. For example, two of the most widely used algorithms, the\r\n              k\r\n              -cores decomposition and the classic two-block model of Borgatti and Everett, extract fundamentally different structures: The latter partitions a network into a binary hub-and-spoke layout, while the former divides it into a layered hierarchy. We introduce a core-periphery typology to clarify these differences, along with Bayesian stochastic block modeling techniques to classify networks in accordance with this typology. Empirically, we find a rich diversity of core-periphery structure among networks. Through a detailed case study, we demonstrate the importance of acknowledging this diversity and situating networks within the core-periphery typology when conducting domain-specific analyses.",
    "container-title": "Science Advances",
    "DOI": "10.1126/sciadv.abc9800",
    "ISSN": "2375-2548",
    "issue": "12",
    "journalAbbreviation": "Sci. Adv.",
    "language": "en",
    "page": "eabc9800",
    "source": "DOI.org (Crossref)",
    "title": "A clarified typology of core-periphery structure in networks",
    "URL": "https://www.science.org/doi/10.1126/sciadv.abc9800",
    "volume": "7",
    "author": [
      {
        "family": "Gallagher",
        "given": "Ryan J."
      },
      {
        "family": "Young",
        "given": "Jean-Gabriel"
      },
      {
        "family": "Welles",
        "given": "Brooke Foucault"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          24
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          3,
          19
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1126/sciadv.abc9800"
  },
  {
    "id": "vbsKfS8",
    "type": "article",
    "abstract": "Various methods have been proposed in the literature to determine an optimal partitioning of the set of actors in a network into core and periphery subsets. However, these methods either work only for relatively small input sizes, or do not guarantee an optimal answer. In this paper, we propose a new algorithm to solve this problem. This algorithm is efficient and exact, allowing the optimal partitioning for networks of several thousand actors to be computed in under a second. We also show that the optimal core can be characterized as a set containing the actors with the highest degrees in the original network.",
    "DOI": "10.48550/arXiv.1102.5511",
    "note": "arXiv:1102.5511 [physics]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1102.5511",
    "number": "arXiv:1102.5511",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "A Fast Algorithm for the Discrete Core/Periphery Bipartitioning Problem",
    "URL": "http://arxiv.org/abs/1102.5511",
    "author": [
      {
        "family": "Lip",
        "given": "Sean Z. W."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          24
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2011",
          2,
          27
        ]
      ]
    }
  },
  {
    "id": "hf3NPvqm",
    "type": "article",
    "abstract": "The structure of large networks can be revealed by partitioning them to smaller parts, which are easier to handle. One of such decompositions is based on $k$--cores, proposed in 1983 by Seidman. In the paper an efficient, $O(m)$, $m$ is the number of lines, algorithm for determining the cores decomposition of a given network is presented.",
    "DOI": "10.48550/arXiv.cs/0310049",
    "note": "arXiv:cs/0310049\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.cs/0310049",
    "number": "arXiv:cs/0310049",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "An O(m) Algorithm for Cores Decomposition of Networks",
    "URL": "http://arxiv.org/abs/cs/0310049",
    "author": [
      {
        "family": "Batagelj",
        "given": "V."
      },
      {
        "family": "Zaversnik",
        "given": "M."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          24
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2003",
          10,
          25
        ]
      ]
    }
  },
  {
    "id": "VUtRX5J7",
    "type": "chapter",
    "abstract": "This chapter provides a self-contained introduction to the use of Bayesian inference to extract large-scale modular structures from network data, based on the stochastic blockmodel (SBM), as well as its degree-corrected and overlapping generalizations. We focus on nonparametric formulations that allow their inference in a manner that prevents overfitting, and enables model selection. We discuss aspects of the choice of priors, in particular how to avoid underfitting via increased Bayesian hierarchies, and we contrast the task of sampling network partitions from the posterior distribution with finding the single point estimate that maximizes it, while describing efficient algorithms to perform either one. We also show how inferring the SBM can be used to predict missing and spurious links, and shed light on the fundamental limitations of the detectability of modular structures in networks.",
    "note": "arXiv:1705.10225 [cond-mat, physics:physics, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1705.10225",
    "page": "289-332",
    "source": "arXiv.org",
    "title": "Bayesian stochastic blockmodeling",
    "URL": "http://arxiv.org/abs/1705.10225",
    "author": [
      {
        "family": "Peixoto",
        "given": "Tiago P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          30
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          11,
          23
        ]
      ]
    }
  },
  {
    "id": "rsnwcrs7",
    "type": "book",
    "abstract": "Community detection is one of the most important methodological fields of network science, and one which has attracted a significant amount of attention over the past decades. This area deals with the automated division of a network into fundamental building blocks, with the objective of providing a summary of its large-scale structure. Despite its importance and widespread adoption, there is a noticeable gap between what is arguably the state-of-the-art and the methods that are actually used in practice in a variety of fields. Here we attempt to address this discrepancy by dividing existing methods according to whether they have a \"descriptive\" or an \"inferential\" goal. While descriptive methods find patterns in networks based on context-dependent notions of community structure, inferential methods articulate generative models, and attempt to fit them to data. In this way, they are able to provide insights into the mechanisms of network formation, and separate structure from randomness in a manner supported by statistical evidence. We review how employing descriptive methods with inferential aims is riddled with pitfalls and misleading answers, and thus should be in general avoided. We argue that inferential methods are more typically aligned with clearer scientific questions, yield more robust results, and should be in many cases preferred. We attempt to dispel some myths and half-truths often believed when community detection is employed in practice, in an effort to improve both the use of such methods as well as the interpretation of their results.",
    "note": "arXiv:2112.00183 [physics, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2112.00183",
    "source": "arXiv.org",
    "title": "Descriptive vs. inferential community detection in networks: pitfalls, myths, and half-truths",
    "title-short": "Descriptive vs. inferential community detection in networks",
    "URL": "http://arxiv.org/abs/2112.00183",
    "author": [
      {
        "family": "Peixoto",
        "given": "Tiago P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          31
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          7,
          31
        ]
      ]
    }
  }
]