[
  {
    "id": "dEMqP61a",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.",
    "DOI": "10.48550/arXiv.2110.08727",
    "note": "arXiv:2110.08727 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:doi.org/10.48550/arXiv.2110.08727",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation",
    "title-short": "Graph-less Neural Networks",
    "URL": "http://arxiv.org/abs/2110.08727",
    "author": [
      {
        "family": "Zhang",
        "given": "Shichang"
      },
      {
        "family": "Liu",
        "given": "Yozen"
      },
      {
        "family": "Sun",
        "given": "Yizhou"
      },
      {
        "family": "Shah",
        "given": "Neil"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          8
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          23
        ]
      ]
    }
  },
  {
    "id": "49r47O4e",
    "type": "article",
    "abstract": "While Graph Neural Networks (GNNs) have demonstrated their efficacy in dealing with non-Euclidean structural data, they are difficult to be deployed in real applications due to the scalability constraint imposed by multi-hop data dependency. Existing methods attempt to address this scalability issue by training multi-layer perceptrons (MLPs) exclusively on node content features using labels derived from trained GNNs. Even though the performance of MLPs can be significantly improved, two issues prevent MLPs from outperforming GNNs and being used in practice: the ignorance of graph structural information and the sensitivity to node feature noises. In this paper, we propose to learn NOise-robust Structure-aware MLPs On Graphs (NOSMOG) to overcome the challenges. Specifically, we first complement node content with position features to help MLPs capture graph structural information. We then design a novel representational similarity distillation strategy to inject structural node similarities into MLPs. Finally, we introduce the adversarial feature augmentation to ensure stable learning against feature noises and further improve performance. Extensive experiments demonstrate that NOSMOG outperforms GNNs and the state-of-the-art method in both transductive and inductive settings across seven datasets, while maintaining a competitive inference efficiency. Codes are available at https://github.com/meettyj/NOSMOG.",
    "DOI": "10.48550/arXiv.2208.10010",
    "note": "arXiv:2208.10010 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2208.10010",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs",
    "title-short": "NOSMOG",
    "URL": "http://arxiv.org/abs/2208.10010",
    "author": [
      {
        "family": "Tian",
        "given": "Yijun"
      },
      {
        "family": "Zhang",
        "given": "Chuxu"
      },
      {
        "family": "Guo",
        "given": "Zhichun"
      },
      {
        "family": "Zhang",
        "given": "Xiangliang"
      },
      {
        "family": "Chawla",
        "given": "Nitesh V."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          24
        ]
      ]
    }
  },
  {
    "id": "ByADi6ga",
    "type": "article",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems. As an initial step to analyze the inherent generalizability of GNNs, we show the essential difference between MLP and PMLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, by examining their extrapolation behavior, we find that though many GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extremely out-of-distribution samples, they have greater potential to generalize to testing samples near the training data range as natural advantages of GNN architectures.",
    "DOI": "10.48550/arXiv.2212.09034",
    "note": "arXiv:2212.09034 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2212.09034",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs",
    "title-short": "Graph Neural Networks are Inherently Good Generalizers",
    "URL": "http://arxiv.org/abs/2212.09034",
    "author": [
      {
        "family": "Yang",
        "given": "Chenxiao"
      },
      {
        "family": "Wu",
        "given": "Qitian"
      },
      {
        "family": "Wang",
        "given": "Jiahua"
      },
      {
        "family": "Yan",
        "given": "Junchi"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          5
        ]
      ]
    }
  }
]