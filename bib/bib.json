[
  {
    "id": "dEMqP61a",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.",
    "DOI": "10.48550/arXiv.2110.08727",
    "note": "arXiv:2110.08727 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:doi.org/10.48550/arXiv.2110.08727",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation",
    "title-short": "Graph-less Neural Networks",
    "URL": "http://arxiv.org/abs/2110.08727",
    "author": [
      {
        "family": "Zhang",
        "given": "Shichang"
      },
      {
        "family": "Liu",
        "given": "Yozen"
      },
      {
        "family": "Sun",
        "given": "Yizhou"
      },
      {
        "family": "Shah",
        "given": "Neil"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          8
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          23
        ]
      ]
    }
  },
  {
    "id": "49r47O4e",
    "type": "article",
    "abstract": "While Graph Neural Networks (GNNs) have demonstrated their efficacy in dealing with non-Euclidean structural data, they are difficult to be deployed in real applications due to the scalability constraint imposed by multi-hop data dependency. Existing methods attempt to address this scalability issue by training multi-layer perceptrons (MLPs) exclusively on node content features using labels derived from trained GNNs. Even though the performance of MLPs can be significantly improved, two issues prevent MLPs from outperforming GNNs and being used in practice: the ignorance of graph structural information and the sensitivity to node feature noises. In this paper, we propose to learn NOise-robust Structure-aware MLPs On Graphs (NOSMOG) to overcome the challenges. Specifically, we first complement node content with position features to help MLPs capture graph structural information. We then design a novel representational similarity distillation strategy to inject structural node similarities into MLPs. Finally, we introduce the adversarial feature augmentation to ensure stable learning against feature noises and further improve performance. Extensive experiments demonstrate that NOSMOG outperforms GNNs and the state-of-the-art method in both transductive and inductive settings across seven datasets, while maintaining a competitive inference efficiency. Codes are available at https://github.com/meettyj/NOSMOG.",
    "DOI": "10.48550/arXiv.2208.10010",
    "note": "arXiv:2208.10010 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2208.10010",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs",
    "title-short": "NOSMOG",
    "URL": "http://arxiv.org/abs/2208.10010",
    "author": [
      {
        "family": "Tian",
        "given": "Yijun"
      },
      {
        "family": "Zhang",
        "given": "Chuxu"
      },
      {
        "family": "Guo",
        "given": "Zhichun"
      },
      {
        "family": "Zhang",
        "given": "Xiangliang"
      },
      {
        "family": "Chawla",
        "given": "Nitesh V."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          24
        ]
      ]
    }
  },
  {
    "id": "ByADi6ga",
    "type": "article",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems. As an initial step to analyze the inherent generalizability of GNNs, we show the essential difference between MLP and PMLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, by examining their extrapolation behavior, we find that though many GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extremely out-of-distribution samples, they have greater potential to generalize to testing samples near the training data range as natural advantages of GNN architectures.",
    "DOI": "10.48550/arXiv.2212.09034",
    "note": "arXiv:2212.09034 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2212.09034",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs",
    "title-short": "Graph Neural Networks are Inherently Good Generalizers",
    "URL": "http://arxiv.org/abs/2212.09034",
    "author": [
      {
        "family": "Yang",
        "given": "Chenxiao"
      },
      {
        "family": "Wu",
        "given": "Qitian"
      },
      {
        "family": "Wang",
        "given": "Jiahua"
      },
      {
        "family": "Yan",
        "given": "Junchi"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          5
        ]
      ]
    }
  },
  {
    "id": "6iMLCeK9",
    "type": "article",
    "abstract": "Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs.",
    "DOI": "10.48550/arXiv.2205.10803",
    "note": "arXiv:2205.10803 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2205.10803",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
    "title-short": "GraphMAE",
    "URL": "http://arxiv.org/abs/2205.10803",
    "author": [
      {
        "family": "Hou",
        "given": "Zhenyu"
      },
      {
        "family": "Liu",
        "given": "Xiao"
      },
      {
        "family": "Cen",
        "given": "Yukuo"
      },
      {
        "family": "Dong",
        "given": "Yuxiao"
      },
      {
        "family": "Yang",
        "given": "Hongxia"
      },
      {
        "family": "Wang",
        "given": "Chunjie"
      },
      {
        "family": "Tang",
        "given": "Jie"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          12
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          7,
          13
        ]
      ]
    }
  },
  {
    "id": "2hXkUYg3",
    "type": "article-journal",
    "container-title": "Journal of Statistical Mechanics: Theory and Experiment",
    "DOI": "10.1088/1742-5468/2008/10/P10008",
    "ISSN": "1742-5468",
    "issue": "10",
    "journalAbbreviation": "J. Stat. Mech.",
    "page": "P10008",
    "source": "DOI.org (Crossref)",
    "title": "Fast unfolding of communities in large networks",
    "URL": "https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008",
    "volume": "2008",
    "author": [
      {
        "family": "Blondel",
        "given": "Vincent D"
      },
      {
        "family": "Guillaume",
        "given": "Jean-Loup"
      },
      {
        "family": "Lambiotte",
        "given": "Renaud"
      },
      {
        "family": "Lefebvre",
        "given": "Etienne"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2008",
          10,
          9
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1088/1742-5468/2008/10/P10008"
  },
  {
    "id": "umB3JCfk",
    "type": "article-journal",
    "abstract": "Many real-world networks are so large that we must simplify their structure before we can extract useful information about the systems they represent. As the tools for doing these simplifications proliferate within the network literature, researchers would benefit from some guidelines about which of the so-called community detection algorithms are most appropriate for the structures they are studying and the questions they are asking. Here we show that different methods highlight different aspects of a network's structure and that the the sort of information that we seek to extract about the system must guide us in our decision. For example, many community detection algorithms, including the popular modularity maximization approach, infer module assignments from an underlying model of the network formation process. However, we are not always as interested in how a system's network structure was formed, as we are in how a network's extant structure influences the system's behavior. To see how structure influences current behavior, we will recognize that links in a network induce movement across the network and result in system-wide interdependence. In doing so, we explicitly acknowledge that most networks carry flow. To highlight and simplify the network structure with respect to this flow, we use the map equation. We present an intuitive derivation of this flow-based and information-theoretic method and provide an interactive on-line application that anyone can use to explore the mechanics of the map equation. The differences between the map equation and the modularity maximization approach are not merely conceptual. Because the map equation attends to patterns of flow on the network and the modularity maximization approach does not, the two methods can yield dramatically different results for some network structures. To illustrate this and build our understanding of each method, we partition several sample networks. We also describe an algorithm and provide source code to efficiently decompose large weighted and directed networks based on the map equation.",
    "container-title": "The European Physical Journal Special Topics",
    "DOI": "10.1140/epjst/e2010-01179-1",
    "ISSN": "1951-6401",
    "issue": "1",
    "journalAbbreviation": "Eur. Phys. J. Spec. Top.",
    "language": "en",
    "page": "13-23",
    "source": "Springer Link",
    "title": "The map equation",
    "URL": "https://doi.org/10.1140/epjst/e2010-01179-1",
    "volume": "178",
    "author": [
      {
        "family": "Rosvall",
        "given": "M."
      },
      {
        "family": "Axelsson",
        "given": "D."
      },
      {
        "family": "Bergstrom",
        "given": "C. T."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2009",
          11,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1140/epjst/e2010-01179-1"
  },
  {
    "id": "1AQ92Btmc",
    "type": "article-journal",
    "abstract": "Stochastic blockmodels have been proposed as a tool for detecting community structure in networks as well as for generating synthetic networks for use as benchmarks. Most blockmodels, however, ignore variation in vertex degree, making them unsuitable for applications to real-world networks, which typically display broad degree distributions that can significantly affect the results. Here we demonstrate how the generalization of blockmodels to incorporate this missing element leads to an improved objective function for community detection in complex networks. We also propose a heuristic algorithm for community detection using this objective function or its non-degree-corrected counterpart and show that the degree-corrected version dramatically outperforms the uncorrected one in both real-world and synthetic networks.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.83.016107",
    "issue": "1",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "016107",
    "source": "APS",
    "title": "Stochastic blockmodels and community structure in networks",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.83.016107",
    "volume": "83",
    "author": [
      {
        "family": "Karrer",
        "given": "Brian"
      },
      {
        "family": "Newman",
        "given": "M. E. J."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2011",
          1,
          21
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.83.016107"
  },
  {
    "id": "H0sxwTcP",
    "type": "article-journal",
    "abstract": "The stochastic block model is able to generate random graphs with different types of network partitions, ranging from the traditional assortative structures to the disassortative structures. Since the stochastic block model does not specify which mixing pattern is desired, the inference algorithms discover the locally most likely nodes’ partition, regardless of its type. Here we introduce a new model constraining nodes’ internal degree ratios in the objective function to guide the inference algorithms to converge to the desired type of structure in the observed network data. We show experimentally that given the regularized model, the inference algorithms, such as Markov chain Monte Carlo, reliably and quickly find the assortative or disassortative structure as directed by the value of a single parameter. In contrast, when the sought-after assortative community structure is not strong in the observed network, the traditional inference algorithms using the degree-corrected stochastic block model tend to converge to undesired disassortative partitions.",
    "container-title": "Scientific Reports",
    "DOI": "10.1038/s41598-019-49580-5",
    "ISSN": "2045-2322",
    "issue": "1",
    "journalAbbreviation": "Sci Rep",
    "language": "en",
    "page": "13247",
    "source": "www.nature.com",
    "title": "A Regularized Stochastic Block Model for the robust community detection in complex networks",
    "URL": "https://www.nature.com/articles/s41598-019-49580-5",
    "volume": "9",
    "author": [
      {
        "family": "Lu",
        "given": "Xiaoyan"
      },
      {
        "family": "Szymanski",
        "given": "Boleslaw K."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          9,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1038/s41598-019-49580-5"
  },
  {
    "id": "1P6Rfctx",
    "type": "article-journal",
    "abstract": "We present an efficient algorithm for the inference of stochastic block models in large networks. The algorithm can be used as an optimized Markov chain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced susceptibility to getting trapped in metastable states, or as a greedy agglomerative heuristic, with an almost linear O(Nln2N) complexity, where N is the number of nodes in the network, independent of the number of blocks being inferred. We show that the heuristic is capable of delivering results which are indistinguishable from the more exact and numerically expensive MCMC method in many artificial and empirical networks, despite being much faster. The method is entirely unbiased towards any specific mixing pattern, and in particular it does not favor assortative community structures.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.89.012804",
    "issue": "1",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "012804",
    "source": "APS",
    "title": "Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.89.012804",
    "volume": "89",
    "author": [
      {
        "family": "Peixoto",
        "given": "Tiago P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2014",
          1,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.89.012804"
  },
  {
    "id": "uqJs6E0V",
    "type": "article-journal",
    "abstract": "We propose and study a set of algorithms for discovering community structure in networks—natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible “betweenness” measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.69.026113",
    "issue": "2",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "026113",
    "source": "APS",
    "title": "Finding and evaluating community structure in networks",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.69.026113",
    "volume": "69",
    "author": [
      {
        "family": "Newman",
        "given": "M. E. J."
      },
      {
        "family": "Girvan",
        "given": "M."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          25
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2004",
          2,
          26
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.69.026113"
  }
]