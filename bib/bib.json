[
  {
    "id": "dEMqP61a",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.",
    "DOI": "10.48550/arXiv.2110.08727",
    "note": "arXiv:2110.08727 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:doi.org/10.48550/arXiv.2110.08727",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation",
    "title-short": "Graph-less Neural Networks",
    "URL": "http://arxiv.org/abs/2110.08727",
    "author": [
      {
        "family": "Zhang",
        "given": "Shichang"
      },
      {
        "family": "Liu",
        "given": "Yozen"
      },
      {
        "family": "Sun",
        "given": "Yizhou"
      },
      {
        "family": "Shah",
        "given": "Neil"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          8
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          23
        ]
      ]
    }
  },
  {
    "id": "49r47O4e",
    "type": "article",
    "abstract": "While Graph Neural Networks (GNNs) have demonstrated their efficacy in dealing with non-Euclidean structural data, they are difficult to be deployed in real applications due to the scalability constraint imposed by multi-hop data dependency. Existing methods attempt to address this scalability issue by training multi-layer perceptrons (MLPs) exclusively on node content features using labels derived from trained GNNs. Even though the performance of MLPs can be significantly improved, two issues prevent MLPs from outperforming GNNs and being used in practice: the ignorance of graph structural information and the sensitivity to node feature noises. In this paper, we propose to learn NOise-robust Structure-aware MLPs On Graphs (NOSMOG) to overcome the challenges. Specifically, we first complement node content with position features to help MLPs capture graph structural information. We then design a novel representational similarity distillation strategy to inject structural node similarities into MLPs. Finally, we introduce the adversarial feature augmentation to ensure stable learning against feature noises and further improve performance. Extensive experiments demonstrate that NOSMOG outperforms GNNs and the state-of-the-art method in both transductive and inductive settings across seven datasets, while maintaining a competitive inference efficiency. Codes are available at https://github.com/meettyj/NOSMOG.",
    "DOI": "10.48550/arXiv.2208.10010",
    "note": "arXiv:2208.10010 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2208.10010",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs",
    "title-short": "NOSMOG",
    "URL": "http://arxiv.org/abs/2208.10010",
    "author": [
      {
        "family": "Tian",
        "given": "Yijun"
      },
      {
        "family": "Zhang",
        "given": "Chuxu"
      },
      {
        "family": "Guo",
        "given": "Zhichun"
      },
      {
        "family": "Zhang",
        "given": "Xiangliang"
      },
      {
        "family": "Chawla",
        "given": "Nitesh V."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          24
        ]
      ]
    }
  },
  {
    "id": "ByADi6ga",
    "type": "article",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems. As an initial step to analyze the inherent generalizability of GNNs, we show the essential difference between MLP and PMLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, by examining their extrapolation behavior, we find that though many GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extremely out-of-distribution samples, they have greater potential to generalize to testing samples near the training data range as natural advantages of GNN architectures.",
    "DOI": "10.48550/arXiv.2212.09034",
    "note": "arXiv:2212.09034 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2212.09034",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs",
    "title-short": "Graph Neural Networks are Inherently Good Generalizers",
    "URL": "http://arxiv.org/abs/2212.09034",
    "author": [
      {
        "family": "Yang",
        "given": "Chenxiao"
      },
      {
        "family": "Wu",
        "given": "Qitian"
      },
      {
        "family": "Wang",
        "given": "Jiahua"
      },
      {
        "family": "Yan",
        "given": "Junchi"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          5
        ]
      ]
    }
  },
  {
    "id": "6iMLCeK9",
    "type": "article",
    "abstract": "Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs.",
    "DOI": "10.48550/arXiv.2205.10803",
    "note": "arXiv:2205.10803 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2205.10803",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
    "title-short": "GraphMAE",
    "URL": "http://arxiv.org/abs/2205.10803",
    "author": [
      {
        "family": "Hou",
        "given": "Zhenyu"
      },
      {
        "family": "Liu",
        "given": "Xiao"
      },
      {
        "family": "Cen",
        "given": "Yukuo"
      },
      {
        "family": "Dong",
        "given": "Yuxiao"
      },
      {
        "family": "Yang",
        "given": "Hongxia"
      },
      {
        "family": "Wang",
        "given": "Chunjie"
      },
      {
        "family": "Tang",
        "given": "Jie"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          12
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          7,
          13
        ]
      ]
    }
  },
  {
    "id": "2hXkUYg3",
    "type": "article-journal",
    "container-title": "Journal of Statistical Mechanics: Theory and Experiment",
    "DOI": "10.1088/1742-5468/2008/10/P10008",
    "ISSN": "1742-5468",
    "issue": "10",
    "journalAbbreviation": "J. Stat. Mech.",
    "page": "P10008",
    "source": "DOI.org (Crossref)",
    "title": "Fast unfolding of communities in large networks",
    "URL": "https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008",
    "volume": "2008",
    "author": [
      {
        "family": "Blondel",
        "given": "Vincent D"
      },
      {
        "family": "Guillaume",
        "given": "Jean-Loup"
      },
      {
        "family": "Lambiotte",
        "given": "Renaud"
      },
      {
        "family": "Lefebvre",
        "given": "Etienne"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2008",
          10,
          9
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1088/1742-5468/2008/10/P10008"
  },
  {
    "id": "umB3JCfk",
    "type": "article-journal",
    "abstract": "Many real-world networks are so large that we must simplify their structure before we can extract useful information about the systems they represent. As the tools for doing these simplifications proliferate within the network literature, researchers would benefit from some guidelines about which of the so-called community detection algorithms are most appropriate for the structures they are studying and the questions they are asking. Here we show that different methods highlight different aspects of a network's structure and that the the sort of information that we seek to extract about the system must guide us in our decision. For example, many community detection algorithms, including the popular modularity maximization approach, infer module assignments from an underlying model of the network formation process. However, we are not always as interested in how a system's network structure was formed, as we are in how a network's extant structure influences the system's behavior. To see how structure influences current behavior, we will recognize that links in a network induce movement across the network and result in system-wide interdependence. In doing so, we explicitly acknowledge that most networks carry flow. To highlight and simplify the network structure with respect to this flow, we use the map equation. We present an intuitive derivation of this flow-based and information-theoretic method and provide an interactive on-line application that anyone can use to explore the mechanics of the map equation. The differences between the map equation and the modularity maximization approach are not merely conceptual. Because the map equation attends to patterns of flow on the network and the modularity maximization approach does not, the two methods can yield dramatically different results for some network structures. To illustrate this and build our understanding of each method, we partition several sample networks. We also describe an algorithm and provide source code to efficiently decompose large weighted and directed networks based on the map equation.",
    "container-title": "The European Physical Journal Special Topics",
    "DOI": "10.1140/epjst/e2010-01179-1",
    "ISSN": "1951-6401",
    "issue": "1",
    "journalAbbreviation": "Eur. Phys. J. Spec. Top.",
    "language": "en",
    "page": "13-23",
    "source": "Springer Link",
    "title": "The map equation",
    "URL": "https://doi.org/10.1140/epjst/e2010-01179-1",
    "volume": "178",
    "author": [
      {
        "family": "Rosvall",
        "given": "M."
      },
      {
        "family": "Axelsson",
        "given": "D."
      },
      {
        "family": "Bergstrom",
        "given": "C. T."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2009",
          11,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1140/epjst/e2010-01179-1"
  },
  {
    "id": "1AQ92Btmc",
    "type": "article-journal",
    "abstract": "Stochastic blockmodels have been proposed as a tool for detecting community structure in networks as well as for generating synthetic networks for use as benchmarks. Most blockmodels, however, ignore variation in vertex degree, making them unsuitable for applications to real-world networks, which typically display broad degree distributions that can significantly affect the results. Here we demonstrate how the generalization of blockmodels to incorporate this missing element leads to an improved objective function for community detection in complex networks. We also propose a heuristic algorithm for community detection using this objective function or its non-degree-corrected counterpart and show that the degree-corrected version dramatically outperforms the uncorrected one in both real-world and synthetic networks.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.83.016107",
    "issue": "1",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "016107",
    "source": "APS",
    "title": "Stochastic blockmodels and community structure in networks",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.83.016107",
    "volume": "83",
    "author": [
      {
        "family": "Karrer",
        "given": "Brian"
      },
      {
        "family": "Newman",
        "given": "M. E. J."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2011",
          1,
          21
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.83.016107"
  },
  {
    "id": "H0sxwTcP",
    "type": "article-journal",
    "abstract": "The stochastic block model is able to generate random graphs with different types of network partitions, ranging from the traditional assortative structures to the disassortative structures. Since the stochastic block model does not specify which mixing pattern is desired, the inference algorithms discover the locally most likely nodes’ partition, regardless of its type. Here we introduce a new model constraining nodes’ internal degree ratios in the objective function to guide the inference algorithms to converge to the desired type of structure in the observed network data. We show experimentally that given the regularized model, the inference algorithms, such as Markov chain Monte Carlo, reliably and quickly find the assortative or disassortative structure as directed by the value of a single parameter. In contrast, when the sought-after assortative community structure is not strong in the observed network, the traditional inference algorithms using the degree-corrected stochastic block model tend to converge to undesired disassortative partitions.",
    "container-title": "Scientific Reports",
    "DOI": "10.1038/s41598-019-49580-5",
    "ISSN": "2045-2322",
    "issue": "1",
    "journalAbbreviation": "Sci Rep",
    "language": "en",
    "page": "13247",
    "source": "www.nature.com",
    "title": "A Regularized Stochastic Block Model for the robust community detection in complex networks",
    "URL": "https://www.nature.com/articles/s41598-019-49580-5",
    "volume": "9",
    "author": [
      {
        "family": "Lu",
        "given": "Xiaoyan"
      },
      {
        "family": "Szymanski",
        "given": "Boleslaw K."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          9,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1038/s41598-019-49580-5"
  },
  {
    "id": "1P6Rfctx",
    "type": "article-journal",
    "abstract": "We present an efficient algorithm for the inference of stochastic block models in large networks. The algorithm can be used as an optimized Markov chain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced susceptibility to getting trapped in metastable states, or as a greedy agglomerative heuristic, with an almost linear O(Nln2N) complexity, where N is the number of nodes in the network, independent of the number of blocks being inferred. We show that the heuristic is capable of delivering results which are indistinguishable from the more exact and numerically expensive MCMC method in many artificial and empirical networks, despite being much faster. The method is entirely unbiased towards any specific mixing pattern, and in particular it does not favor assortative community structures.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.89.012804",
    "issue": "1",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "012804",
    "source": "APS",
    "title": "Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.89.012804",
    "volume": "89",
    "author": [
      {
        "family": "Peixoto",
        "given": "Tiago P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          21
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2014",
          1,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.89.012804"
  },
  {
    "id": "uqJs6E0V",
    "type": "article-journal",
    "abstract": "We propose and study a set of algorithms for discovering community structure in networks—natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible “betweenness” measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.69.026113",
    "issue": "2",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "026113",
    "source": "APS",
    "title": "Finding and evaluating community structure in networks",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.69.026113",
    "volume": "69",
    "author": [
      {
        "family": "Newman",
        "given": "M. E. J."
      },
      {
        "family": "Girvan",
        "given": "M."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          25
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2004",
          2,
          26
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1103/PhysRevE.69.026113"
  },
  {
    "id": "18wo6Er1H",
    "type": "article",
    "abstract": "We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently \"diverse\". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.",
    "DOI": "10.48550/arXiv.2009.11848",
    "note": "arXiv:2009.11848 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2009.11848",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks",
    "title-short": "How Neural Networks Extrapolate",
    "URL": "http://arxiv.org/abs/2009.11848",
    "author": [
      {
        "family": "Xu",
        "given": "Keyulu"
      },
      {
        "family": "Zhang",
        "given": "Mozhi"
      },
      {
        "family": "Li",
        "given": "Jingling"
      },
      {
        "family": "Du",
        "given": "Simon S."
      },
      {
        "family": "Kawarabayashi",
        "given": "Ken-ichi"
      },
      {
        "family": "Jegelka",
        "given": "Stefanie"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          3,
          2
        ]
      ]
    }
  },
  {
    "id": "14nyanSAU",
    "type": "article",
    "abstract": "In recent years, graph neural networks (GNNs) have emerged as a successful tool in a variety of graph-related applications. However, the performance of GNNs can be deteriorated when noisy connections occur in the original graph structures; besides, the dependence on explicit structures prevents GNNs from being applied to general unstructured scenarios. To address these issues, recently emerged deep graph structure learning (GSL) methods propose to jointly optimize the graph structure along with GNN under the supervision of a node classification task. Nonetheless, these methods focus on a supervised learning scenario, which leads to several problems, i.e., the reliance on labels, the bias of edge distribution, and the limitation on application tasks. In this paper, we propose a more practical GSL paradigm, unsupervised graph structure learning, where the learned graph topology is optimized by data itself without any external guidance (i.e., labels). To solve the unsupervised GSL problem, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) with the aid of self-supervised contrastive learning. Specifically, we generate a learning target from the original data as an \"anchor graph\", and use a contrastive loss to maximize the agreement between the anchor graph and the learned graph. To provide persistent guidance, we design a novel bootstrapping mechanism that upgrades the anchor graph with learned structures during model learning. We also design a series of graph learners and post-processing schemes to model the structures to learn. Extensive experiments on eight benchmark datasets demonstrate the significant effectiveness of our proposed SUBLIME and high quality of the optimized graphs.",
    "DOI": "10.48550/arXiv.2201.06367",
    "note": "arXiv:2201.06367 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2201.06367",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Towards Unsupervised Deep Graph Structure Learning",
    "URL": "http://arxiv.org/abs/2201.06367",
    "author": [
      {
        "family": "Liu",
        "given": "Yixin"
      },
      {
        "family": "Zheng",
        "given": "Yu"
      },
      {
        "family": "Zhang",
        "given": "Daokun"
      },
      {
        "family": "Chen",
        "given": "Hongxu"
      },
      {
        "family": "Peng",
        "given": "Hao"
      },
      {
        "family": "Pan",
        "given": "Shirui"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          1,
          17
        ]
      ]
    }
  },
  {
    "id": "19EFWTVYY",
    "type": "article",
    "abstract": "Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.",
    "DOI": "10.48550/arXiv.2102.05034",
    "note": "arXiv:2102.05034 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2102.05034",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks",
    "title-short": "SLAPS",
    "URL": "http://arxiv.org/abs/2102.05034",
    "author": [
      {
        "family": "Fatemi",
        "given": "Bahare"
      },
      {
        "family": "Asri",
        "given": "Layla El"
      },
      {
        "family": "Kazemi",
        "given": "Seyed Mehran"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          10,
          31
        ]
      ]
    }
  },
  {
    "id": "PXPd62Wl",
    "type": "article",
    "abstract": "Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.",
    "DOI": "10.48550/arXiv.1911.05485",
    "note": "arXiv:1911.05485 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1911.05485",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Diffusion Improves Graph Learning",
    "URL": "http://arxiv.org/abs/1911.05485",
    "author": [
      {
        "family": "Gasteiger",
        "given": "Johannes"
      },
      {
        "family": "Weißenberger",
        "given": "Stefan"
      },
      {
        "family": "Günnemann",
        "given": "Stephan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          4,
          5
        ]
      ]
    }
  },
  {
    "id": "5H4Nt6Ww",
    "type": "article",
    "abstract": "We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: https://github.com/kavehhassani/mvgrl",
    "DOI": "10.48550/arXiv.2006.05582",
    "note": "arXiv:2006.05582 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2006.05582",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Contrastive Multi-View Representation Learning on Graphs",
    "URL": "http://arxiv.org/abs/2006.05582",
    "author": [
      {
        "family": "Hassani",
        "given": "Kaveh"
      },
      {
        "family": "Khasahmadi",
        "given": "Amir Hosein"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          6,
          9
        ]
      ]
    }
  },
  {
    "id": "3suxKdnN",
    "type": "article",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "DOI": "10.48550/arXiv.1706.03762",
    "note": "arXiv:1706.03762 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1706.03762",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Attention Is All You Need",
    "URL": "http://arxiv.org/abs/1706.03762",
    "author": [
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Polosukhin",
        "given": "Illia"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          10
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2017",
          12,
          5
        ]
      ]
    }
  },
  {
    "id": "r7iuiKky",
    "type": "article-journal",
    "abstract": "Stochastic blockmodel (SBM) is a widely used statistical network representation model, with good interpretability, expressiveness, generalization, and flexibility, which has become prevalent and important in the field of network science over the last years. However, learning an optimal SBM for a given network is an NP-hard problem. This results in significant limitations when it comes to applications of SBMs in large-scale networks, because of the significant computational overhead of existing SBM models, as well as their learning methods. Reducing the cost of SBM learning and making it scalable for handling large-scale networks, while maintaining the good theoretical properties of SBM, remains an unresolved problem. In this work, we address this challenging task from a novel perspective of model redefinition. We propose a novel redefined SBM with Poisson distribution and its block-wise learning algorithm that can efficiently analyse large-scale networks. Extensive validation conducted on both artificial and real-world data shows that our proposed method significantly outperforms the state-of-the-art methods in terms of a reasonable trade-off between accuracy and scalability.1",
    "container-title": "ACM Transactions on Knowledge Discovery from Data",
    "DOI": "10.1145/3442589",
    "ISSN": "1556-4681",
    "issue": "3",
    "journalAbbreviation": "ACM Trans. Knowl. Discov. Data",
    "page": "46:1–46:28",
    "source": "ACM Digital Library",
    "title": "A Scalable Redefined Stochastic Blockmodel",
    "URL": "https://doi.org/10.1145/3442589",
    "volume": "15",
    "author": [
      {
        "family": "Liu",
        "given": "Xueyan"
      },
      {
        "family": "Yang",
        "given": "Bo"
      },
      {
        "family": "Chen",
        "given": "Hechang"
      },
      {
        "family": "Musial",
        "given": "Katarzyna"
      },
      {
        "family": "Chen",
        "given": "Hongxu"
      },
      {
        "family": "Li",
        "given": "Yang"
      },
      {
        "family": "Zuo",
        "given": "Wanli"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          4,
          21
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1145/3442589"
  },
  {
    "id": "Lw8wOTy5",
    "type": "article",
    "abstract": "Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel \"readout\" mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.",
    "DOI": "10.48550/arXiv.2201.08821",
    "note": "arXiv:2201.08821 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2201.08821",
    "number": "arXiv:2201.08821",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention",
    "URL": "http://arxiv.org/abs/2201.08821",
    "author": [
      {
        "family": "Wu",
        "given": "Zhanghao"
      },
      {
        "family": "Jain",
        "given": "Paras"
      },
      {
        "family": "Wright",
        "given": "Matthew A."
      },
      {
        "family": "Mirhoseini",
        "given": "Azalia"
      },
      {
        "family": "Gonzalez",
        "given": "Joseph E."
      },
      {
        "family": "Stoica",
        "given": "Ion"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          18
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          1,
          21
        ]
      ]
    }
  },
  {
    "id": "YFznIUBN",
    "type": "article",
    "abstract": "Anti-money laundering (AML) regulations play a critical role in safeguarding financial systems, but bear high costs for institutions and drive financial exclusion for those on the socioeconomic and international margins. The advent of cryptocurrency has introduced an intriguing paradox: pseudonymity allows criminals to hide in plain sight, but open data gives more power to investigators and enables the crowdsourcing of forensic analysis. Meanwhile advances in learning algorithms show great promise for the AML toolkit. In this workshop tutorial, we motivate the opportunity to reconcile the cause of safety with that of financial inclusion. We contribute the Elliptic Data Set, a time series graph of over 200K Bitcoin transactions (nodes), 234K directed payment flows (edges), and 166 node features, including ones based on non-public data; to our knowledge, this is the largest labelled transaction data set publicly available in any cryptocurrency. We share results from a binary classification task predicting illicit transactions using variations of Logistic Regression (LR), Random Forest (RF), Multilayer Perceptrons (MLP), and Graph Convolutional Networks (GCN), with GCN being of special interest as an emergent new method for capturing relational information. The results show the superiority of Random Forest (RF), but also invite algorithmic work to combine the respective powers of RF and graph methods. Lastly, we consider visualization for analysis and explainability, which is difficult given the size and dynamism of real-world transaction graphs, and we offer a simple prototype capable of navigating the graph and observing model performance on illicit activity over time. With this tutorial and data set, we hope to a) invite feedback in support of our ongoing inquiry, and b) inspire others to work on this societally important challenge.",
    "DOI": "10.48550/arXiv.1908.02591",
    "note": "arXiv:1908.02591 [cs, q-fin]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1908.02591",
    "number": "arXiv:1908.02591",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics",
    "title-short": "Anti-Money Laundering in Bitcoin",
    "URL": "http://arxiv.org/abs/1908.02591",
    "author": [
      {
        "family": "Weber",
        "given": "Mark"
      },
      {
        "family": "Domeniconi",
        "given": "Giacomo"
      },
      {
        "family": "Chen",
        "given": "Jie"
      },
      {
        "family": "Weidele",
        "given": "Daniel Karl I."
      },
      {
        "family": "Bellei",
        "given": "Claudio"
      },
      {
        "family": "Robinson",
        "given": "Tom"
      },
      {
        "family": "Leiserson",
        "given": "Charles E."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          7,
          31
        ]
      ]
    }
  },
  {
    "id": "4QBCTyMI",
    "type": "article-journal",
    "abstract": "Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code is available at https://github.com/IBM/EvolveGCN.",
    "container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
    "DOI": "10.1609/aaai.v34i04.5984",
    "ISSN": "2374-3468",
    "issue": "04",
    "language": "en",
    "page": "5363-5370",
    "source": "ojs.aaai.org",
    "title": "EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs",
    "title-short": "EvolveGCN",
    "URL": "https://ojs.aaai.org/index.php/AAAI/article/view/5984",
    "volume": "34",
    "author": [
      {
        "family": "Pareja",
        "given": "Aldo"
      },
      {
        "family": "Domeniconi",
        "given": "Giacomo"
      },
      {
        "family": "Chen",
        "given": "Jie"
      },
      {
        "family": "Ma",
        "given": "Tengfei"
      },
      {
        "family": "Suzumura",
        "given": "Toyotaro"
      },
      {
        "family": "Kanezashi",
        "given": "Hiroki"
      },
      {
        "family": "Kaler",
        "given": "Tim"
      },
      {
        "family": "Schardl",
        "given": "Tao"
      },
      {
        "family": "Leiserson",
        "given": "Charles"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          26
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          4,
          3
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1609/aaai.v34i04.5984"
  },
  {
    "id": "9dxgnN5q",
    "type": "article",
    "abstract": "Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.",
    "DOI": "10.48550/arXiv.2305.19987",
    "note": "arXiv:2305.19987 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2305.19987",
    "number": "arXiv:2305.19987",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs",
    "title-short": "InGram",
    "URL": "http://arxiv.org/abs/2305.19987",
    "author": [
      {
        "family": "Lee",
        "given": "Jaejun"
      },
      {
        "family": "Chung",
        "given": "Chanyoung"
      },
      {
        "family": "Whang",
        "given": "Joyce Jiyoung"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          27
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          1
        ]
      ]
    }
  },
  {
    "id": "xUmHCWLY",
    "type": "article",
    "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library.",
    "DOI": "10.48550/arXiv.2105.14491",
    "note": "arXiv:2105.14491 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2105.14491",
    "number": "arXiv:2105.14491",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "How Attentive are Graph Attention Networks?",
    "URL": "http://arxiv.org/abs/2105.14491",
    "author": [
      {
        "family": "Brody",
        "given": "Shaked"
      },
      {
        "family": "Alon",
        "given": "Uri"
      },
      {
        "family": "Yahav",
        "given": "Eran"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          7,
          28
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          1,
          31
        ]
      ]
    }
  },
  {
    "id": "hl1cJIOU",
    "type": "paper-conference",
    "abstract": "In node classification tasks, graph convolutional neural networks (GCNs) have demonstrated competitive performance over traditional methods on diverse graph data. However, it is known that the performance of GCNs degrades with increasing number of layers (oversmoothing problem) and recent studies have also shown that GCNs may perform worse in heterophilous graphs, where neighboring nodes tend to belong to different classes (heterophily problem). These two problems are usually viewed as unrelated, and thus are studied independently, often at the graph filter level from a spectral perspective.We are the first to take a unified perspective to jointly explain the oversmoothing and heterophily problems at the node level. Specifically, we profile the nodes via two quantitative metrics: the relative degree of a node (compared to its neighbors) and the node-level heterophily. Our theory shows that the interplay of these two profiling metrics defines three cases of node behaviors, which explain the oversmoothing and heterophily problems jointly and can predict the performance of GCNs. Based on insights from our theory, we show theoretically and empirically the effectiveness of two strategies: structure-based edge correction, which learns corrected edge weights from structural properties (i.e., degrees), and feature-based edge correction, which learns signed edge weights from node features. Compared to other approaches, which tend to handle well either heterophily or oversmoothing, we show that our model, GGCN, which incorporates the two strategies performs well in both problems. We provide a longer version of this paper in [1] and codes on https://github.com/YujunYan/Heterophily_and_oversmoothing.",
    "container-title": "2022 IEEE International Conference on Data Mining (ICDM)",
    "DOI": "10.1109/ICDM54844.2022.00169",
    "event-title": "2022 IEEE International Conference on Data Mining (ICDM)",
    "note": "ISSN: 2374-8486\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1109/ICDM54844.2022.00169",
    "page": "1287-1292",
    "source": "IEEE Xplore",
    "title": "Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks",
    "title-short": "Two Sides of the Same Coin",
    "author": [
      {
        "family": "Yan",
        "given": "Yujun"
      },
      {
        "family": "Hashemi",
        "given": "Milad"
      },
      {
        "family": "Swersky",
        "given": "Kevin"
      },
      {
        "family": "Yang",
        "given": "Yaoqing"
      },
      {
        "family": "Koutra",
        "given": "Danai"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022",
          11
        ]
      ]
    },
    "URL": "https://doi.org/10.1109/ICDM54844.2022.00169"
  },
  {
    "id": "BebO8uzL",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-the-art GNNs on most tasks without incurring significant computational burden.",
    "DOI": "10.48550/arXiv.2210.07606",
    "note": "arXiv:2210.07606 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2210.07606",
    "number": "arXiv:2210.07606",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Revisiting Heterophily For Graph Neural Networks",
    "URL": "http://arxiv.org/abs/2210.07606",
    "author": [
      {
        "family": "Luan",
        "given": "Sitao"
      },
      {
        "family": "Hua",
        "given": "Chenqing"
      },
      {
        "family": "Lu",
        "given": "Qincheng"
      },
      {
        "family": "Zhu",
        "given": "Jiaqi"
      },
      {
        "family": "Zhao",
        "given": "Mingde"
      },
      {
        "family": "Zhang",
        "given": "Shuyuan"
      },
      {
        "family": "Chang",
        "given": "Xiao-Wen"
      },
      {
        "family": "Precup",
        "given": "Doina"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          1
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          10,
          14
        ]
      ]
    }
  },
  {
    "id": "4HJCEYmk",
    "type": "paper-conference",
    "container-title": "NeurIPS",
    "title": "OTKGE: Multi-modal Knowledge Graph Embeddings via Optimal Transport",
    "URL": "http://papers.nips.cc/paper\\_files/paper/2022/hash/ffdb280e7c7b4c4af30e04daf5a84b98-Abstract-Conference.html",
    "author": [
      {
        "family": "Cao",
        "given": "Zongsheng"
      },
      {
        "family": "Xu",
        "given": "Qianqian"
      },
      {
        "family": "Yang",
        "given": "Zhiyong"
      },
      {
        "family": "He",
        "given": "Yuan"
      },
      {
        "family": "Cao",
        "given": "Xiaochun"
      },
      {
        "family": "Huang",
        "given": "Qingming"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://dblp.org/rec/conf/nips/CaoXYHCH22.bib"
  },
  {
    "id": "N08DsO0I",
    "type": "article",
    "abstract": "We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-IGN) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks (GNN). When trained on a large-scale graph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer (TokenGT) achieves significantly better results compared to GNN baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt.",
    "DOI": "10.48550/arXiv.2207.02505",
    "note": "arXiv:2207.02505 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2207.02505",
    "number": "arXiv:2207.02505",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Pure Transformers are Powerful Graph Learners",
    "URL": "http://arxiv.org/abs/2207.02505",
    "author": [
      {
        "family": "Kim",
        "given": "Jinwoo"
      },
      {
        "family": "Nguyen",
        "given": "Tien Dat"
      },
      {
        "family": "Min",
        "given": "Seonwoo"
      },
      {
        "family": "Cho",
        "given": "Sungjun"
      },
      {
        "family": "Lee",
        "given": "Moontae"
      },
      {
        "family": "Lee",
        "given": "Honglak"
      },
      {
        "family": "Hong",
        "given": "Seunghoon"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          10,
          22
        ]
      ]
    }
  },
  {
    "id": "18JvSI3xc",
    "type": "article",
    "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \"X-former\" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored \"X-former\" models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
    "DOI": "10.48550/arXiv.2009.06732",
    "note": "arXiv:2009.06732 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2009.06732",
    "number": "arXiv:2009.06732",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Efficient Transformers: A Survey",
    "title-short": "Efficient Transformers",
    "URL": "http://arxiv.org/abs/2009.06732",
    "author": [
      {
        "family": "Tay",
        "given": "Yi"
      },
      {
        "family": "Dehghani",
        "given": "Mostafa"
      },
      {
        "family": "Bahri",
        "given": "Dara"
      },
      {
        "family": "Metzler",
        "given": "Donald"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          14
        ]
      ]
    }
  },
  {
    "id": "IHZXfRPy",
    "type": "article",
    "abstract": "Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",
    "DOI": "10.48550/arXiv.2103.03206",
    "note": "arXiv:2103.03206 [cs, eess]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2103.03206",
    "number": "arXiv:2103.03206",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Perceiver: General Perception with Iterative Attention",
    "title-short": "Perceiver",
    "URL": "http://arxiv.org/abs/2103.03206",
    "author": [
      {
        "family": "Jaegle",
        "given": "Andrew"
      },
      {
        "family": "Gimeno",
        "given": "Felix"
      },
      {
        "family": "Brock",
        "given": "Andrew"
      },
      {
        "family": "Zisserman",
        "given": "Andrew"
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Carreira",
        "given": "Joao"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          6,
          22
        ]
      ]
    }
  },
  {
    "id": "EijShNli",
    "type": "article",
    "abstract": "The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",
    "note": "arXiv:2106.05234 [cs]\nversion: 5\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2106.05234v5",
    "number": "arXiv:2106.05234",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Do Transformers Really Perform Bad for Graph Representation?",
    "URL": "http://arxiv.org/abs/2106.05234",
    "author": [
      {
        "family": "Ying",
        "given": "Chengxuan"
      },
      {
        "family": "Cai",
        "given": "Tianle"
      },
      {
        "family": "Luo",
        "given": "Shengjie"
      },
      {
        "family": "Zheng",
        "given": "Shuxin"
      },
      {
        "family": "Ke",
        "given": "Guolin"
      },
      {
        "family": "He",
        "given": "Di"
      },
      {
        "family": "Shen",
        "given": "Yanming"
      },
      {
        "family": "Liu",
        "given": "Tie-Yan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          11,
          23
        ]
      ]
    }
  },
  {
    "id": "19GdVA8uU",
    "type": "article",
    "abstract": "The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.",
    "DOI": "10.48550/arXiv.2002.04745",
    "note": "arXiv:2002.04745 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2002.04745",
    "number": "arXiv:2002.04745",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "On Layer Normalization in the Transformer Architecture",
    "URL": "http://arxiv.org/abs/2002.04745",
    "author": [
      {
        "family": "Xiong",
        "given": "Ruibin"
      },
      {
        "family": "Yang",
        "given": "Yunchang"
      },
      {
        "family": "He",
        "given": "Di"
      },
      {
        "family": "Zheng",
        "given": "Kai"
      },
      {
        "family": "Zheng",
        "given": "Shuxin"
      },
      {
        "family": "Xing",
        "given": "Chen"
      },
      {
        "family": "Zhang",
        "given": "Huishuai"
      },
      {
        "family": "Lan",
        "given": "Yanyan"
      },
      {
        "family": "Wang",
        "given": "Liwei"
      },
      {
        "family": "Liu",
        "given": "Tie-Yan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          6,
          29
        ]
      ]
    }
  },
  {
    "id": "mY7YPlVe",
    "type": "article",
    "abstract": "The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.",
    "DOI": "10.48550/arXiv.2102.11972",
    "note": "arXiv:2102.11972 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2102.11972",
    "number": "arXiv:2102.11972",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
    "URL": "http://arxiv.org/abs/2102.11972",
    "author": [
      {
        "family": "Narang",
        "given": "Sharan"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Tay",
        "given": "Yi"
      },
      {
        "family": "Fedus",
        "given": "William"
      },
      {
        "family": "Fevry",
        "given": "Thibault"
      },
      {
        "family": "Matena",
        "given": "Michael"
      },
      {
        "family": "Malkan",
        "given": "Karishma"
      },
      {
        "family": "Fiedel",
        "given": "Noah"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Lan",
        "given": "Zhenzhong"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Li",
        "given": "Wei"
      },
      {
        "family": "Ding",
        "given": "Nan"
      },
      {
        "family": "Marcus",
        "given": "Jake"
      },
      {
        "family": "Roberts",
        "given": "Adam"
      },
      {
        "family": "Raffel",
        "given": "Colin"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          7
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          9,
          10
        ]
      ]
    }
  },
  {
    "id": "1EWVWsRQN",
    "type": "article",
    "abstract": "The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using domain weights tuned on downstream tasks.",
    "DOI": "10.48550/arXiv.2305.10429",
    "note": "arXiv:2305.10429 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2305.10429",
    "number": "arXiv:2305.10429",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
    "title-short": "DoReMi",
    "URL": "http://arxiv.org/abs/2305.10429",
    "author": [
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Pham",
        "given": "Hieu"
      },
      {
        "family": "Dong",
        "given": "Xuanyi"
      },
      {
        "family": "Du",
        "given": "Nan"
      },
      {
        "family": "Liu",
        "given": "Hanxiao"
      },
      {
        "family": "Lu",
        "given": "Yifeng"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Le",
        "given": "Quoc V."
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      },
      {
        "family": "Yu",
        "given": "Adams Wei"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          15
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          5,
          24
        ]
      ]
    }
  },
  {
    "id": "FvCnruSt",
    "type": "article",
    "abstract": "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
    "DOI": "10.48550/arXiv.1911.00359",
    "note": "arXiv:1911.00359 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1911.00359",
    "number": "arXiv:1911.00359",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    "title-short": "CCNet",
    "URL": "http://arxiv.org/abs/1911.00359",
    "author": [
      {
        "family": "Wenzek",
        "given": "Guillaume"
      },
      {
        "family": "Lachaux",
        "given": "Marie-Anne"
      },
      {
        "family": "Conneau",
        "given": "Alexis"
      },
      {
        "family": "Chaudhary",
        "given": "Vishrav"
      },
      {
        "family": "Guzmán",
        "given": "Francisco"
      },
      {
        "family": "Joulin",
        "given": "Armand"
      },
      {
        "family": "Grave",
        "given": "Edouard"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          11,
          14
        ]
      ]
    }
  },
  {
    "id": "jBsuAX4y",
    "type": "article",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "DOI": "10.48550/arXiv.1412.6980",
    "note": "arXiv:1412.6980 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1412.6980",
    "number": "arXiv:1412.6980",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Adam: A Method for Stochastic Optimization",
    "title-short": "Adam",
    "URL": "http://arxiv.org/abs/1412.6980",
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P."
      },
      {
        "family": "Ba",
        "given": "Jimmy"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          23
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2017",
          1,
          29
        ]
      ]
    }
  },
  {
    "id": "1Bp0NnCGs",
    "type": "article-journal",
    "abstract": "We introduce a new typology and corresponding statistical models for characterizing the core-periphery structure of networks.\r\n          , \r\n            \r\n              Core-periphery structure, the arrangement of a network into a dense core and sparse periphery, is a versatile descriptor of various social, biological, and technological networks. In practice, different core-periphery algorithms are often applied interchangeably despite the fact that they can yield inconsistent descriptions of core-periphery structure. For example, two of the most widely used algorithms, the\r\n              k\r\n              -cores decomposition and the classic two-block model of Borgatti and Everett, extract fundamentally different structures: The latter partitions a network into a binary hub-and-spoke layout, while the former divides it into a layered hierarchy. We introduce a core-periphery typology to clarify these differences, along with Bayesian stochastic block modeling techniques to classify networks in accordance with this typology. Empirically, we find a rich diversity of core-periphery structure among networks. Through a detailed case study, we demonstrate the importance of acknowledging this diversity and situating networks within the core-periphery typology when conducting domain-specific analyses.",
    "container-title": "Science Advances",
    "DOI": "10.1126/sciadv.abc9800",
    "ISSN": "2375-2548",
    "issue": "12",
    "journalAbbreviation": "Sci. Adv.",
    "language": "en",
    "page": "eabc9800",
    "source": "DOI.org (Crossref)",
    "title": "A clarified typology of core-periphery structure in networks",
    "URL": "https://www.science.org/doi/10.1126/sciadv.abc9800",
    "volume": "7",
    "author": [
      {
        "family": "Gallagher",
        "given": "Ryan J."
      },
      {
        "family": "Young",
        "given": "Jean-Gabriel"
      },
      {
        "family": "Welles",
        "given": "Brooke Foucault"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          24
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          3,
          19
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1126/sciadv.abc9800"
  },
  {
    "id": "vbsKfS8",
    "type": "article",
    "abstract": "Various methods have been proposed in the literature to determine an optimal partitioning of the set of actors in a network into core and periphery subsets. However, these methods either work only for relatively small input sizes, or do not guarantee an optimal answer. In this paper, we propose a new algorithm to solve this problem. This algorithm is efficient and exact, allowing the optimal partitioning for networks of several thousand actors to be computed in under a second. We also show that the optimal core can be characterized as a set containing the actors with the highest degrees in the original network.",
    "DOI": "10.48550/arXiv.1102.5511",
    "note": "arXiv:1102.5511 [physics]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1102.5511",
    "number": "arXiv:1102.5511",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "A Fast Algorithm for the Discrete Core/Periphery Bipartitioning Problem",
    "URL": "http://arxiv.org/abs/1102.5511",
    "author": [
      {
        "family": "Lip",
        "given": "Sean Z. W."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          24
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2011",
          2,
          27
        ]
      ]
    }
  },
  {
    "id": "hf3NPvqm",
    "type": "article",
    "abstract": "The structure of large networks can be revealed by partitioning them to smaller parts, which are easier to handle. One of such decompositions is based on $k$--cores, proposed in 1983 by Seidman. In the paper an efficient, $O(m)$, $m$ is the number of lines, algorithm for determining the cores decomposition of a given network is presented.",
    "DOI": "10.48550/arXiv.cs/0310049",
    "note": "arXiv:cs/0310049\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.cs/0310049",
    "number": "arXiv:cs/0310049",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "An O(m) Algorithm for Cores Decomposition of Networks",
    "URL": "http://arxiv.org/abs/cs/0310049",
    "author": [
      {
        "family": "Batagelj",
        "given": "V."
      },
      {
        "family": "Zaversnik",
        "given": "M."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          24
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2003",
          10,
          25
        ]
      ]
    }
  },
  {
    "id": "VUtRX5J7",
    "type": "chapter",
    "abstract": "This chapter provides a self-contained introduction to the use of Bayesian inference to extract large-scale modular structures from network data, based on the stochastic blockmodel (SBM), as well as its degree-corrected and overlapping generalizations. We focus on nonparametric formulations that allow their inference in a manner that prevents overfitting, and enables model selection. We discuss aspects of the choice of priors, in particular how to avoid underfitting via increased Bayesian hierarchies, and we contrast the task of sampling network partitions from the posterior distribution with finding the single point estimate that maximizes it, while describing efficient algorithms to perform either one. We also show how inferring the SBM can be used to predict missing and spurious links, and shed light on the fundamental limitations of the detectability of modular structures in networks.",
    "note": "arXiv:1705.10225 [cond-mat, physics:physics, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1705.10225",
    "page": "289-332",
    "source": "arXiv.org",
    "title": "Bayesian stochastic blockmodeling",
    "URL": "http://arxiv.org/abs/1705.10225",
    "author": [
      {
        "family": "Peixoto",
        "given": "Tiago P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          30
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          11,
          23
        ]
      ]
    }
  },
  {
    "id": "rsnwcrs7",
    "type": "book",
    "abstract": "Community detection is one of the most important methodological fields of network science, and one which has attracted a significant amount of attention over the past decades. This area deals with the automated division of a network into fundamental building blocks, with the objective of providing a summary of its large-scale structure. Despite its importance and widespread adoption, there is a noticeable gap between what is arguably the state-of-the-art and the methods that are actually used in practice in a variety of fields. Here we attempt to address this discrepancy by dividing existing methods according to whether they have a \"descriptive\" or an \"inferential\" goal. While descriptive methods find patterns in networks based on context-dependent notions of community structure, inferential methods articulate generative models, and attempt to fit them to data. In this way, they are able to provide insights into the mechanisms of network formation, and separate structure from randomness in a manner supported by statistical evidence. We review how employing descriptive methods with inferential aims is riddled with pitfalls and misleading answers, and thus should be in general avoided. We argue that inferential methods are more typically aligned with clearer scientific questions, yield more robust results, and should be in many cases preferred. We attempt to dispel some myths and half-truths often believed when community detection is employed in practice, in an effort to improve both the use of such methods as well as the interpretation of their results.",
    "note": "arXiv:2112.00183 [physics, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2112.00183",
    "source": "arXiv.org",
    "title": "Descriptive vs. inferential community detection in networks: pitfalls, myths, and half-truths",
    "title-short": "Descriptive vs. inferential community detection in networks",
    "URL": "http://arxiv.org/abs/2112.00183",
    "author": [
      {
        "family": "Peixoto",
        "given": "Tiago P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          8,
          31
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          7,
          31
        ]
      ]
    }
  },
  {
    "id": "74v1AdGk",
    "type": "paper-conference",
    "container-title": "Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining",
    "DOI": "10.1145/3539597.3570410",
    "event-place": "Singapore Singapore",
    "event-title": "WSDM '23: The Sixteenth ACM International Conference on Web Search and Data Mining",
    "ISBN": "9781450394079",
    "language": "en",
    "page": "375-383",
    "publisher": "ACM",
    "publisher-place": "Singapore Singapore",
    "source": "DOI.org (Crossref)",
    "title": "S2TUL: A Semi-Supervised Framework for Trajectory-User Linking",
    "title-short": "S2TUL",
    "URL": "https://dl.acm.org/doi/10.1145/3539597.3570410",
    "author": [
      {
        "family": "Deng",
        "given": "Liwei"
      },
      {
        "family": "Sun",
        "given": "Hao"
      },
      {
        "family": "Zhao",
        "given": "Yan"
      },
      {
        "family": "Liu",
        "given": "Shuncheng"
      },
      {
        "family": "Zheng",
        "given": "Kai"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          9,
          12
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          27
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1145/3539597.3570410"
  },
  {
    "id": "1HHILcPS3",
    "type": "article-journal",
    "abstract": "A core step of mining human mobility data is to learn accurate representations for user-generated check-in sequences. The learned representations should be able to fully describe the spatial-temporal mobility patterns of users and the high-level semantics of traveling. However, existing check-in sequence representation learning is usually implicitly achieved by end-to-end models designed for specific downstream tasks, resulting in unsatisfactory generalizable abilities and poor performance. Besides, although the sequence representation learning models that follow the contrastive learning pre-training paradigm have achieved breakthroughs in many fields like NLP, they fail to simultaneously consider the unique spatial-temporal characteristics of check-in sequences and need manual adjustments on the data augmentation strategies. So, directly applying them to check-in sequences cannot yield a meaningful pretext task. To this end, in this paper we propose a contrastive pre-training model with adversarial perturbations for check-in sequence representation learning (CACSR). Firstly, we design a novel spatial-temporal augmentation block for disturbing the spatial-temporal features of check-in sequences in the latent space to relieve the stress of designing manual data augmentation strategies. Secondly, to construct an effective contrastive pretext task, we generate “hard” positive and negative pairs for the check-in sequence by adversarial training. These two designs encourage the model to capture the high-level spatial-temporal patterns and semantics of check-in sequences while ignoring the noisy and unimportant details. We demonstrate the effectiveness and versatility of CACSR on two kinds of downstream tasks using three real-world datasets. The results show that our model outperforms both the state-of-the-art pre-training methods and the end-to-end models.",
    "container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
    "DOI": "10.1609/aaai.v37i4.25546",
    "ISSN": "2374-3468",
    "issue": "4",
    "language": "en",
    "page": "4276-4283",
    "source": "ojs.aaai.org",
    "title": "Contrastive Pre-training with Adversarial Perturbations for Check-In Sequence Representation Learning",
    "URL": "https://ojs.aaai.org/index.php/AAAI/article/view/25546",
    "volume": "37",
    "author": [
      {
        "family": "Gong",
        "given": "Letian"
      },
      {
        "family": "Lin",
        "given": "Youfang"
      },
      {
        "family": "Guo",
        "given": "Shengnan"
      },
      {
        "family": "Lin",
        "given": "Yan"
      },
      {
        "family": "Wang",
        "given": "Tianyi"
      },
      {
        "family": "Zheng",
        "given": "Erwen"
      },
      {
        "family": "Zhou",
        "given": "Zeyu"
      },
      {
        "family": "Wan",
        "given": "Huaiyu"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          9,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          26
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1609/aaai.v37i4.25546"
  },
  {
    "id": "1CM9SFHvL",
    "type": "article",
    "abstract": "Trajectory-User Linking (TUL) is crucial for human mobility modeling by linking different trajectories to users with the exploration of complex mobility patterns. Existing works mainly rely on the recurrent neural framework to encode the temporal dependencies in trajectories, have fall short in capturing spatial-temporal global context for TUL prediction. To fill this gap, this work presents a new hierarchical spatio-temporal attention neural network, called AttnTUL, to jointly encode the local trajectory transitional patterns and global spatial dependencies for TUL. Specifically, our first model component is built over the graph neural architecture to preserve the local and global context and enhance the representation paradigm of geographical regions and user trajectories. Additionally, a hierarchically structured attention network is designed to simultaneously encode the intra-trajectory and inter-trajectory dependencies, with the integration of the temporal attention mechanism and global elastic attentional encoder. Extensive experiments demonstrate the superiority of our AttnTUL method as compared to state-of-the-art baselines on various trajectory datasets. The source code of our model is available at \\url{https://anonymous.4open.science/r/Attn_TUL}.",
    "DOI": "10.48550/arXiv.2302.10903",
    "note": "arXiv:2302.10903 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2302.10903",
    "number": "arXiv:2302.10903",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Trajectory-User Linking via Hierarchical Spatio-Temporal Attention Networks",
    "URL": "http://arxiv.org/abs/2302.10903",
    "author": [
      {
        "family": "Chen",
        "given": "Wei"
      },
      {
        "family": "Huang",
        "given": "Chao"
      },
      {
        "family": "Yu",
        "given": "Yanwei"
      },
      {
        "family": "Jiang",
        "given": "Yongguo"
      },
      {
        "family": "Dong",
        "given": "Junyu"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          9,
          20
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          11
        ]
      ]
    }
  },
  {
    "id": "PGy3jyD8",
    "type": "paper-conference",
    "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
    "container-title": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining",
    "DOI": "10.1145/2623330.2623732",
    "note": "arXiv:1403.6652 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1403.6652",
    "page": "701-710",
    "source": "arXiv.org",
    "title": "DeepWalk: Online Learning of Social Representations",
    "title-short": "DeepWalk",
    "URL": "http://arxiv.org/abs/1403.6652",
    "author": [
      {
        "family": "Perozzi",
        "given": "Bryan"
      },
      {
        "family": "Al-Rfou",
        "given": "Rami"
      },
      {
        "family": "Skiena",
        "given": "Steven"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          4
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2014",
          8,
          24
        ]
      ]
    }
  },
  {
    "id": "ALLNXu0E",
    "type": "webpage",
    "abstract": "A local graph partitioning algorithm finds a cut near a specified starting vertex, with a running time that depends largely on the size of the small side of the cut, rather than the size of the input graph. In this paper, we present a local partitioning algorithm using a variation of PageRank with a specified starting distribution. We derive a mixing result for PageRank vectors similar to that for random walks, and show that the ordering of the vertices produced by a PageRank vector reveals a cut with small conductance. In particular, we show that for any set C with conductance Phi and volume k, a PageRank vector with a certain starting distribution can be used to produce a set with conductance (O(radic(Phi log k)). We present an improved algorithm for computing approximate PageRank vectors, which allows us to find such a set in time proportional to its size. In particular, we can find a cut with conductance at most oslash, whose small side has volume at least 2\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">b</sup>\n in time O(2 log m/(2\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">b</sup>\n log\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>\n m/oslash\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>\n) where m is the number of edges in the graph. By combining small sets found by this local partitioning algorithm, we obtain a cut with conductance oslash and approximately optimal balance in time O(m log\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">4</sup>\n m/oslash)",
    "language": "en-US",
    "title": "Local Graph Partitioning using PageRank Vectors",
    "URL": "https://ieeexplore.ieee.org/document/4031383/",
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1109/FOCS.2006.44",
    "author": [
      {
        "family": "Reid",
        "given": "Andersen"
      },
      {
        "family": "Fan",
        "given": "Chung"
      },
      {
        "family": "Kevin",
        "given": "Lang"
      }
    ]
  },
  {
    "id": "BPyBK0dp",
    "type": "article",
    "abstract": "Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.",
    "DOI": "10.48550/arXiv.1607.00653",
    "note": "arXiv:1607.00653 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1607.00653",
    "number": "arXiv:1607.00653",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "node2vec: Scalable Feature Learning for Networks",
    "title-short": "node2vec",
    "URL": "http://arxiv.org/abs/1607.00653",
    "author": [
      {
        "family": "Grover",
        "given": "Aditya"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          4
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2016",
          7,
          3
        ]
      ]
    }
  },
  {
    "id": "14N53kyrQ",
    "type": "article",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "DOI": "10.48550/arXiv.1609.02907",
    "note": "arXiv:1609.02907 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1609.02907",
    "number": "arXiv:1609.02907",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Semi-Supervised Classification with Graph Convolutional Networks",
    "URL": "http://arxiv.org/abs/1609.02907",
    "author": [
      {
        "family": "Kipf",
        "given": "Thomas N."
      },
      {
        "family": "Welling",
        "given": "Max"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          4
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2017",
          2,
          22
        ]
      ]
    }
  },
  {
    "id": "5aIS0RLz",
    "type": "paper-conference",
    "abstract": "With the upsurge of online banking, mobile payment, and virtual currency, new money-laundering crimes easily conceal in the enormous transaction volume. The traditional rule-based methods with large amounts of alerting thresholds are already incapable of handling the fast-changing transaction networks. Recently, the DL models represented by the graph neural networks (GNNs) show the potential to capture money-laundering modes with high accuracy. However, most related works are still far from practical deployment in the industry. Based on our practice at WeBank, there are three major challenges: Firstly, supervised learning is infeasible facing the extraordinarily large-scale but imbalanced data, with hundreds of millions of active accounts but only thousands of anomalies. Secondly, the real-world transactions form a sparse network with millions of isolated user groups, which overflows the expressive ability of current node-level GNNs. Thirdly, the explanation for each suspicious account is mandatory by the government for double check, which conflicts with the black-box nature of most DL models. Therefore, we proposed Diga, the first work to apply the diffusion probabilistic model to a graph anomaly detection problem with three novel techniques: the biased K-hop PageRank, the semi-supervised guided diffusion and the novel weight-sharing GNN layer. The effectiveness and efficiency of Diga are verified via intensive experiments on both industrial and public datasets.",
    "collection-title": "KDD '23",
    "container-title": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
    "DOI": "10.1145/3580305.3599806",
    "event-place": "New York, NY, USA",
    "ISBN": "9798400701030",
    "page": "4404–4413",
    "publisher": "Association for Computing Machinery",
    "publisher-place": "New York, NY, USA",
    "source": "ACM Digital Library",
    "title": "Diga: Guided Diffusion Model for Graph Recovery in Anti-Money Laundering",
    "title-short": "Diga",
    "URL": "https://dl.acm.org/doi/10.1145/3580305.3599806",
    "author": [
      {
        "family": "Li",
        "given": "Xujia"
      },
      {
        "family": "Li",
        "given": "Yuan"
      },
      {
        "family": "Mo",
        "given": "Xueying"
      },
      {
        "family": "Xiao",
        "given": "Hebing"
      },
      {
        "family": "Shen",
        "given": "Yanyan"
      },
      {
        "family": "Chen",
        "given": "Lei"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          10
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          8,
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1145/3580305.3599806"
  },
  {
    "id": "1NwIeJDz",
    "type": "article-journal",
    "container-title": "IEEE Trans. Knowl. Data Eng.",
    "DOI": "10.1109/TKDE.2019.2912606",
    "issue": "10",
    "page": "1897-1908",
    "title": "BATON: Batch One-Hop Personalized PageRanks with Efficiency and Accuracy.",
    "title-short": "BATON",
    "volume": "32",
    "author": [
      {
        "family": "Luo",
        "given": "Siqiang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "URL": "https://dblp.org/rec/journals/tkde/LuoXLK20",
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://dblp.org/rec/journals/tkde/LuoXLK20"
  },
  {
    "id": "NfltEOJJ",
    "type": "article",
    "abstract": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
    "DOI": "10.48550/arXiv.2310.04560",
    "note": "arXiv:2310.04560 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2310.04560",
    "number": "arXiv:2310.04560",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
    "title-short": "Talk like a Graph",
    "URL": "http://arxiv.org/abs/2310.04560",
    "author": [
      {
        "family": "Fatemi",
        "given": "Bahare"
      },
      {
        "family": "Halcrow",
        "given": "Jonathan"
      },
      {
        "family": "Perozzi",
        "given": "Bryan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          10,
          6
        ]
      ]
    }
  },
  {
    "id": "MHZjeftY",
    "type": "article",
    "abstract": "In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with a cost less than 1 dollar.",
    "DOI": "10.48550/arXiv.2310.04668",
    "note": "arXiv:2310.04668 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2310.04668",
    "number": "arXiv:2310.04668",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Label-free Node Classification on Graphs with Large Language Models (LLMS)",
    "URL": "http://arxiv.org/abs/2310.04668",
    "author": [
      {
        "family": "Chen",
        "given": "Zhikai"
      },
      {
        "family": "Mao",
        "given": "Haitao"
      },
      {
        "family": "Wen",
        "given": "Hongzhi"
      },
      {
        "family": "Han",
        "given": "Haoyu"
      },
      {
        "family": "Jin",
        "given": "Wei"
      },
      {
        "family": "Zhang",
        "given": "Haiyang"
      },
      {
        "family": "Liu",
        "given": "Hui"
      },
      {
        "family": "Tang",
        "given": "Jiliang"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          10,
          12
        ]
      ]
    }
  },
  {
    "id": "10BVsN5CV",
    "type": "article",
    "abstract": "We study the problem of semi-supervised learning with Graph Neural Networks (GNNs) in an active learning setup. We propose GraphPart, a novel partition-based active learning approach for GNNs. GraphPart first splits the graph into disjoint partitions and then selects representative nodes within each partition to query. The proposed method is motivated by a novel analysis of the classification error under realistic smoothness assumptions over the graph and the node features. Extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing active learning methods for GNNs under a wide range of annotation budget constraints. In addition, the proposed method does not introduce additional hyperparameters, which is crucial for model training, especially in the active learning setting where a labeled validation set may not be available.",
    "DOI": "10.48550/arXiv.2201.09391",
    "note": "arXiv:2201.09391 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2201.09391",
    "number": "arXiv:2201.09391",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Partition-Based Active Learning for Graph Neural Networks",
    "URL": "http://arxiv.org/abs/2201.09391",
    "author": [
      {
        "family": "Ma",
        "given": "Jiaqi"
      },
      {
        "family": "Ma",
        "given": "Ziqiao"
      },
      {
        "family": "Chai",
        "given": "Joyce"
      },
      {
        "family": "Mei",
        "given": "Qiaozhu"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          3,
          17
        ]
      ]
    }
  },
  {
    "id": "1GjbjpB2d",
    "type": "article-journal",
    "abstract": "Data selection methods, such as active learning and core-set selection, are useful tools for improving the data efficiency of deep learning models on large-scale datasets. However, recent deep learning models have moved forward from independent and identically distributed data to graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to the emergence of Graph Neural Networks (GNNs) that go beyond the models existing data selection methods are designed for. Therefore, we present GRAIN, an efficient framework that opens up a new perspective through connecting data selection in GNNs with social influence maximization. By exploiting the common patterns of GNNs, GRAIN introduces a novel feature propagation concept, a diversified influence maximization objective with novel influence and diversity functions, and a greedy algorithm with an approximation guarantee into a unified framework. Empirical studies on public datasets demonstrate that GRAIN significantly improves both the performance and efficiency of data selection (including active learning and core-set selection) for GNNs. To the best of our knowledge, this is the first attempt to bridge two largely parallel threads of research, data selection, and social influence maximization, in the setting of GNNs, paving new ways for improving data efficiency.",
    "container-title": "Proceedings of the VLDB Endowment",
    "DOI": "10.14778/3476249.3476295",
    "ISSN": "2150-8097",
    "issue": "11",
    "journalAbbreviation": "Proc. VLDB Endow.",
    "page": "2473–2482",
    "source": "ACM Digital Library",
    "title": "GRAIN: improving data efficiency of graph neural networks via diversified influence maximization",
    "title-short": "GRAIN",
    "URL": "https://doi.org/10.14778/3476249.3476295",
    "volume": "14",
    "author": [
      {
        "family": "Zhang",
        "given": "Wentao"
      },
      {
        "family": "Yang",
        "given": "Zhi"
      },
      {
        "family": "Wang",
        "given": "Yexin"
      },
      {
        "family": "Shen",
        "given": "Yu"
      },
      {
        "family": "Li",
        "given": "Yang"
      },
      {
        "family": "Wang",
        "given": "Liang"
      },
      {
        "family": "Cui",
        "given": "Bin"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          17
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          7,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.14778/3476249.3476295"
  },
  {
    "id": "1BNJOO4QD",
    "type": "article",
    "abstract": "Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM.",
    "DOI": "10.48550/arXiv.2307.03393",
    "note": "arXiv:2307.03393 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2307.03393",
    "number": "arXiv:2307.03393",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs",
    "URL": "http://arxiv.org/abs/2307.03393",
    "author": [
      {
        "family": "Chen",
        "given": "Zhikai"
      },
      {
        "family": "Mao",
        "given": "Haitao"
      },
      {
        "family": "Li",
        "given": "Hang"
      },
      {
        "family": "Jin",
        "given": "Wei"
      },
      {
        "family": "Wen",
        "given": "Hongzhi"
      },
      {
        "family": "Wei",
        "given": "Xiaochi"
      },
      {
        "family": "Wang",
        "given": "Shuaiqiang"
      },
      {
        "family": "Yin",
        "given": "Dawei"
      },
      {
        "family": "Fan",
        "given": "Wenqi"
      },
      {
        "family": "Liu",
        "given": "Hui"
      },
      {
        "family": "Tang",
        "given": "Jiliang"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          18
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          8,
          2
        ]
      ]
    }
  },
  {
    "id": "1pKPJvuH",
    "type": "article",
    "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
    "DOI": "10.48550/arXiv.2306.08302",
    "note": "arXiv:2306.08302 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2306.08302",
    "number": "arXiv:2306.08302",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "title-short": "Unifying Large Language Models and Knowledge Graphs",
    "URL": "http://arxiv.org/abs/2306.08302",
    "author": [
      {
        "family": "Pan",
        "given": "Shirui"
      },
      {
        "family": "Luo",
        "given": "Linhao"
      },
      {
        "family": "Wang",
        "given": "Yufei"
      },
      {
        "family": "Chen",
        "given": "Chen"
      },
      {
        "family": "Wang",
        "given": "Jiapu"
      },
      {
        "family": "Wu",
        "given": "Xindong"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          18
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          20
        ]
      ]
    }
  },
  {
    "id": "JR6nEkYx",
    "type": "article",
    "abstract": "Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space. Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation. For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs well across different tasks, making it the first general-purpose graph classification model across domains.",
    "DOI": "10.48550/arXiv.2310.00149",
    "note": "arXiv:2310.00149 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2310.00149",
    "number": "arXiv:2310.00149",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "One for All: Towards Training One Graph Model for All Classification Tasks",
    "title-short": "One for All",
    "URL": "http://arxiv.org/abs/2310.00149",
    "author": [
      {
        "family": "Liu",
        "given": "Hao"
      },
      {
        "family": "Feng",
        "given": "Jiarui"
      },
      {
        "family": "Kong",
        "given": "Lecheng"
      },
      {
        "family": "Liang",
        "given": "Ningyue"
      },
      {
        "family": "Tao",
        "given": "Dacheng"
      },
      {
        "family": "Chen",
        "given": "Yixin"
      },
      {
        "family": "Zhang",
        "given": "Muhan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          18
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          9,
          29
        ]
      ]
    }
  },
  {
    "id": "S9FrUa25",
    "type": "article-journal",
    "abstract": "Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction tuning paradigm. Our framework incorporates a text-graph grounding component to establish a connection between textual information and graph structures. Additionally, we propose a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector. This paradigm explores self-supervised graph structural signals and task-specific graph instructions, to guide LLMs in understanding complex graph structures and improving their adaptability across different downstream tasks. Our framework is evaluated on supervised and zero-shot graph learning tasks, demonstrating superior generalization and outperforming state-of-the-art baselines.",
    "DOI": "10.48550/ARXIV.2310.13023",
    "source": "DOI.org (Datacite)",
    "title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
    "title-short": "GraphGPT",
    "URL": "https://arxiv.org/abs/2310.13023",
    "author": [
      {
        "family": "Tang",
        "given": "Jiabin"
      },
      {
        "family": "Yang",
        "given": "Yuhao"
      },
      {
        "family": "Wei",
        "given": "Wei"
      },
      {
        "family": "Shi",
        "given": "Lei"
      },
      {
        "family": "Su",
        "given": "Lixin"
      },
      {
        "family": "Cheng",
        "given": "Suqi"
      },
      {
        "family": "Yin",
        "given": "Dawei"
      },
      {
        "family": "Huang",
        "given": "Chao"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          30
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2310.13023"
  },
  {
    "id": "Gy4Veci2",
    "type": "article",
    "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
    "DOI": "10.48550/arXiv.2212.01588",
    "note": "arXiv:2212.01588 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2212.01588",
    "number": "arXiv:2212.01588",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "RHO ($\\rho$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
    "title-short": "RHO ($\\rho$)",
    "URL": "http://arxiv.org/abs/2212.01588",
    "author": [
      {
        "family": "Ji",
        "given": "Ziwei"
      },
      {
        "family": "Liu",
        "given": "Zihan"
      },
      {
        "family": "Lee",
        "given": "Nayeon"
      },
      {
        "family": "Yu",
        "given": "Tiezheng"
      },
      {
        "family": "Wilie",
        "given": "Bryan"
      },
      {
        "family": "Zeng",
        "given": "Min"
      },
      {
        "family": "Fung",
        "given": "Pascale"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          10,
          30
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          5,
          12
        ]
      ]
    }
  },
  {
    "id": "tTOo1TeT",
    "type": "article",
    "abstract": "Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms previous adapter-based methods. The code will be released at https://github.com/lixinustc/GraphAdapter",
    "DOI": "10.48550/arXiv.2309.13625",
    "note": "arXiv:2309.13625 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2309.13625",
    "number": "arXiv:2309.13625",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph",
    "title-short": "GraphAdapter",
    "URL": "http://arxiv.org/abs/2309.13625",
    "author": [
      {
        "family": "Li",
        "given": "Xin"
      },
      {
        "family": "Lian",
        "given": "Dongze"
      },
      {
        "family": "Lu",
        "given": "Zhihe"
      },
      {
        "family": "Bai",
        "given": "Jiawang"
      },
      {
        "family": "Chen",
        "given": "Zhibo"
      },
      {
        "family": "Wang",
        "given": "Xinchao"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          11,
          8
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          9,
          24
        ]
      ]
    }
  },
  {
    "id": "3A8id4H0",
    "type": "article-journal",
    "abstract": "Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming—one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.",
    "container-title": "International Journal of Computer Vision",
    "DOI": "10.1007/s11263-022-01653-1",
    "ISSN": "1573-1405",
    "issue": "9",
    "journalAbbreviation": "Int J Comput Vis",
    "language": "en",
    "page": "2337-2348",
    "source": "Springer Link",
    "title": "Learning to Prompt for Vision-Language Models",
    "URL": "https://doi.org/10.1007/s11263-022-01653-1",
    "volume": "130",
    "author": [
      {
        "family": "Zhou",
        "given": "Kaiyang"
      },
      {
        "family": "Yang",
        "given": "Jingkang"
      },
      {
        "family": "Loy",
        "given": "Chen Change"
      },
      {
        "family": "Liu",
        "given": "Ziwei"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          11,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          9,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1007/s11263-022-01653-1"
  },
  {
    "id": "1BEL7Xyvz",
    "type": "article",
    "abstract": "Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring these semantics using relation-aware ensemble leads to a much larger search space than general ensemble methods. To address this issue, we propose a divide-search-combine algorithm RelEns-DSC that searches the relation-wise ensemble weights independently. This algorithm has the same computation cost as general ensemble methods but with much better performance. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method in efficiently searching relation-aware ensemble weights and achieving state-of-the-art embedding performance. The code is public at https://github.com/LARS-research/RelEns.",
    "DOI": "10.48550/arXiv.2310.08917",
    "note": "arXiv:2310.08917 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2310.08917",
    "number": "arXiv:2310.08917",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Relation-aware Ensemble Learning for Knowledge Graph Embedding",
    "URL": "http://arxiv.org/abs/2310.08917",
    "author": [
      {
        "family": "Yue",
        "given": "Ling"
      },
      {
        "family": "Zhang",
        "given": "Yongqi"
      },
      {
        "family": "Yao",
        "given": "Quanming"
      },
      {
        "family": "Li",
        "given": "Yong"
      },
      {
        "family": "Wu",
        "given": "Xian"
      },
      {
        "family": "Zhang",
        "given": "Ziheng"
      },
      {
        "family": "Lin",
        "given": "Zhenxi"
      },
      {
        "family": "Zheng",
        "given": "Yefeng"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          11,
          13
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          10,
          13
        ]
      ]
    }
  },
  {
    "id": "DILKu5TG",
    "type": "article-journal",
    "abstract": "Many complex networks exhibit a modular structure of densely connected groups of nodes. Usually, such a modular structure is uncovered by the optimization of some quality function. Although flawed, modularity remains one of the most popular quality functions. The Louvain algorithm was originally developed for optimizing modularity, but has been applied to a variety of methods. As such, speeding up the Louvain algorithm enables the analysis of larger graphs in a shorter time for various methods. We here suggest to consider moving nodes to a random neighbor community, instead of the best neighbor community. Although incredibly simple, it reduces the theoretical runtime complexity from O(m) to O(nlog〈k〉) in networks with a clear community structure. In benchmark networks, it speeds up the algorithm roughly 2–3 times, while in some real networks it even reaches 10 times faster runtimes. This improvement is due to two factors: (1) a random neighbor is likely to be in a “good” community and (2) random neighbors are likely to be hubs, helping the convergence. Finally, the performance gain only slightly diminishes the quality, especially for modularity, thus providing a good quality-performance ratio. However, these gains are less pronounced, or even disappear, for some other measures such as significance or surprise.",
    "container-title": "Physical Review E",
    "DOI": "10.1103/PhysRevE.92.032801",
    "issue": "3",
    "journalAbbreviation": "Phys. Rev. E",
    "page": "032801",
    "source": "APS",
    "title": "Faster unfolding of communities: Speeding up the Louvain algorithm",
    "title-short": "Faster unfolding of communities",
    "URL": "https://link.aps.org/doi/10.1103/PhysRevE.92.032801",
    "volume": "92",
    "author": [
      {
        "family": "Traag",
        "given": "V. A."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          1,
          2
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2015",
          9,
          3
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://link.aps.org/doi/10.1103/PhysRevE.92.032801"
  },
  {
    "id": "QICAbbUL",
    "type": "article",
    "abstract": "A sketch is one of the most intuitive and versatile tools humans use to convey their ideas visually. An animated sketch opens another dimension to the expression of ideas and is widely used by designers for a variety of purposes. Animating sketches is a laborious process, requiring extensive experience and professional design skills. In this work, we present a method that automatically adds motion to a single-subject sketch (hence, \"breathing life into it\"), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited. Our method does not require extensive training, but instead leverages the motion prior of a large pretrained text-to-video diffusion model using a score-distillation loss to guide the placement of strokes. To promote natural and smooth motion and to better preserve the sketch's appearance, we model the learned motion through two components. The first governs small local deformations and the second controls global affine transformations. Surprisingly, we find that even models that struggle to generate sketch videos on their own can still serve as a useful backbone for animating abstract representations.",
    "DOI": "10.48550/arXiv.2311.13608",
    "note": "arXiv:2311.13608 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2311.13608",
    "number": "arXiv:2311.13608",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Breathing Life Into Sketches Using Text-to-Video Priors",
    "URL": "http://arxiv.org/abs/2311.13608",
    "author": [
      {
        "family": "Gal",
        "given": "Rinon"
      },
      {
        "family": "Vinker",
        "given": "Yael"
      },
      {
        "family": "Alaluf",
        "given": "Yuval"
      },
      {
        "family": "Bermano",
        "given": "Amit H."
      },
      {
        "family": "Cohen-Or",
        "given": "Daniel"
      },
      {
        "family": "Shamir",
        "given": "Ariel"
      },
      {
        "family": "Chechik",
        "given": "Gal"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          3,
          12
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          11,
          21
        ]
      ]
    }
  },
  {
    "id": "J0Nrj2s8",
    "type": "article",
    "abstract": "In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and Edge2Image, but with a single-step inference. This work suggests that single-step diffusion models can serve as strong backbones for a range of GAN learning objectives. Our code and models are available at https://github.com/GaParmar/img2img-turbo.",
    "DOI": "10.48550/arXiv.2403.12036",
    "note": "arXiv:2403.12036 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2403.12036",
    "number": "arXiv:2403.12036",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "One-Step Image Translation with Text-to-Image Models",
    "URL": "http://arxiv.org/abs/2403.12036",
    "author": [
      {
        "family": "Parmar",
        "given": "Gaurav"
      },
      {
        "family": "Park",
        "given": "Taesung"
      },
      {
        "family": "Narasimhan",
        "given": "Srinivasa"
      },
      {
        "family": "Zhu",
        "given": "Jun-Yan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          4,
          2
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          3,
          18
        ]
      ]
    }
  },
  {
    "id": "EdWYslIq",
    "type": "paper-conference",
    "abstract": "Graph Anomaly Detection (GAD) has surfaced as a significant field of research, predominantly due to its substantial influence in production environments. Although existing approaches for node anomaly detection have shown effectiveness, they have yet to fully address two major challenges: operating in settings with limited supervision and managing class imbalance effectively. In response to these challenges, we propose a novel model, ConsisGAD, which is tailored for GAD in scenarios characterized by limited supervision and is anchored in the principles of consistency training. Under limited supervision, ConsisGAD effectively leverages the abundance of unlabeled data for consistency training by incorporating a novel learnable data augmentation mechanism, thereby introducing controlled noise into the dataset. Moreover, ConsisGAD takes advantage of the variance in homophily distribution between normal and anomalous nodes to craft a simplified GNN backbone, enhancing its capability to distinguish effectively between these two classes. Comprehensive experiments on several benchmark datasets validate the superior performance of ConsisGAD in comparison to state-of-the-art baselines. Our code is available at https://github.com/Xtra-Computing/ConsisGAD.",
    "event-title": "The Twelfth International Conference on Learning Representations",
    "language": "en",
    "source": "openreview.net",
    "title": "Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision",
    "URL": "https://openreview.net/forum?id=elMKXvhhQ9",
    "author": [
      {
        "family": "Chen",
        "given": "Nan"
      },
      {
        "family": "Liu",
        "given": "Zemin"
      },
      {
        "family": "Hooi",
        "given": "Bryan"
      },
      {
        "family": "He",
        "given": "Bingsheng"
      },
      {
        "family": "Fathony",
        "given": "Rizal"
      },
      {
        "family": "Hu",
        "given": "Jun"
      },
      {
        "family": "Chen",
        "given": "Jia"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          19
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          10,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://openreview.net/forum?id=elMKXvhhQ9"
  },
  {
    "id": "Yfy2yMFF",
    "type": "paper-conference",
    "abstract": "The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over \"serverful\" infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as optimization algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/performance tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with efficient (i.e., reduced) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS.",
    "collection-title": "SIGMOD '21",
    "container-title": "Proceedings of the 2021 International Conference on Management of Data",
    "DOI": "10.1145/3448016.3459240",
    "event-place": "New York, NY, USA",
    "ISBN": "9781450383431",
    "page": "857–871",
    "publisher": "Association for Computing Machinery",
    "publisher-place": "New York, NY, USA",
    "source": "ACM Digital Library",
    "title": "Towards Demystifying Serverless Machine Learning Training",
    "URL": "https://doi.org/10.1145/3448016.3459240",
    "author": [
      {
        "family": "Jiang",
        "given": "Jiawei"
      },
      {
        "family": "Gan",
        "given": "Shaoduo"
      },
      {
        "family": "Liu",
        "given": "Yue"
      },
      {
        "family": "Wang",
        "given": "Fanlin"
      },
      {
        "family": "Alonso",
        "given": "Gustavo"
      },
      {
        "family": "Klimovic",
        "given": "Ana"
      },
      {
        "family": "Singla",
        "given": "Ankit"
      },
      {
        "family": "Wu",
        "given": "Wentao"
      },
      {
        "family": "Zhang",
        "given": "Ce"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          12
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          6,
          18
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.1145/3448016.3459240"
  },
  {
    "id": "CVg73F4B",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.",
    "DOI": "10.48550/arXiv.2004.05718",
    "note": "arXiv:2004.05718 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2004.05718",
    "number": "arXiv:2004.05718",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Principal Neighbourhood Aggregation for Graph Nets",
    "URL": "http://arxiv.org/abs/2004.05718",
    "author": [
      {
        "family": "Corso",
        "given": "Gabriele"
      },
      {
        "family": "Cavalleri",
        "given": "Luca"
      },
      {
        "family": "Beaini",
        "given": "Dominique"
      },
      {
        "family": "Liò",
        "given": "Pietro"
      },
      {
        "family": "Veličković",
        "given": "Petar"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          13
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          12,
          31
        ]
      ]
    }
  },
  {
    "id": "vU3izXD5",
    "type": "article",
    "abstract": "Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between \"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.",
    "DOI": "10.48550/arXiv.1806.01261",
    "note": "arXiv:1806.01261 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1806.01261",
    "number": "arXiv:1806.01261",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Relational inductive biases, deep learning, and graph networks",
    "URL": "http://arxiv.org/abs/1806.01261",
    "author": [
      {
        "family": "Battaglia",
        "given": "Peter W."
      },
      {
        "family": "Hamrick",
        "given": "Jessica B."
      },
      {
        "family": "Bapst",
        "given": "Victor"
      },
      {
        "family": "Sanchez-Gonzalez",
        "given": "Alvaro"
      },
      {
        "family": "Zambaldi",
        "given": "Vinicius"
      },
      {
        "family": "Malinowski",
        "given": "Mateusz"
      },
      {
        "family": "Tacchetti",
        "given": "Andrea"
      },
      {
        "family": "Raposo",
        "given": "David"
      },
      {
        "family": "Santoro",
        "given": "Adam"
      },
      {
        "family": "Faulkner",
        "given": "Ryan"
      },
      {
        "family": "Gulcehre",
        "given": "Caglar"
      },
      {
        "family": "Song",
        "given": "Francis"
      },
      {
        "family": "Ballard",
        "given": "Andrew"
      },
      {
        "family": "Gilmer",
        "given": "Justin"
      },
      {
        "family": "Dahl",
        "given": "George"
      },
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Allen",
        "given": "Kelsey"
      },
      {
        "family": "Nash",
        "given": "Charles"
      },
      {
        "family": "Langston",
        "given": "Victoria"
      },
      {
        "family": "Dyer",
        "given": "Chris"
      },
      {
        "family": "Heess",
        "given": "Nicolas"
      },
      {
        "family": "Wierstra",
        "given": "Daan"
      },
      {
        "family": "Kohli",
        "given": "Pushmeet"
      },
      {
        "family": "Botvinick",
        "given": "Matt"
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Li",
        "given": "Yujia"
      },
      {
        "family": "Pascanu",
        "given": "Razvan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          13
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2018",
          10,
          17
        ]
      ]
    }
  },
  {
    "id": "3D4nhcoE",
    "type": "webpage",
    "title": "Graph Feature Preprocessor — Snap ML 1.15.6 documentation",
    "URL": "https://snapml.readthedocs.io/en/latest/graph_preprocessor.html",
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://snapml.readthedocs.io/en/latest/graph_preprocessor.html"
  },
  {
    "id": "XxQOzz4E",
    "type": "article",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
    "DOI": "10.48550/arXiv.1710.10903",
    "note": "arXiv:1710.10903 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1710.10903",
    "number": "arXiv:1710.10903",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph Attention Networks",
    "URL": "http://arxiv.org/abs/1710.10903",
    "author": [
      {
        "family": "Veličković",
        "given": "Petar"
      },
      {
        "family": "Cucurull",
        "given": "Guillem"
      },
      {
        "family": "Casanova",
        "given": "Arantxa"
      },
      {
        "family": "Romero",
        "given": "Adriana"
      },
      {
        "family": "Liò",
        "given": "Pietro"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          13
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2018",
          2,
          4
        ]
      ]
    }
  },
  {
    "id": "104xo4Pb2",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "DOI": "10.48550/arXiv.1810.00826",
    "note": "arXiv:1810.00826 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.1810.00826",
    "number": "arXiv:1810.00826",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "How Powerful are Graph Neural Networks?",
    "URL": "http://arxiv.org/abs/1810.00826",
    "author": [
      {
        "family": "Xu",
        "given": "Keyulu"
      },
      {
        "family": "Hu",
        "given": "Weihua"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Jegelka",
        "given": "Stefanie"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          13
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          2,
          22
        ]
      ]
    }
  },
  {
    "id": "1ETNH5u2u",
    "type": "paper-conference",
    "abstract": "Graph neural networks are designed to learn functions on graphs. Typically, the relevant target functions are invariant with respect to actions by permutations. Therefore the design of some graph neural network architectures has been inspired by graph-isomorphism algorithms. The classical Weisfeiler-Lehman algorithm (WL) -- a graph-isomorphism test based on color refinement -- became relevant to the study of graph neural networks. The WL test can be generalized to a hierarchy of higher-order tests, known as $k$-WL. This hierarchy has been used to characterize the expressive power of graph neural networks, and to inspire the design of graph neural network architectures. A few variants of the WL hierarchy appear in the literature. The goal of this short note is pedagogical and practical: We explain the differences between the WL and folklore-WL formulations, with pointers to existing discussions in the literature. We illuminate the differences between the formulations by visualizing an example.",
    "container-title": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "DOI": "10.1109/ICASSP39728.2021.9413523",
    "note": "arXiv:2201.07083 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2201.07083",
    "page": "8533-8537",
    "source": "arXiv.org",
    "title": "A Short Tutorial on The Weisfeiler-Lehman Test And Its Variants",
    "URL": "http://arxiv.org/abs/2201.07083",
    "author": [
      {
        "family": "Huang",
        "given": "Ningyuan"
      },
      {
        "family": "Villar",
        "given": "Soledad"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          13
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          6,
          6
        ]
      ]
    }
  },
  {
    "id": "SfavsEd8",
    "type": "article-journal",
    "abstract": "This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.",
    "container-title": "Neural Networks",
    "DOI": "10.1016/0893-6080(89)90020-8",
    "ISSN": "0893-6080",
    "issue": "5",
    "journalAbbreviation": "Neural Networks",
    "page": "359-366",
    "source": "ScienceDirect",
    "title": "Multilayer feedforward networks are universal approximators",
    "URL": "https://www.sciencedirect.com/science/article/pii/0893608089900208",
    "volume": "2",
    "author": [
      {
        "family": "Hornik",
        "given": "Kurt"
      },
      {
        "family": "Stinchcombe",
        "given": "Maxwell"
      },
      {
        "family": "White",
        "given": "Halbert"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          13
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1989",
          1,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://www.sciencedirect.com/science/article/abs/pii/0893608089900208"
  },
  {
    "id": "IgrRizxk",
    "type": "paper-conference",
    "abstract": "We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.",
    "container-title": "Advances in Neural Information Processing Systems",
    "publisher": "Curran Associates, Inc.",
    "source": "Neural Information Processing Systems",
    "title": "Translating Embeddings for Modeling Multi-relational Data",
    "URL": "https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html",
    "volume": "26",
    "author": [
      {
        "family": "Bordes",
        "given": "Antoine"
      },
      {
        "family": "Usunier",
        "given": "Nicolas"
      },
      {
        "family": "Garcia-Duran",
        "given": "Alberto"
      },
      {
        "family": "Weston",
        "given": "Jason"
      },
      {
        "family": "Yakhnenko",
        "given": "Oksana"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          8,
          14
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2013"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html"
  }
]