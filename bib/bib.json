[
  {
    "id": "dEMqP61a",
    "type": "article",
    "abstract": "Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.",
    "DOI": "10.48550/arXiv.2110.08727",
    "note": "arXiv:2110.08727 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:doi.org/10.48550/arXiv.2110.08727",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation",
    "title-short": "Graph-less Neural Networks",
    "URL": "http://arxiv.org/abs/2110.08727",
    "author": [
      {
        "family": "Zhang",
        "given": "Shichang"
      },
      {
        "family": "Liu",
        "given": "Yozen"
      },
      {
        "family": "Sun",
        "given": "Yizhou"
      },
      {
        "family": "Shah",
        "given": "Neil"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          8
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          23
        ]
      ]
    }
  },
  {
    "id": "49r47O4e",
    "type": "article",
    "abstract": "While Graph Neural Networks (GNNs) have demonstrated their efficacy in dealing with non-Euclidean structural data, they are difficult to be deployed in real applications due to the scalability constraint imposed by multi-hop data dependency. Existing methods attempt to address this scalability issue by training multi-layer perceptrons (MLPs) exclusively on node content features using labels derived from trained GNNs. Even though the performance of MLPs can be significantly improved, two issues prevent MLPs from outperforming GNNs and being used in practice: the ignorance of graph structural information and the sensitivity to node feature noises. In this paper, we propose to learn NOise-robust Structure-aware MLPs On Graphs (NOSMOG) to overcome the challenges. Specifically, we first complement node content with position features to help MLPs capture graph structural information. We then design a novel representational similarity distillation strategy to inject structural node similarities into MLPs. Finally, we introduce the adversarial feature augmentation to ensure stable learning against feature noises and further improve performance. Extensive experiments demonstrate that NOSMOG outperforms GNNs and the state-of-the-art method in both transductive and inductive settings across seven datasets, while maintaining a competitive inference efficiency. Codes are available at https://github.com/meettyj/NOSMOG.",
    "DOI": "10.48550/arXiv.2208.10010",
    "note": "arXiv:2208.10010 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2208.10010",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs",
    "title-short": "NOSMOG",
    "URL": "http://arxiv.org/abs/2208.10010",
    "author": [
      {
        "family": "Tian",
        "given": "Yijun"
      },
      {
        "family": "Zhang",
        "given": "Chuxu"
      },
      {
        "family": "Guo",
        "given": "Zhichun"
      },
      {
        "family": "Zhang",
        "given": "Xiangliang"
      },
      {
        "family": "Chawla",
        "given": "Nitesh V."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          24
        ]
      ]
    }
  },
  {
    "id": "ByADi6ga",
    "type": "article",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems. As an initial step to analyze the inherent generalizability of GNNs, we show the essential difference between MLP and PMLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, by examining their extrapolation behavior, we find that though many GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extremely out-of-distribution samples, they have greater potential to generalize to testing samples near the training data range as natural advantages of GNN architectures.",
    "DOI": "10.48550/arXiv.2212.09034",
    "note": "arXiv:2212.09034 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2212.09034",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs",
    "title-short": "Graph Neural Networks are Inherently Good Generalizers",
    "URL": "http://arxiv.org/abs/2212.09034",
    "author": [
      {
        "family": "Yang",
        "given": "Chenxiao"
      },
      {
        "family": "Wu",
        "given": "Qitian"
      },
      {
        "family": "Wang",
        "given": "Jiahua"
      },
      {
        "family": "Yan",
        "given": "Junchi"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          9
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          5
        ]
      ]
    }
  },
  {
    "id": "6iMLCeK9",
    "type": "article",
    "abstract": "Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs.",
    "DOI": "10.48550/arXiv.2205.10803",
    "note": "arXiv:2205.10803 [cs]\nThis CSL Item was generated by Manubot v0.5.6 from its persistent identifier (standard_id).\nstandard_id: url:https://doi.org/10.48550/arXiv.2205.10803",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
    "title-short": "GraphMAE",
    "URL": "http://arxiv.org/abs/2205.10803",
    "author": [
      {
        "family": "Hou",
        "given": "Zhenyu"
      },
      {
        "family": "Liu",
        "given": "Xiao"
      },
      {
        "family": "Cen",
        "given": "Yukuo"
      },
      {
        "family": "Dong",
        "given": "Yuxiao"
      },
      {
        "family": "Yang",
        "given": "Hongxia"
      },
      {
        "family": "Wang",
        "given": "Chunjie"
      },
      {
        "family": "Tang",
        "given": "Jie"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2023",
          6,
          12
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          7,
          13
        ]
      ]
    }
  }
]