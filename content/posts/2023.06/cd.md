---
title: "Structural Community Detection"
date: 2023-06-19T10:55:48+08:00
draft: false
mathjax: true
bibFile: bib/bib.json
---

<!-- 本文专注于解释社区发现的两个经典算法：Louvain方法和Infomap方法。 -->
本文专注于整理社区发现的方法。

## 问题定义

给定一个图$G=(V,E)$，找到一个映射$g:V\to \{1,2,\cdots,K\}$，$g$将图中的顶点映射到社区标签。

## Louvain方法

Louvain方法 {{< cite "2hXkUYg3" >}} 是一种贪心算法，其优化目标是模块度，如下式所示：

$$Q=\frac{1}{2m}\sum_{i,j}\left[A_{ij}-\frac{k_ik_j}{2m}\right]\delta(c_i,c_j)$$

其中$A_{ij}$表示顶点$i$和顶点$j$之间连边的权重；$k_i=\sum_jA_{ij}$表示与顶点$i$相连的边的权重之和；$c_i$表示顶点$i$的社区标签；$\delta(u,v)$在$u=v$时等于1，否则等于0；$m=\frac{1}{2}\sum_{ij}A_{ij}$。

Louvain算法分为两阶段。
```
初始时设定每个顶点独立属于一个社区
# 第一阶段
生成一个随机的顶点序列Queue
For each node i in Queue:
    For each neighbor j of i:
        尝试将i的社区标签c_i修改为j的社区标签c_j
        计算模块度的增长DQ
    If max(DQ)>0:
        修改i的社区标签
    Else:
        保持i的社区标签不变
# 第二阶段
For each community c_i in G:
    将社区标签为c_i的所有顶点聚合为一个新的顶点
    原c_i内的连边转化为新顶点的自环，边权为原边权之和
    原c_i内顶点与另一社区c_j内顶点的所有连边聚合为一条连边，边权为原边权之和
# 重新执行第一阶段，直到模块度Q不再增加
```

第一阶段中顶点序列的顺序会影响算法的输出。作者发现不同的顶点处理顺序会影响算法的时间效率，但不会对最终的模块度造成过大（significant）的影响。

## Infomap方法

Infomap方法 {{< cite "umB3JCfk" >}} 与Louvain方法相似，但使用了不同的优化目标。作者发现了社区发现与编码的联系，即最优的社区发现结果能够使得描述图上随机游走路径的编码长度最短。如下图所示。当未做社区划分时，对于图(a)中的随机游走，需要在图(b)中使用所有路径上顶点的编码进行描述，一共$314$bits；当把图划分成四个社区后，在图(c)中，为每个社区设定起始编码和中止编码，路径节点的编码长度可以缩小，这样整条游走路径的编码长度缩短为$243$bits；在图(d)中，若忽略社区内部的游走路径，则可以产生更短的的粗糙编码。

<img src="https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/images/iShot_2023-06-20_10.36.17.png" />

对于一个含有$n$个节点的图，可以将其划分为$1,2,\cdots,n$个社区，即一共有$n$种社区个数的选择（下图中为$1,2,\cdots,25$共25种选择）。在比较不同社区划分对编码长度的影响时，作者发现，描述社区的平均编码长度随着社区个数的增加而单调递增，描述节点的平均编码长度随着社区个数的增加而单调递减。将二者相加即为描述游走路径的平均编码长度。当产生最优的社区个数($4$)时，平均编码长度最短($3.09$bits)

<img src="https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/images/iShot_2023-06-20_10.36.29.png" />

具体来说，Infomap的优化目标是最小化如下的Map Equation。

$$L(M)=q_{\curvearrowright}H(Q)+\sum_{i=1}^mp_{\circlearrowright}^iH(P^i)$$

其中$H(Q),H(P^i)$分别描述节点编码的平均长度和描述社区$i$的编码平均长度。$q_{i\curvearrowright}$表示离开社区$i$的概率，$q_{\curvearrowright}=\sum_{i=1}^mq_{i\curvearrowright}$表示随机游走切换社区的概率。$p_{\alpha}$表示访问节点$\alpha$的概率，$p_{\circlearrowright}=\sum_{\alpha\in i}p_{\alpha}+q_{i\curvearrowright}$表示访问及离开社区$i$的概率之和。

它基于香农源编码定理：当使用$n$个编码描述一个随机变量$X$的$n$种状态时，若每个状态$i$出现的概率为$p_i$，则该编码的长度不能低于随机变量$X$自身的熵：$H(X)=-\sum_{i=1}^np_i\log(p_i)$。

## 统计推断方法

{{< cite "rsnwcrs7" >}} 将社区发现方法归为2类：描述型方法和推断型方法。上述Louvain方法及其变体、Infomap方法属于描述型方法，而随机块模型属于推断型方法。作者阐述了推断型方法的优越性。

描述型方法使用穷举法找到使优化目标最优的结果输出。例如，在大规模集成电路的设计中，人们希望互相连接的晶体管被放置得尽可能近，以节省空间。因此，通过描述型方法将电路板划分（partition）成若干鲜有连接的块。另一个例子是CPU上的并发任务调度。为了将并行的进程分布到不同的处理器上，人们首先将进程之间的依赖关系建模成网络，接着使用描述型方法进行划分，使得不同处理器的进程之间的依赖数尽可能少。
因此，描述型方法适用于传统的图“划分”问题，目标是将图划分成若干紧密连接的块。

推断型方法通过生成模型将社区和图的生成过程联系到一起，它适用于现代常说的“社区发现”问题，用于数据分析，可以通过观测到的图结构对图进行科学的理解。以观测的图为“证据”，求得最有可能的社区结构。相比之下，推断型方法善于从数据本身出发客观地给出社区，而描述型方法由主观的目的进行划分。

如下图所示。作者通过描述火星上的一种地貌进行类比。描述型方法会讲图中有一张脸，有眼睛、鼻子和嘴巴等；而推断型方法只会讲图中是一座山。看成是人脸的解释结合了一些无关紧要的特征，实际上图中只是一座山。用人脸解释山并不能对理解该结构的生成过程有什么帮助。另一个例子是下面的图(c)和图(d)。作者随机地生成一张图，其中保证13个顶点有20的顶点度，其他230个顶点有1的顶点度。每个顶点伸出“半边”（stub），随机地与其他顶点的半边相连。使用描述型方法会得到一个看起来有道理的社区划分，也符合我们的直觉。然而，作者使用同一个随机生成模型又生成另一张图，再把描述型方法得到的社区划分代入，发现的可视化结果不再有道理。推断型方法则认为该图中没有社区结构，因为观测到的图结构无法给出充足的统计证据表明有社区。实际的图生成过程也确实如此。

<img src="https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/images/iShot_2023-09-03_10.48.36.png" />

<img src="https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/images/iShot_2023-09-03_11.00.12.png" />

### 描述还是推断？

看起来两种方法具有不同的特点。在选择方法前，回答如下的问题：

> 在方法返回一种社区划分后，如果得知图具有随机性，那么该社区划分是否不再有意义？

如果回答为“是”，那么应该使用推断型方法；否则二者均可。
描述型方法往往是一个单一目标的优化问题，与图生成无关。
推断型方法基于图的生成模型，将它返回的社区代入生成模型，再次生成一张图，会发现原来的社区划分仍然是有意义的。如下图所示。

<img src="https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/images/iShot_2023-09-03_11.09.29.png" />

### 理解推断型方法

给定观测的图结构$A$，推断型方法返回一种社区划分$b$，使得如下的后验概率最大化：

$$P(b|A)=\frac{P(A|b)P(b)}{P(A)}$$

其中$P(A|b)$是似然函数，$P(A)$是用于概率归一化的常量。

令$e_{rs}$表示社区$r,s$之间连边的个数，$k_i$表示顶点$i$的度数，著名的度保护随机块模型的似然函数为

$$P(A|b)=\sum_{e,k}P(A|k,e,b)P(k|e,b)P(e|b)$$

上式表示先从$P(e|b)$中采样社区之间的连边个数$e$，再从$P(k|e,b)$中采样顶点的度数$k$，最后图的结构需要遵循采样的$e,k$。

从描述长度的角度，有$P(A|b)P(b)=2^{-\Sigma(A,b)}$，其中$\Sigma(A,b)$表示图的描述长度。代入度保护模型，得到

$$
\begin{align}
\Sigma(A,b) &= &[-\log_2P(A|k,e,b)] &+[-\log_2P(k|e,b)-\log_2P(e|b)-\log_2P(b)] \\\
&= &D(A|k,e,b) &+ M(k,e,b)
\end{align}
$$

其中，$M(k,e,b)$表示描述模型的参数需要的比特数，$D(A|k,e,b)$表示描述图结构本身需要的比特数。
显然，**最大化后验概率$P(b|A)$等价于最小化描述长度$\Sigma(A,b)$**。
如果模型复杂，$M(k,e,b)$会变大，同时$D(A|k,e,b)$会变小，这是因为条件越复杂，符合该条件的网络结构越少。这样$M(k,e,b)$可以看做一种正则化的项。

从描述长度的角度，前面的描述型方法往往会“过拟合”数据，会在描述中加入大量无关紧要的“噪声”信息。而推断型方法有天然的正则项，可以减少过拟合。

除了减少过拟合，描述长度还可以作为一种评估不同模型好坏的无监督参量。如下图所示。

<img src="https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/images/iShot_2023-09-03_11.45.30.png" />

### 推断型方法面临的挑战

- 时间效率。优化随机块模型是NP难的问题。
- 模型太简单会导致无法充分地描述数据，太复杂时模型的拟合会变得更难（局部最优问题等）。

### 优化模块度方法的局限性

作者整理了优化模块度的方法面临的局限性，以及相对应的，推断型方法如何解决了这些问题：

<img src="https://raw.githubusercontent.com/yliuhz/blogs/master/content/posts/images/iShot_2023-09-03_11.54.10.png" />